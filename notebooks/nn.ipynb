{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba946e7e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Neural Network Model, Error Analysis, and Tuning</b></center>\n",
    "<br>\n",
    "\n",
    "**By**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**Dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**Motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general populationâ€™s opinions and reactions.\n",
    "\n",
    "**Goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a380d-da2b-4ec3-99b3-f09ee41a46f5",
   "metadata": {},
   "source": [
    "# **1. Project Setup**"
   ]
  },
  {
   "cell_type": "code",
   "id": "92ea17f6-d1a3-4c4f-8e51-e3478eed17fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:52:23.544497Z",
     "start_time": "2026-02-05T08:52:23.467440Z"
    }
   },
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import hstack\n",
    "import itertools\n",
    "\n",
    "# sci-kit learn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# General Imports\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../lib\"))"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "d92d3968-22e5-452c-9a05-952a4829579a",
   "metadata": {},
   "source": [
    "# **2. Data Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3490a-14d8-4b20-a051-9ff692ab905e",
   "metadata": {},
   "source": [
    "Run the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "id": "c665e7fa-2134-4610-8ba6-c687e92fa59c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.499994Z",
     "start_time": "2026-02-05T08:52:23.795790Z"
    }
   },
   "source": [
    "import IPython.core.page\n",
    "import builtins\n",
    "import time\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "pager = IPython.core.page.page\n",
    "helper = builtins.help\n",
    "\n",
    "IPython.core.page.page = lambda *args, **kwargs: None\n",
    "builtins.help = lambda *args, **kwargs: None\n",
    "\n",
    "try:\n",
    "    with capture_output():\n",
    "        %run data.ipynb\n",
    "finally:\n",
    "    IPython.core.page.page = pager\n",
    "    builtins.help = helper\n",
    "\n",
    "print(\"Data Setup is DONE\")\n",
    "\n",
    "# Tests\n",
    "assert X.shape == (162_801, 29318), \"Feature matrix shape is wrong; expected (162_801, 29318)\"\n",
    "assert y.shape == (162_801,), \"Labels shape is wrong; expected (162_801,)\"\n",
    "\n",
    "assert X_train.shape == (113_960, 29_318), \"Train shape is wrong; expected (113_960, 2)\"\n",
    "assert X_test.shape == (48_841, 29_318), \"Test shape is wrong; expected (48_841, 2)\"\n",
    "\n",
    "assert y_train.shape == (113_960,), \"Train labels shape is wrong; expected (113_960,)\"\n",
    "assert y_test.shape == (48_841,), \"Test labels shape is wrong; expected (48_841,)\"\n",
    "print(\"All tests passed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Setup is DONE\n",
      "All tests passed.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "71f85b79-979b-41fa-804a-812d364c5842",
   "metadata": {},
   "source": [
    "Create a combined TRAIn dataset (combined X and y train)"
   ]
  },
  {
   "cell_type": "code",
   "id": "dea2f91a-08ce-45c5-af68-6ec767f2e219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.528889Z",
     "start_time": "2026-02-05T08:53:04.523230Z"
    }
   },
   "source": [
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        X: scipy sparse matrix (features)\n",
    "        y: pandas Series or numpy array (labels)\n",
    "        \"\"\"\n",
    "        self.X = X.tocsr() if hasattr(X, 'tocsr') else X\n",
    "        self.y = y.values if hasattr(y, 'values') else y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert sparse row to dense tensor\n",
    "        x = torch.FloatTensor(self.X[idx].toarray().flatten())\n",
    "        y = torch.LongTensor([int(self.y[idx])])[0]\n",
    "        return x, y\n",
    "\n",
    "def prepare_dataset(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Transforms X_train and y_train into a PyTorch Dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : scipy sparse matrix\n",
    "        Feature matrix\n",
    "    y_train : pandas Series\n",
    "        Labels\n",
    "    mapping : dict\n",
    "        Mapping from original labels to class indices (default: {-1: 0, 0: 1, 1: 2})\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dataset : SparseDataset\n",
    "        PyTorch Dataset object ready for DataLoader\n",
    "    TRAIN_csr : scipy sparse matrix\n",
    "        Combined X and y matrix in CSR format\n",
    "    \"\"\"\n",
    "    mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    y_train = y_train.map(mapping)\n",
    "    \n",
    "    TRAIN = hstack([X_train, y_train.values.reshape(-1, 1)])\n",
    "    \n",
    "    assert TRAIN.shape[0] == X_train.shape[0], \"Row count mismatch!\"\n",
    "    assert TRAIN.shape[1] == X_train.shape[1] + 1, \"Column count mismatch!\"\n",
    "    \n",
    "    TRAIN_csr = TRAIN.tocsr()\n",
    "    \n",
    "    last_col = TRAIN_csr[:, -1].toarray().flatten()\n",
    "    flat_y = y_train.values.flatten()\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"TRAIN shape:  \", TRAIN_csr.shape)\n",
    "    print(last_col, flat_y)\n",
    "    \n",
    "    assert np.array_equal(last_col, flat_y), \"The combined TRAIN set is not the same!\"\n",
    "    \n",
    "    dataset = SparseDataset(X_train, y_train)\n",
    "    \n",
    "    return dataset, TRAIN_csr"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "25d18677-81e5-49fe-a575-b6cf95655d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.594105Z",
     "start_time": "2026-02-05T08:53:04.558184Z"
    }
   },
   "source": [
    "TRAIN_object, TRAIN_csr = prepare_dataset(X_train, y_train)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (113960, 29318)\n",
      "TRAIN shape:   (113960, 29319)\n",
      "[0 1 2 ... 0 2 2] [0 1 2 ... 0 2 2]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "fa44fa53-033c-4adb-8422-cf23813b91fb",
   "metadata": {},
   "source": [
    "# **3. Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9168c785-75cd-4a28-92ae-6afc9eeb8e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.609892Z",
     "start_time": "2026-02-05T08:53:04.599591Z"
    }
   },
   "source": [
    "class MyLittlePony(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, n_hiddens, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(vocab_size, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "\n",
    "        for _ in range(n_hiddens - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(\n",
    "            hidden_dim, \n",
    "            # There are 3 states at the end for the three sentiments\n",
    "            3\n",
    "        ))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "f5a96ee3-83e6-4af3-9383-612a43b089e1",
   "metadata": {},
   "source": [
    "## **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "id": "960631a1-8842-4e53-8a13-8423e9e377a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.640404Z",
     "start_time": "2026-02-05T08:53:04.635492Z"
    }
   },
   "source": [
    "class Hyperparameters:\n",
    "    \"\"\"\n",
    "    Hyperparameters for the multi-layer perceptron (MLP) used for sentiment analysis.\n",
    "\n",
    "    Remark (Zhean). I defined this as a class to enforce IMMUTABILITY of the hyperparamters. That is,\n",
    "    no matter what happens in the code, we can ensure that these are never changed.\n",
    "\n",
    "    # Hyperparameters\n",
    "    * N_EPOCHS: The number of training epochs.\n",
    "    * N_HIDDENS: The number of hidden layers in the MLP.\n",
    "    * N_SNEAKY_NEURONS: The number of neurons in each hidden layer.\n",
    "\n",
    "    # Usage\n",
    "    ```\n",
    "    print(Hyperparameters.N_EPOCHS)        \n",
    "    print(Hyperparameters.N_HIDDENS)       \n",
    "    print(Hyperparameters.N_SNEAKY_NEURONS)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    N_EPOCHS         = [10, 20, 30, 40, 50]\n",
    "    N_HIDDENS        = [2, 4, 8, 16]\n",
    "    N_SNEAKY_NEURONS = [128, 256, 512]\n",
    "    N_BATCH_SIZE     = [64, 128, 256, 512]\n",
    "    OPTIMIZER        = [torch.optim.Adam]\n",
    "    \n",
    "    # Fixed hyperparameters\n",
    "    CRITERION = nn.CrossEntropyLoss()\n",
    "    N_LEARNING_RATE = 1e-3"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "4319ee6b-88d9-4856-96f7-2c9c07600b56",
   "metadata": {},
   "source": [
    "## **Initializing the MLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25ad6a-cae3-43f7-9cd3-7d8a409f1cbc",
   "metadata": {},
   "source": [
    "Use CUDA if available."
   ]
  },
  {
   "cell_type": "code",
   "id": "e1c11623-a7e7-45ea-adf9-85d28565ca31",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.667932Z",
     "start_time": "2026-02-05T08:53:04.659672Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "b829a663-948d-4d48-b898-56397d6abd06",
   "metadata": {},
   "source": [
    "# **4. Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "id": "14214db7-8333-47cd-8399-962021020153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.698203Z",
     "start_time": "2026-02-05T08:53:04.688050Z"
    }
   },
   "source": [
    "def train(model, criterion, optimizer, train_loader, epoch, device):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for _ in range(epoch):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_correct / total_samples\n",
    "\n",
    "def valid(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Move data to GPU/device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_correct / total_samples"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "ffd76b6a-686b-4057-a2ea-9ce067358b28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.740971Z",
     "start_time": "2026-02-05T08:53:04.725446Z"
    }
   },
   "source": [
    "# Source - https://stackoverflow.com/a/64386444\n",
    "# Posted by Skipper, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2026-02-03, License - CC BY-SA 4.0\n",
    "\n",
    "# define a cross validation function\n",
    "def crossvalid(\n",
    "    model_class, \n",
    "    vocab_size,\n",
    "    hidden_dim,\n",
    "    n_hiddens, \n",
    "    epochs, \n",
    "    criterion, \n",
    "    optimizer_class, \n",
    "    lr,\n",
    "    dataset, \n",
    "    k_fold, \n",
    "    device, \n",
    "    batch_size\n",
    "):\n",
    "\n",
    "    train_score = pd.Series()\n",
    "    val_score = pd.Series()\n",
    "    total_size = len(dataset)\n",
    "    fraction = 1/k_fold\n",
    "    seg = int(total_size * fraction)\n",
    "\n",
    "    # stratified k fold\n",
    "    labels = TRAIN_object.y # labels\n",
    "    stratified_folds = StratifiedKFold(n_splits = k_fold, shuffle = True, random_state = 5) # StratifiedKFold instance\n",
    "\n",
    "    allocate = np.zeros(len(labels)) # dummies for split\n",
    "    fold_indices = list(stratified_folds.split(allocate, labels)) # actual stratified splitting\n",
    "\n",
    "    print(\"Original Distribution:\", np.bincount(labels) / len(labels)) # to check distribution, can remove\n",
    "    for i in tqdm(range(k_fold), desc=\"K-Fold CV\"):\n",
    "        model = model_class(vocab_size, hidden_dim, n_hiddens).to(device)\n",
    "        optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "\n",
    "        train_indices, val_indices = fold_indices[i]\n",
    "\n",
    "        train_set = torch.utils.data.dataset.Subset(dataset,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(dataset,val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "        \n",
    "        train_acc = train(model, criterion, optimizer, train_loader, epochs, device)\n",
    "        train_score.at[i] = train_acc\n",
    "        val_acc = valid(model, criterion, val_loader, device)\n",
    "        val_score.at[i] = val_acc\n",
    "\n",
    "        fold_labels = labels[val_indices] # to check distribution, can remove\n",
    "        distribution = np.bincount(fold_labels) / len(fold_labels) # to check distribution, can remove\n",
    "        print(f\"Fold {i} Val Distribution: {distribution}\") # to check distribution, can remove\n",
    "    \n",
    "    return train_score, val_score"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "e9d3af82-b388-4544-a902-95d1fd385b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T09:17:41.496682Z",
     "start_time": "2026-02-05T08:53:04.774720Z"
    }
   },
   "source": [
    "# get all possible combinations of the hyperparameters\n",
    "hyperparam_combination = itertools.product(\n",
    "    Hyperparameters.N_BATCH_SIZE,\n",
    "    Hyperparameters.N_EPOCHS, \n",
    "    Hyperparameters.N_HIDDENS, \n",
    "    Hyperparameters.N_SNEAKY_NEURONS,\n",
    "    Hyperparameters.OPTIMIZER\n",
    ")\n",
    "\n",
    "for batch_size_choice, epoch_choice, hidden_choice, neurons_choice, optimizer_choice in hyperparam_combination:\n",
    "    train_score, val_score = crossvalid(\n",
    "        # Constants\n",
    "        model_class     = MyLittlePony,\n",
    "        k_fold          = 5,\n",
    "        device          = device,\n",
    "        vocab_size      = TRAIN_csr.shape[1] - 1,\n",
    "        \n",
    "        # Variable Hyperparameters\n",
    "        hidden_dim      = neurons_choice,\n",
    "        n_hiddens       = hidden_choice,\n",
    "        epochs          = epoch_choice,\n",
    "        optimizer_class = optimizer_choice,\n",
    "        batch_size      = batch_size_choice,\n",
    "        dataset         = TRAIN_object,\n",
    "\n",
    "        # Constant Hyperparameter\n",
    "        criterion       = Hyperparameters.CRITERION, \n",
    "        lr              = Hyperparameters.N_LEARNING_RATE,\n",
    "    )\n",
    "    print(epoch_choice, hidden_choice, neurons_choice, optimizer_choice)\n",
    "    print(train_score, val_score)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Distribution: [0.21730432 0.33796069 0.44473499]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "K-Fold CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1a894a7041d4ec1a4e58c15614eae13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Val Distribution: [0.21731309 0.33796946 0.44471744]\n",
      "Fold 1 Val Distribution: [0.21731309 0.33796946 0.44471744]\n",
      "Fold 2 Val Distribution: [0.21731309 0.33796946 0.44471744]\n",
      "Fold 3 Val Distribution: [0.21731309 0.33792559 0.44476132]\n",
      "Fold 4 Val Distribution: [0.21726922 0.33796946 0.44476132]\n",
      "10 2 128 <class 'torch.optim.adam.Adam'>\n",
      "0    0.949639\n",
      "1    0.948639\n",
      "2    0.948948\n",
      "3    0.949093\n",
      "4    0.949409\n",
      "dtype: float64 0    0.815813\n",
      "1    0.819235\n",
      "2    0.814233\n",
      "3    0.817962\n",
      "4    0.816339\n",
      "dtype: float64\n",
      "Original Distribution: [0.21730432 0.33796069 0.44473499]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "K-Fold CV:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c1b5c665fd84780ab605d3a4dfaf429"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Val Distribution: [0.21731309 0.33796946 0.44471744]\n",
      "Fold 1 Val Distribution: [0.21731309 0.33796946 0.44471744]\n",
      "Fold 2 Val Distribution: [0.21731309 0.33796946 0.44471744]\n",
      "Fold 3 Val Distribution: [0.21731309 0.33792559 0.44476132]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      2\u001B[39m hyperparam_combination = itertools.product(\n\u001B[32m      3\u001B[39m     Hyperparameters.N_BATCH_SIZE,\n\u001B[32m      4\u001B[39m     Hyperparameters.N_EPOCHS, \n\u001B[32m   (...)\u001B[39m\u001B[32m      7\u001B[39m     Hyperparameters.OPTIMIZER\n\u001B[32m      8\u001B[39m )\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_size_choice, epoch_choice, hidden_choice, neurons_choice, optimizer_choice \u001B[38;5;129;01min\u001B[39;00m hyperparam_combination:\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m     train_score, val_score = \u001B[43mcrossvalid\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Constants\u001B[39;49;00m\n\u001B[32m     13\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_class\u001B[49m\u001B[43m     \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mMyLittlePony\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m        \u001B[49m\u001B[43mk_fold\u001B[49m\u001B[43m          \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m          \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[43m      \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mTRAIN_csr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Variable Hyperparameters\u001B[39;49;00m\n\u001B[32m     19\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_dim\u001B[49m\u001B[43m      \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mneurons_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_hiddens\u001B[49m\u001B[43m       \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m          \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptimizer_class\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m      \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m         \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mTRAIN_object\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \n\u001B[32m     26\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Constant Hyperparameter\u001B[39;49;00m\n\u001B[32m     27\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m       \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mHyperparameters\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCRITERION\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m              \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mHyperparameters\u001B[49m\u001B[43m.\u001B[49m\u001B[43mN_LEARNING_RATE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m     \u001B[38;5;28mprint\u001B[39m(epoch_choice, hidden_choice, neurons_choice, optimizer_choice)\n\u001B[32m     31\u001B[39m     \u001B[38;5;28mprint\u001B[39m(train_score, val_score)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 47\u001B[39m, in \u001B[36mcrossvalid\u001B[39m\u001B[34m(model_class, vocab_size, hidden_dim, n_hiddens, epochs, criterion, optimizer_class, lr, dataset, k_fold, device, batch_size)\u001B[39m\n\u001B[32m     44\u001B[39m train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers=\u001B[32m0\u001B[39m, pin_memory=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     45\u001B[39m val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers=\u001B[32m0\u001B[39m, pin_memory=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m train_acc = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     48\u001B[39m train_score.at[i] = train_acc\n\u001B[32m     49\u001B[39m val_acc = valid(model, criterion, val_loader, device)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, criterion, optimizer, train_loader, epoch, device)\u001B[39m\n\u001B[32m      4\u001B[39m total_samples = \u001B[32m0\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epoch):\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    738\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    739\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    740\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m741\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    742\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    743\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    744\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    745\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    746\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    747\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    799\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    800\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m801\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    802\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    803\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:413\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    411\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    412\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:413\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    411\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    412\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[22]\u001B[39m\u001B[32m, line 15\u001B[39m, in \u001B[36mSparseDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[32m     14\u001B[39m     \u001B[38;5;66;03m# Convert sparse row to dense tensor\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m     x = torch.FloatTensor(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtoarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m.flatten())\n\u001B[32m     16\u001B[39m     y = torch.LongTensor([\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m.y[idx])])[\u001B[32m0\u001B[39m]\n\u001B[32m     17\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x, y\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1005\u001B[39m, in \u001B[36m_cs_matrix.toarray\u001B[39m\u001B[34m(self, order, out)\u001B[39m\n\u001B[32m   1003\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m order \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1004\u001B[39m     order = \u001B[38;5;28mself\u001B[39m._swap(\u001B[33m'\u001B[39m\u001B[33mcf\u001B[39m\u001B[33m'\u001B[39m)[\u001B[32m0\u001B[39m]\n\u001B[32m-> \u001B[39m\u001B[32m1005\u001B[39m out = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_process_toarray_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (out.flags.c_contiguous \u001B[38;5;129;01mor\u001B[39;00m out.flags.f_contiguous):\n\u001B[32m   1007\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mOutput array must be C or F contiguous\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Media Files\\Real Stuff\\School\\Code Files\\Cloned Repositories\\STINTSY-Order-of-Erin\\.venv\\Lib\\site-packages\\scipy\\sparse\\_base.py:1523\u001B[39m, in \u001B[36m_spbase._process_toarray_args\u001B[39m\u001B[34m(self, order, out)\u001B[39m\n\u001B[32m   1521\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[32m   1522\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1523\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m np.zeros(\u001B[38;5;28mself\u001B[39m.shape, dtype=\u001B[38;5;28mself\u001B[39m.dtype, order=order)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
