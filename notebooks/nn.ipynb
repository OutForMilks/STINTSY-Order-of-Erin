{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba946e7e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Neural Network Model, Error Analysis, and Tuning</b></center>\n",
    "<br>\n",
    "\n",
    "**By**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**Dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**Motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general population’s opinions and reactions.\n",
    "\n",
    "**Goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a380d-da2b-4ec3-99b3-f09ee41a46f5",
   "metadata": {},
   "source": [
    "# **1. Project Setup**\n",
    "\n",
    "We import the relevant libraries for our neural network, with the most important one being PyTorch. PyTorch provides tools used to build and train neural networks, and it allows us to use our NVIDIA GPU's CUDA cores for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ea17f6-d1a3-4c4f-8e51-e3478eed17fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:52:23.544497Z",
     "start_time": "2026-02-05T08:52:23.467440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "\n",
    "# General Imports\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../lib\"))\n",
    "\n",
    "# Select device (using CUDA if possible)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d3968-22e5-452c-9a05-952a4829579a",
   "metadata": {},
   "source": [
    "# **2. Data Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3490a-14d8-4b20-a051-9ff692ab905e",
   "metadata": {},
   "source": "Now, we run the data processing pipeline. The below cell's sole purpose is to run the `data.ipynb` notebook, which runs everything related to data cleaning and data pre-processing."
  },
  {
   "cell_type": "code",
   "id": "c665e7fa-2134-4610-8ba6-c687e92fa59c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T06:54:30.027760Z",
     "start_time": "2026-02-10T06:53:55.788995Z"
    }
   },
   "source": [
    "import IPython.core.page\n",
    "import builtins\n",
    "import time\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "pager = IPython.core.page.page\n",
    "helper = builtins.help\n",
    "\n",
    "IPython.core.page.page = lambda *args, **kwargs: None\n",
    "builtins.help = lambda *args, **kwargs: None\n",
    "\n",
    "try:\n",
    "    with capture_output():\n",
    "        %run data.ipynb\n",
    "finally:\n",
    "    IPython.core.page.page = pager\n",
    "    builtins.help = helper\n",
    "\n",
    "print(\"Data Setup is DONE\")\n",
    "\n",
    "# Tests\n",
    "assert X.shape == (162_801, 29318), \"Feature matrix shape is wrong; expected (162_801, 29318)\"\n",
    "assert y.shape == (162_801,), \"Labels shape is wrong; expected (162_801,)\"\n",
    "\n",
    "assert X_train.shape == (113_960, 29_318), \"Train shape is wrong; expected (113_960, 2)\"\n",
    "assert X_test.shape == (48_841, 29_318), \"Test shape is wrong; expected (48_841, 2)\"\n",
    "\n",
    "assert y_train.shape == (113_960,), \"Train labels shape is wrong; expected (113_960,)\"\n",
    "assert y_test.shape == (48_841,), \"Test labels shape is wrong; expected (48_841,)\"\n",
    "print(\"All tests passed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Setup is DONE\n",
      "All tests passed.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "71f85b79-979b-41fa-804a-812d364c5842",
   "metadata": {},
   "source": [
    "We then prepare the dataset to be PyTorch-compatible.\n",
    "\n",
    "Because our labels in `y_train` are -1, 0, and 1, they are incompatible with PyTorch which requires class labels to be non-negative integers. TorchableSet maps them to non-negative integers so that PyTorch can handle them.\n",
    "\n",
    "TorchableSet also ensures that the features (`X_train`) are stored in a sparse matrix and the labels (`y_train`) are in a NumPy array for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32ae221-08c4-40af-8bd1-e67806772cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from barn.data_preparer import TorchableSet\n",
    "TRAIN = TorchableSet(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3bbbb-7eaf-445b-bf4e-3352ae41392c",
   "metadata": {},
   "source": [
    "# **3. Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26bddc-2d97-402e-a537-b2042372a893",
   "metadata": {},
   "source": [
    "## **Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9168c785-75cd-4a28-92ae-6afc9eeb8e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.609892Z",
     "start_time": "2026-02-05T08:53:04.599591Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLittlePony(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, n_hiddens, dropout):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(vocab_size, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "\n",
    "        for _ in range(n_hiddens - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(\n",
    "            hidden_dim, \n",
    "            # There are 3 states at the end for the three sentiments\n",
    "            3\n",
    "        ))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829a663-948d-4d48-b898-56397d6abd06",
   "metadata": {},
   "source": [
    "# **4. Training the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad3700-202a-4906-82ad-0fb06a0eba04",
   "metadata": {},
   "source": [
    "Import the scripts to be used for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14214db7-8333-47cd-8399-962021020153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-05T08:53:04.698203Z",
     "start_time": "2026-02-05T08:53:04.688050Z"
    }
   },
   "outputs": [],
   "source": [
    "from barn.trainer import train, validate, crossvalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38cbfc-9e75-457e-8b18-c87492071708",
   "metadata": {},
   "source": [
    "## **Computational Resources**\n",
    "\n",
    "The training setup was performed on the following machine:\n",
    "   * CPU: AMD Ryzen 7 7800X3D 8-Core Processor\n",
    "   * GPU: NVIDIA GeForce RTX 3070 \n",
    "   * RAM: 16 GB DDR4 (2×8 GB) dual-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd19dfa-7eac-417a-aaf1-9483b3d57365",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce59325-642d-422c-a805-aacd063d8451",
   "metadata": {},
   "source": [
    "As a testbench, we first defined an arbitrary set of simple hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac355ba8-07fc-4bd7-a3c3-91170b476383",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim     = 2\n",
    "hidden_neurons = 32\n",
    "dropout        = 0.2\n",
    "batch_size     = 32\n",
    "learning_rate  = 1e-3\n",
    "weight_decay   = 1e-5\n",
    "epoch          = 5\n",
    "batch_size     = 32\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "mlp = MyLittlePony(TRAIN.vocab_size, hidden_dim, hidden_neurons, dropout)\n",
    "\n",
    "mlp.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c34fa-d191-4b7c-8318-8d9b3b0ab8dd",
   "metadata": {},
   "source": [
    "Then, train the model on these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32297de1-3b8d-4e20-8201-89770220f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = train(\n",
    "    model        = mlp,\n",
    "    criterion    = criterion,\n",
    "    optimizer    = optimizer,\n",
    "    epoch        = epoch,\n",
    "    train_loader = DataLoader(TRAIN, batch_size=batch_size, shuffle=True),\n",
    "    device       = DEVICE\n",
    ")\n",
    "\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b49fd7-e0a7-43cb-9c2a-ead1d01cf32f",
   "metadata": {},
   "source": [
    "## **Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d704d-5316-4c1f-8752-8265fb5bdbcc",
   "metadata": {},
   "source": [
    "Now, let's evaluate the model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4d5b5-8167-44d6-a394-4de01f66a62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = validate(\n",
    "    model      = mlp,\n",
    "    val_loader = DataLoader(TRAIN, batch_size=batch_size, shuffle=False),\n",
    "    device     = DEVICE\n",
    ")\n",
    "\n",
    "print(validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa706a",
   "metadata": {},
   "source": [
    "Save the notebook for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': mlp.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'validation_accuracy': validation_accuracy,\n",
    "    'hyperparameters': {\n",
    "        'hidden_dim': hidden_dim,\n",
    "        'hidden_neurons': hidden_neurons,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "}, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c48dba-d62d-4287-9a44-86a273494b0b",
   "metadata": {},
   "source": [
    "# **5. Model Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed547dcf-d28f-46d7-926f-b66cfa5030cf",
   "metadata": {},
   "source": [
    "## **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33932e-311e-44e0-b7f1-cbbb96361ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    \"\"\"\n",
    "    Hyperparameters for the multi-layer perceptron (MLP) used for sentiment analysis.\n",
    "\n",
    "    Remark (Zhean). I defined this as a class to enforce IMMUTABILITY of the hyperparamters. That is,\n",
    "    no matter what happens in the code, we can ensure that these are never changed.\n",
    "\n",
    "    # Hyperparameters\n",
    "    * N_EPOCHS: The number of training epochs.\n",
    "    * N_HIDDENS: The number of hidden layers in the MLP.\n",
    "    * N_SNEAKY_NEURONS: The number of neurons in each hidden layer.\n",
    "    * N_BATCH_SIZE: The batch size.\n",
    "    * \n",
    "\n",
    "    # Usage\n",
    "    ```\n",
    "    print(Hyperparameters.N_EPOCHS)        \n",
    "    print(Hyperparameters.N_HIDDENS)       \n",
    "    print(Hyperparameters.N_SNEAKY_NEURONS)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Choosable Hyperparameters\n",
    "    # add subscript (C_) to denote it as choosable\n",
    "    C_EPOCHS         = [5]#, 10 , 20 , 30]\n",
    "    C_HIDDENS        = [1  ] #, 2  , 3]           \n",
    "    C_SNEAKY_NEURONS = [128] #, 256, 512]\n",
    "    C_BATCH_SIZE     = [64 , 128] #, 256]\n",
    "    C_DROPOUT        = [0.2]#, 0.3, 0.5]     \n",
    "    \n",
    "    # Fixed (choice-less) hyperparameters\n",
    "    OPTIMIZER        = torch.optim.Adam\n",
    "    CRITERION        = nn.CrossEntropyLoss()\n",
    "    LEARNING_RATE    = 1e-3\n",
    "    WEIGHT_DECAY     = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fede647-a04b-4b53-95d1-9fde41b7ca4a",
   "metadata": {},
   "source": [
    "Now, we can get all the possible combinations of the Hyperparameters by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bf82e-0975-4449-9a93-ec85fd285606",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_combinations = itertools.product(\n",
    "    Hyperparameters.C_BATCH_SIZE,\n",
    "    Hyperparameters.C_EPOCHS, \n",
    "    Hyperparameters.C_HIDDENS, \n",
    "    Hyperparameters.C_SNEAKY_NEURONS,\n",
    "    Hyperparameters.C_DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece23970-fb9b-4d30-926a-3c8729df07cf",
   "metadata": {},
   "source": [
    "## **Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fba57-bc4b-41ff-ba28-35e88fdea40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from barn.gs import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84854c-de42-4db7-a2be-3c96ac4ec9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, best_cfg = grid_search(\n",
    "#     model_class=MyLittlePony,\n",
    "#     hyperparam_combinations=hyperparam_combinations,\n",
    "#     optimizer=Hyperparameters.OPTIMIZER,\n",
    "#     criterion=Hyperparameters.CRITERION,\n",
    "#     learning_rate=Hyperparameters.LEARNING_RATE,\n",
    "#     weight_decay=Hyperparameters.WEIGHT_DECAY,\n",
    "#     dataset=TRAIN,\n",
    "#     vocab_size=TRAIN.vocab_size,\n",
    "#     device=DEVICE,\n",
    "#     k_fold=5,\n",
    "#     save_path=\"best_pony.pt\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
