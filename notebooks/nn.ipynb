{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba946e7e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Neural Network Model, Error Analysis, and Tuning</b></center>\n",
    "<br>\n",
    "\n",
    "**By**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**Dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**Motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general populationâ€™s opinions and reactions.\n",
    "\n",
    "**Goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a380d-da2b-4ec3-99b3-f09ee41a46f5",
   "metadata": {},
   "source": [
    "# **1. Project Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ea17f6-d1a3-4c4f-8e51-e3478eed17fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:30:23.162838Z",
     "start_time": "2026-02-03T09:30:23.159786Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# General Imports\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../lib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d3968-22e5-452c-9a05-952a4829579a",
   "metadata": {},
   "source": [
    "# **2. Data Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3490a-14d8-4b20-a051-9ff692ab905e",
   "metadata": {},
   "source": [
    "Run the data processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c665e7fa-2134-4610-8ba6-c687e92fa59c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:30:40.207825Z",
     "start_time": "2026-02-03T09:30:23.195691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Setup is DONE\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "import IPython.core.page\n",
    "import builtins\n",
    "import time\n",
    "from IPython.utils.capture import capture_output\n",
    "\n",
    "pager = IPython.core.page.page\n",
    "helper = builtins.help\n",
    "\n",
    "IPython.core.page.page = lambda *args, **kwargs: None\n",
    "builtins.help = lambda *args, **kwargs: None\n",
    "\n",
    "try:\n",
    "    with capture_output():\n",
    "        %run data.ipynb\n",
    "finally:\n",
    "    IPython.core.page.page = pager\n",
    "    builtins.help = helper\n",
    "\n",
    "print(\"Data Setup is DONE\")\n",
    "\n",
    "# Tests\n",
    "assert X.shape == (162_801, 29318), \"Feature matrix shape is wrong; expected (162_801, 29318)\"\n",
    "assert y.shape == (162_801,), \"Labels shape is wrong; expected (162_801,)\"\n",
    "\n",
    "assert X_train.shape == (113_960, 29_318), \"Train shape is wrong; expected (113_960, 2)\"\n",
    "assert X_test.shape == (48_841, 29_318), \"Test shape is wrong; expected (48_841, 2)\"\n",
    "\n",
    "assert y_train.shape == (113_960,), \"Train labels shape is wrong; expected (113_960,)\"\n",
    "assert y_test.shape == (48_841,), \"Test labels shape is wrong; expected (48_841,)\"\n",
    "print(\"All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f85b79-979b-41fa-804a-812d364c5842",
   "metadata": {},
   "source": [
    "Create a combined TRAIn dataset (combined X and y train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea2f91a-08ce-45c5-af68-6ec767f2e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        X: scipy sparse matrix (features)\n",
    "        y: pandas Series or numpy array (labels)\n",
    "        \"\"\"\n",
    "        self.X = X.tocsr() if hasattr(X, 'tocsr') else X\n",
    "        self.y = y.values if hasattr(y, 'values') else y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert sparse row to dense tensor\n",
    "        x = torch.FloatTensor(self.X[idx].toarray().flatten())\n",
    "        y = torch.LongTensor([int(self.y[idx])])[0]\n",
    "        return x, y\n",
    "\n",
    "def prepare_dataset(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Transforms X_train and y_train into a PyTorch Dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : scipy sparse matrix\n",
    "        Feature matrix\n",
    "    y_train : pandas Series\n",
    "        Labels\n",
    "    mapping : dict\n",
    "        Mapping from original labels to class indices (default: {-1: 0, 0: 1, 1: 2})\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dataset : SparseDataset\n",
    "        PyTorch Dataset object ready for DataLoader\n",
    "    TRAIN_csr : scipy sparse matrix\n",
    "        Combined X and y matrix in CSR format\n",
    "    \"\"\"\n",
    "    mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    y_train = y_train.map(mapping)\n",
    "    \n",
    "    TRAIN = hstack([X_train, y_train.values.reshape(-1, 1)])\n",
    "    \n",
    "    assert TRAIN.shape[0] == X_train.shape[0], \"Row count mismatch!\"\n",
    "    assert TRAIN.shape[1] == X_train.shape[1] + 1, \"Column count mismatch!\"\n",
    "    \n",
    "    TRAIN_csr = TRAIN.tocsr()\n",
    "    \n",
    "    last_col = TRAIN_csr[:, -1].toarray().flatten()\n",
    "    flat_y = y_train.values.flatten()\n",
    "    \n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"TRAIN shape:  \", TRAIN_csr.shape)\n",
    "    print(last_col, flat_y)\n",
    "    \n",
    "    assert np.array_equal(last_col, flat_y), \"The combined TRAIN set is not the same!\"\n",
    "    \n",
    "    dataset = SparseDataset(X_train, y_train)\n",
    "    \n",
    "    return dataset, TRAIN_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d18677-81e5-49fe-a575-b6cf95655d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (113960, 29318)\n",
      "TRAIN shape:   (113960, 29319)\n",
      "[0 1 2 ... 0 2 2] [0 1 2 ... 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_object, TRAIN_csr = prepare_dataset(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa44fa53-033c-4adb-8422-cf23813b91fb",
   "metadata": {},
   "source": [
    "# **3. Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9168c785-75cd-4a28-92ae-6afc9eeb8e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:30:40.297400Z",
     "start_time": "2026-02-03T09:30:40.293756Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLittlePony(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, n_hiddens=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        # ERROR PRONE: THIS RELIES ON A SIDE EFFECT (i.e., TRAIN AS A GLOBAL VALUE)\n",
    "        # TRAIN.shape[1] - 1\n",
    "        # we do (- 1) because we concatenated y_train to X_train hence it adds a column.\n",
    "        layers = [nn.Linear(vocab_size, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "\n",
    "        for _ in range(n_hiddens - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        # There are 3 states at the end for the three sentiments\n",
    "        layers.append(nn.Linear(hidden_dim, 3))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a96ee3-83e6-4af3-9383-612a43b089e1",
   "metadata": {},
   "source": [
    "## **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960631a1-8842-4e53-8a13-8423e9e377a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:30:40.335370Z",
     "start_time": "2026-02-03T09:30:40.332814Z"
    }
   },
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    \"\"\"\n",
    "    Hyperparameters for the multi-layer perceptron (MLP) used for sentiment analysis.\n",
    "\n",
    "    Remark (Zhean). I defined this as a class to enforce IMMUTABILITY of the hyperparamters. That is,\n",
    "    no matter what happens in the code, we can ensure that these are never changed.\n",
    "\n",
    "    # Hyperparameters\n",
    "    * N_EPOCHS: The number of training epochs.\n",
    "    * N_HIDDENS: The number of hidden layers in the MLP.\n",
    "    * N_SNEAKY_NEURONS: The number of neurons in each hidden layer.\n",
    "\n",
    "    # Usage\n",
    "    ```\n",
    "    print(Hyperparameters.N_EPOCHS)        \n",
    "    print(Hyperparameters.N_HIDDENS)       \n",
    "    print(Hyperparameters.N_SNEAKY_NEURONS)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    N_EPOCHS = [10, 20, 30, 40, 50]\n",
    "    N_HIDDENS = [2, 4]\n",
    "    N_SNEAKY_NEURONS = [128, 256]\n",
    "    N_LEARNING_RATE = [1e-3]\n",
    "    CRITERION = [nn.CrossEntropyLoss()]\n",
    "    # ISN'T LR also a hyperparameter?\n",
    "    # ADD OPTIMIZER HERE LATER\n",
    "    # torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    OPTIMIZER = [torch.optim.Adam]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4319ee6b-88d9-4856-96f7-2c9c07600b56",
   "metadata": {},
   "source": [
    "## **Initializing the MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c11623-a7e7-45ea-adf9-85d28565ca31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:30:40.390146Z",
     "start_time": "2026-02-03T09:30:40.364603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "MLP initialized\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MyLittlePony(\n",
    "    vocab_size  = TRAIN_csr.shape[1] - 1, \n",
    "    hidden_dim  = Hyperparameters.N_SNEAKY_NEURONS[0],\n",
    "    n_hiddens   = Hyperparameters.N_HIDDENS[0]\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"MLP initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829a663-948d-4d48-b898-56397d6abd06",
   "metadata": {},
   "source": [
    "# **4. Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14214db7-8333-47cd-8399-962021020153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:45:02.551801Z",
     "start_time": "2026-02-03T09:30:40.446166Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epoch):\n",
    "    model.train()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for _ in range(epoch):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_correct / total_samples\n",
    "\n",
    "def valid(model, criterion, optimizer, val_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd76b6a-686b-4057-a2ea-9ce067358b28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source - https://stackoverflow.com/a/64386444\n",
    "# Posted by Skipper, modified by community. See post 'Timeline' for change history\n",
    "# Retrieved 2026-02-03, License - CC BY-SA 4.0\n",
    "\n",
    "# define a cross validation function\n",
    "def crossvalid(model, epochs, criterion, optimizer, dataset, k_fold):\n",
    "    \n",
    "    train_score = pd.Series()\n",
    "    val_score = pd.Series()\n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    fraction = 1/k_fold\n",
    "    seg = int(total_size * fraction)\n",
    "    for i in range(k_fold):\n",
    "        trll = 0\n",
    "        trlr = i * seg\n",
    "        vall = trlr\n",
    "        valr = i * seg + seg\n",
    "        trrl = valr\n",
    "        trrr = total_size\n",
    "\n",
    "        train_left_indices = list(range(trll,trlr))\n",
    "        train_right_indices = list(range(trrl,trrr))\n",
    "        \n",
    "        train_indices = train_left_indices + train_right_indices\n",
    "        val_indices = list(range(vall,valr))\n",
    "        \n",
    "        train_set = torch.utils.data.dataset.Subset(dataset,train_indices)\n",
    "        val_set = torch.utils.data.dataset.Subset(dataset,val_indices)\n",
    "\n",
    "        # BATCH SIZE HERE IS ANOTHER HYPERPARAMETER\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True, num_workers=4)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=50, shuffle=True, num_workers=4)\n",
    "        train_acc = train(model,criterion,optimizer,train_loader,epochs)\n",
    "        train_score.at[i] = train_acc\n",
    "        val_acc = valid(model,criterion,optimizer,val_loader)\n",
    "        val_score.at[i] = val_acc\n",
    "    \n",
    "    return train_score, val_score\n",
    "\n",
    "train_score, val_score = crossvalid(\n",
    "    model, \n",
    "    epochs = Hyperparameters.N_EPOCHS[0],\n",
    "    criterion=Hyperparameters.CRITERION[0], \n",
    "    optimizer=Hyperparameters.OPTIMIZER[0](model.parameters(), lr=Hyperparameters.N_LEARNING_RATE[0]),\n",
    "    dataset=TRAIN_object,\n",
    "    k_fold=5\n",
    ")\n",
    "print(train_score, val_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd50210-79ec-448c-95a9-457914549d4a",
   "metadata": {},
   "source": [
    "# **5. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e6372-c87d-4038-8bf1-13c286e0f64e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-03T09:45:09.368870Z",
     "start_time": "2026-02-03T09:45:02.825205Z"
    }
   },
   "outputs": [],
   "source": [
    "# def evaluate(model, data_loader, device):\n",
    "#     \"\"\"\n",
    "#     Evaluate the model on a validation/test dataset.\n",
    "\n",
    "#     # Parameters\n",
    "#     * model: PyTorch model\n",
    "#     * data_loader: DataLoader for validation/test set\n",
    "#     * device: torch.device ('cpu' or 'cuda')\n",
    "\n",
    "#     # Returns\n",
    "#     * accuracy: float, proportion of correct predictions\n",
    "#     \"\"\"\n",
    "#     model.eval() \n",
    "#     correct, total = 0, 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for x_batch, y_batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "#             x_batch = x_batch.to(device, non_blocking=True)\n",
    "#             y_batch = y_batch.to(device, non_blocking=True)\n",
    "\n",
    "#             logits = model(x_batch)\n",
    "            \n",
    "#             prediction = logits.argmax(dim=1)\n",
    "\n",
    "#             correct += (prediction == y_batch).sum().item()\n",
    "#             total += y_batch.size(0)\n",
    "\n",
    "#     return correct / total\n",
    "\n",
    "# val_accuracy = evaluate(model, train_loader, device)\n",
    "# print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
