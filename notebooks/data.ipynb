{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44906e4e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Data Description, Cleaning, Exploratory Data Analysis, and Preprocessing</b></center>\n",
    "<br>\n",
    "\n",
    "**by**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general population‚Äôs opinions and reactions.\n",
    "\n",
    "**goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis.\n",
    "\n",
    "### **dataset description**\n",
    "\n",
    "The Twitter Sentiments Dataset is a dataset that contains nearly 163k tweets from Twitter. The time period of when these were collected is unknown, but it was published to Mendeley Data on May 14, 2021 by Sherif Hussein of Mansoura University.\n",
    "\n",
    "Tweets were extracted using the Twitter API, but the specifics of how the tweets were selected are unmentioned. The tweets are mostly English with a mix of some Hindi words for code-switching <u>(El-Demerdash., 2021)</u>. All of them seem to be talking about the political state of India. Most tweets mention Narendra Modi, the current Prime Minister of India.\n",
    "\n",
    "Each tweet was assigned a label using TextBlob's sentiment analysis <u>(El‚ÄëDemerdash, Hussein, & Zaki, 2021)</u>, which assigns labels automatically.\n",
    "\n",
    "Twitter_Data\n",
    "- **`clean_text`**: The tweet's text\n",
    "- **`category`**: The tweet's sentiment category\n",
    "\n",
    "What each row and column represents: `each row represents one tweet.` <br>\n",
    "Number of observations: `162,980`\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Code-switching is the practice of alternating between two languages $L_1$ (the native language) and $L_2$ (the source language) in a conversation. In this context, the code-switching is done to appear more casual since the conversation is done via Twitter (now, X). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47491b2f",
   "metadata": {},
   "source": [
    "## **1 project set up**\n",
    "We set the global imports for the projects (ensure these are installed via uv and is part of the environment). Furthermore, load the dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7578d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:01:26.803271Z",
     "start_time": "2025-06-17T14:01:26.567527Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Use lib directory\n",
    "sys.path.append(os.path.abspath(\"../lib\"))\n",
    "\n",
    "# Imports from lib files\n",
    "from janitor import clean, find_spam_and_empty, rem_stopwords\n",
    "from lemmatize import lemmatizer\n",
    "from boilerplate import stopwords_set\n",
    "from bag_of_words import BagOfWordsModel\n",
    "\n",
    "# Load raw data file\n",
    "df = pd.read_csv(\"../data/Twitter_Data.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ec446",
   "metadata": {},
   "source": [
    "## **2 data cleaning**\n",
    "This section discusses the methodology for data cleaning.\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in [george m].\n",
    "\n",
    "The cleaning pipeline has four main functions. The first function is the `normalize` function, it normalizes the text input to ASCII-only characters (say, \"c√≥mo est√°s\" becomes \"como estas\") and lowercases alphabetic symbols. The dataset contains Unicode characters (e.g., emojis and accented characters) which the function replaces to the empty string (`''`). Then, `rem_punctuation` removes the punctuation marks and special characters with the empty string. Then, `collapse_whitespace` collapses all whitespace characters to a single space. Formally, it is a transducer \n",
    "\n",
    "$$\n",
    "\\Box^+ \\mapsto \\Box \\qquad \\text{where the space character is } \\Box\n",
    "$$\n",
    "\n",
    "Since the domain of the corpus is Twitter, spam may become an issue by the vector representation step. Hence we employed some simple rule-based spam detection systems. The rule-based system removes words in the string that contains the same letter or substring thrice, consecutively, or strings like \"aaah\" or \"ahahahaha\" which is common expressions used in Twitter. These are filtered the regular expression below:\n",
    "\n",
    "$$\n",
    "\\text{same\\_char\\_thrice} := (.)\\textbackslash1^{\\{2,\\}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text{same\\_substring\\_twice} := (.^+)\\textbackslash1^+\n",
    "$$\n",
    "\n",
    "Furthermore, it also removes any string that has a length less than three, since these are either stopwords (that weren't detected in the stopword removal stage) or more spam, or strings like \"dd\", \"aa\", among others. Finally, we remove any string $s:\\text{String}$, with length $|s|$, if the condition below is true:\n",
    "\n",
    "$$\n",
    "\\frac{\\texttt{\\#\\_unique\\_chars}(s)}{|s|} < 0.3 + \\left(\\frac{0.1 \\cdot \\text{min}(|s|, 10)}{10}\\right)\n",
    "$$\n",
    "\n",
    "This is an adaptive character diversity threshold for the string $s$. It calculates the diversity of characters in a string; if the string repeats the same character alot, we expect it to be unintelligible or useless, hence we remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac69746-f471-4a66-9292-c2b363d12de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_ours\"] = df[\"clean_text\"].map(clean)\n",
    "df[\"clean_ours\"] = df[\"clean_ours\"].map(find_spam_and_empty)\n",
    "df = df.dropna(subset=[\"clean_ours\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb4d20",
   "metadata": {},
   "source": [
    "# **3 preprocessing**\n",
    "\n",
    "> üèóÔ∏è Perhaps swap S3 and S4. Refer to literature on what comes first.\n",
    "\n",
    "This section discusses preprocessing steps for the cleaned data.\n",
    "\n",
    "## **lemmatization**\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in <u>(George & Murugesan, 2024)</u>. We preprocess the dataset entries via lemmatization. We use NLTK for this task using WordNetLemmatizer lemmatization, repectively <u>(Bird & Loper, 2004)</u>. For the lemmatization step, we use the WordNet for English lemmatization and Open Multilingual WordNet version 1.4 for translations and multilingual support which is important for our case since some tweets contain text from Indian Languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c3a9b5-b35a-47ca-9ecf-7f950db07395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized\"] = df[\"clean_ours\"].map(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42589219-d005-4ab3-b825-cccb7fa6d663",
   "metadata": {},
   "source": [
    "## **stop word removal**\n",
    "\n",
    "After lemmatization, we may now remove the stop words present in the dataset. The stopword removal _needs_ to be after lemmatization since this step requires all words to be reduces to their base dictionary form, and the `stopword_set` only considers base dictionary forms of the stopwords.\n",
    "\n",
    "**stopwords.** For stop words removal, we refer to the English stopwords dataset defined in NLTK and Wolfram Mathematica <u>(Bird & Loper, 2004; Wolfram Research, 2015)</u>. However, since the task is sentiment analysis, words that invoke polarity, intensification, and negation are important. Words like \"not\" and \"okay\" are commonly included as stopwords. Therefore, the stopwords from [nltk,mathematica] are manually adjusted to only include stopwords that invoke neutrality, examples are \"after\", \"when\", and \"you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a91231f-c21d-41da-9296-7b8607f9cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized\"] = df[\"lemmatized\"].map(lambda t: rem_stopwords(t, stopwords_set))\n",
    "df = df.dropna(subset=[\"lemmatized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd19e4-020a-4e3d-9d0f-6187ce4102d0",
   "metadata": {},
   "source": [
    "## looking at the DataFrame\n",
    "\n",
    "After preprocessing, the dataset now contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "883b7f78-ba3d-4bad-9969-0a09b067e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162942 entries, 0 to 162979\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162942 non-null  object \n",
      " 1   category    162942 non-null  float64\n",
      " 2   clean_ours  162942 non-null  object \n",
      " 3   lemmatized  162942 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54e5b1-3b81-477e-96ee-602c33533a41",
   "metadata": {},
   "source": [
    "Here are 10 randomly picked entries in the dataframe with all columns shown for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e069ce-83c3-4769-8914-b442625a6032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_ours</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161589</th>\n",
       "      <td>should not you feel shame modi didnt order investigation single allegation scam him and his minister honesty honest one should welcome all type investigation</td>\n",
       "      <td>1.0</td>\n",
       "      <td>should not you feel shame modi didnt order investigation single allegation scam him and his minister honesty honest one should welcome all type investigation</td>\n",
       "      <td>feel shame modi order investigation single allegation scam minister honesty honest welcome all type investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40872</th>\n",
       "      <td>going address nation 1145 today\\nsome big announcements are expected</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>going address nation today some big announcements are expected</td>\n",
       "      <td>address nation today big announcement expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35699</th>\n",
       "      <td>the trailer super awesome hero you are simply superb itsuper proud youguys watch the trailer the web series coming soon april only the link the trailer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>the trailer super awesome hero you are simply superb itsuper proud youguys watch the trailer the web series coming soon april only the link the trailer</td>\n",
       "      <td>trailer super awesome hero simply superb itsuper proud youguys watch trailer web series coming soon april only link trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126159</th>\n",
       "      <td>today the whole with was the motto modi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>today the whole with was the motto modi</td>\n",
       "      <td>today motto modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115976</th>\n",
       "      <td>nirav modis attempt try and seek citizenship vanuatu shows was trying move away from india important time says judge</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nirav modis attempt try and seek citizenship vanuatu shows was trying move away from india important time says judge</td>\n",
       "      <td>nirav modis attempt seek citizenship vanuatu move away india important time judge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                           clean_text  \\\n",
       "161589  should not you feel shame modi didnt order investigation single allegation scam him and his minister honesty honest one should welcome all type investigation   \n",
       "40872                                                                                           going address nation 1145 today\\nsome big announcements are expected    \n",
       "35699        the trailer super awesome hero you are simply superb itsuper proud youguys watch the trailer the web series coming soon april only the link the trailer    \n",
       "126159                                                                                                                       today the whole with was the motto modi    \n",
       "115976                                          nirav modis attempt try and seek citizenship vanuatu shows was trying move away from india important time says judge    \n",
       "\n",
       "        category  \\\n",
       "161589       1.0   \n",
       "40872       -1.0   \n",
       "35699        1.0   \n",
       "126159       1.0   \n",
       "115976       1.0   \n",
       "\n",
       "                                                                                                                                                           clean_ours  \\\n",
       "161589  should not you feel shame modi didnt order investigation single allegation scam him and his minister honesty honest one should welcome all type investigation   \n",
       "40872                                                                                                  going address nation today some big announcements are expected   \n",
       "35699         the trailer super awesome hero you are simply superb itsuper proud youguys watch the trailer the web series coming soon april only the link the trailer   \n",
       "126159                                                                                                                        today the whole with was the motto modi   \n",
       "115976                                           nirav modis attempt try and seek citizenship vanuatu shows was trying move away from india important time says judge   \n",
       "\n",
       "                                                                                                                         lemmatized  \n",
       "161589            feel shame modi order investigation single allegation scam minister honesty honest welcome all type investigation  \n",
       "40872                                                                                address nation today big announcement expected  \n",
       "35699   trailer super awesome hero simply superb itsuper proud youguys watch trailer web series coming soon april only link trailer  \n",
       "126159                                                                                                             today motto modi  \n",
       "115976                                            nirav modis attempt seek citizenship vanuatu move away india important time judge  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64d042-e337-4d55-a6e4-0d065abd2738",
   "metadata": {},
   "source": [
    "## **Vector Representation**\n",
    "\n",
    "After the stemming and lemmatization steps, each entry can now be represented as a vector using a Bag of Words (BoW) model. We employ scikit-learn's `CountVectorizer`, which provides a ready-to-use implementation of BoW <u>(Pedregosa et al., 2011)</u>.\n",
    "\n",
    "> **comparison of vectorization techniques:** Traditional vectorization techniques include BoW and Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF weights each word based on its frequency in a document and its rarity across the corpus, reducing the impact of common words. BoW, in contrast, simply counts word occurrences without considering corpus-level frequency. In this project, BoW was chosen because stopwords were already removed during preprocessing, and the dataset is domain-specific <u>(Rani et al., 2022)</u>. In such datasets, frequent words are often meaningful domain keywords, so scaling them down (as TF-IDF would) could reduce the importance of these key terms in the feature representation.\n",
    "\n",
    "**tokenization:** Since the data cleaning and preprocessing stage is comprehensive, the tokenization step in the BoW model reduces to a simple word-boundary split operation. Each preprocessed entry in the DataFrame is split by spaces. For example, the entry `\"shri narendra modis\"` (entry: 42052) becomes `[\"shri\", \"narendra\", \"modis\"]`. By the end of tokenization, all entries are transformed into arrays of strings.\n",
    "\n",
    "**word bigrams:** As noted earlier, modifiers and polarity words are not included in the stopword set. The BoW model constructs a vocabulary containing both unigrams and bigrams. Including bigrams allows the model to capture common word patterns, such as  \n",
    "\n",
    "$$\n",
    "\\left\\langle \\texttt{Adj}\\right\\rangle \\left\\langle \\texttt{M} \\mid \\texttt{Pron} \\right\\rangle \n",
    "$$  \n",
    "\n",
    "or  \n",
    "\n",
    "$$\n",
    "\\left\\langle \\texttt{Adv}\\right\\rangle \\left\\langle \\texttt{V} \\mid \\texttt{Adj} \\mid \\texttt{Adv} \\right\\rangle \n",
    "$$  \n",
    "\n",
    "Words with modifiers have the modifiers directly attached, enabling subsequent models to capture the concept of modification fully. Consequently, after tokenization and bigram construction, the vocabulary size can grow up to $O(n^2)$, where $n$ is the number of unique tokens.\n",
    "\n",
    "**minimum document frequency constraint:** Despite cleaning and spam removal, some tokens remain irrelevant or too rare. To address this, a minimum document frequency constraint is applied: $\\texttt{min\\_df} = 10$, meaning a token must appear in at least 10 documents to be included in the BoW vocabulary. This reduces noise and ensures the model focuses on meaningful terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bfba459-6837-4481-929c-ef5ee023b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zrgnt/STINTSY-Order-of-Erin/.venv/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:526: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BagOfWordsModel' object has no attribute 'sparcity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# some sanity checks\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m bow.matrix.shape[\u001b[32m0\u001b[39m] == df.shape[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mnumber of rows in the matrix DOES NOT matches the number of documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mbow\u001b[49m\u001b[43m.\u001b[49m\u001b[43msparcity\u001b[49m,                       \u001b[33m\"\u001b[39m\u001b[33mthe sparsity is TOO HIGH, something went wrong\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'BagOfWordsModel' object has no attribute 'sparcity'"
     ]
    }
   ],
   "source": [
    "bow = BagOfWordsModel(df[\"lemmatized\"], 10)\n",
    "\n",
    "# some sanity checks\n",
    "assert bow.matrix.shape[0] == df.shape[0], \"number of rows in the matrix DOES NOT matches the number of documents\"\n",
    "assert bow.sparcity,                       \"the sparsity is TOO HIGH, something went wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf5220-712f-4af3-9e2e-0fc3753f5cb1",
   "metadata": {},
   "source": [
    "### **model metrics**\n",
    "\n",
    "To get an idea of the model, we will now look at its shape and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0246d-899e-4bb6-957a-75419316197a",
   "metadata": {},
   "source": [
    "The resulting vector has a shape of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a5b688-b636-4320-bf22-e508a97aa862",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc081f2-0630-4a0c-a40a-aeddcdfc8fb8",
   "metadata": {},
   "source": [
    "The first entry of the pair is the number of documents (the ones that remain after all the data cleaning and preprocessing steps) and the second entry is the number of tokens (or unique words in the vocabulary).\n",
    "\n",
    "The resulting model has a sparsity of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd01aa7-4842-4473-b410-591fd47983f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3277d-2106-490d-a304-8ff6e0780fd5",
   "metadata": {},
   "source": [
    "> üèóÔ∏è perhaps discuss sparsity's relevance\n",
    "\n",
    "Now, looking at the most frequent and least frequent terms in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd4551-90d0-4795-9303-1118fcc058c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_frequencies = np.asarray((bow.matrix > 0).sum(axis=0)).flatten()\n",
    "freq_order = np.argsort(doc_frequencies)[::-1]\n",
    "bow.feature_names[freq_order[:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c2f23-6494-4d41-937f-19664010b138",
   "metadata": {},
   "source": [
    "We see that the main talking point of the Tweets, which hovers around Indian politics with keywords like \"modi\", \"india\", and \"bjp\". For additional context, \"bjp\" referes to the _Bharatiya Janata Party_ which is a conservative political party in India, and one of the two major Indian political parties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b775494-3472-4089-8d86-e24346616155",
   "metadata": {},
   "source": [
    "Now, looking at the least popular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f91a0d-dae1-41af-9f92-c06a973ebbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.feature_names[freq_order[-50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5478a6-4ff0-4d64-a329-11278fe4e60a",
   "metadata": {},
   "source": [
    "We still see that the themes mentioned in the most frequent terms are still present in this subset. Although, more filler or non-distinct words do appear more often, like \"photos\", \"soft\" and \"types\".\n",
    "\n",
    "But the present of words like \"reelection\" and \"wars\" still point to this subset still being relevant to the main theme of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb530e2",
   "metadata": {},
   "source": [
    "# **4 exploratory data analysis**\n",
    "\n",
    "This section discusses the exploratory data analysis conducted on the dataset after cleaning.\n",
    "\n",
    "> Notes from Zhean: <br>\n",
    "> From manual checking via OpenRefine, there are a total of 162972. `df.info()` should have the same result post-processing.\n",
    "> Furthermore, there should be two columns, `clean_text` (which is a bit of a misnormer since it is still dirty) contains the Tweets (text data). The second column is the `category` which contains the sentiment of the Tweet and is a tribool (1 positive, 0 neutral or indeterminate, and -1 for negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ad21e-cb7b-4d91-87f9-a14c15ec8365",
   "metadata": {},
   "source": [
    "# **references**\n",
    "Bird, S., & Loper, E. (2004, July). NLTK: The natural language toolkit. *Proceedings of the ACL Interactive Poster and Demonstration Sessions*, 214‚Äì217. https://aclanthology.org/P04-3031/\n",
    "\n",
    "El-Demerdash, A. A., Hussein, S. E., & Zaki, J. F. W. (2021). Course evaluation based on deep learning and SSA hyperparameters optimization. *Computers, Materials & Continua, 71*(1), 941‚Äì959. https://doi.org/10.32604/cmc.2022.021839\n",
    "\n",
    "George, M., & Murugesan, R. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. *Procedia Computer Science, 244*, 1‚Äì8.\n",
    "\n",
    "Hussein, S. (2021). *Twitter sentiments dataset*. Mendeley.\n",
    "\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research, 12*, 2825‚Äì2830.\n",
    "\n",
    "Rani, D., Kumar, R., & Chauhan, N. (2022, October). Study and comparison of vectorization techniques used in text classification. In *2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)* (pp. 1‚Äì6). IEEE.\n",
    "\n",
    "Wolfram Research. (2015). *DeleteStopwords*. https://reference.wolfram.com/language/ref/DeleteStopwords.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
