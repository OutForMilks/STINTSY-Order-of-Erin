{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44906e4e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Data Description, Cleaning, Exploratory Data Analysis, and Preprocessing</b></center>\n",
    "<br>\n",
    "\n",
    "**By**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**Dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**Motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general populationâ€™s opinions and reactions.\n",
    "\n",
    "**Goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis.\n",
    "\n",
    "### **Dataset Description**\n",
    "\n",
    "The Twitter Sentiments Dataset is a dataset that contains nearly 163k tweets from Twitter. The time period of when these were collected is unknown, but it was published to Mendeley Data on May 14, 2021 by Sherif Hussein of Mansoura University.\n",
    "\n",
    "Tweets were extracted using the Twitter API, but the specifics of how the tweets were selected are unmentioned. The tweets are mostly English with a mix of some Hindi words for code-switching<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1). All of them seem to be talking about the political state of India. Most tweets mention Narendra Modi, the current Prime Minister of India.\n",
    "\n",
    "Each tweet was assigned a label using TextBlob's sentiment analysis (Elâ€‘Demerdash, Hussein, & Zaki, 2021), which assigns labels automatically.\n",
    "\n",
    "Twitter_Data\n",
    "- **`clean_text`**: The tweet's text\n",
    "- **`category`**: The tweet's sentiment category\n",
    "\n",
    "What each row and column represents: `each row represents one tweet.` <br>\n",
    "Number of observations: `162,980`\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. Alaa A. El-Demerdash, J. F. W. Z., Sherif E. Hussein. (2022). Course Evaluation Based on Deep Learning and SSA Hyperparameters Optimization. Computers, Materials & Continua, 71(1), 941â€“959. doi:10.32604/cmc.2022.021839\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Code-switching is the practice of alternating between two languages $L_1$ (the native language) and $L_2$ (the source language) in a conversation. In this context, the code-switching is done to appear more casual since the conversation is done via Twitter (now, X). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47491b2f",
   "metadata": {},
   "source": [
    "## 1 project set up\n",
    "Set up here the imports for the projects (ensure these are installed via uv and is part of the environment). Furthermore, load the dataset here. Also load the raw dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7578d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:01:26.803271Z",
     "start_time": "2025-06-17T14:01:26.567527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/zrgnt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/zrgnt/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import boilerplate file generated by `codegen.py`\n",
    "from boilerplate import stopwords_set\n",
    "\n",
    "# Cleaning Imports\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Imports for Stemming and Lematization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Imports for Text Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Set up NLTK objects for stemming and lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Set up scikit-learn objects for text vectorization\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Load raw data file\n",
    "df = pd.read_csv(\"../data/Twitter_Data.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ec446",
   "metadata": {},
   "source": [
    "## 2 data cleaning\n",
    "This section discusses the methodology for data cleaning.\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in [1].\n",
    "\n",
    "The cleaning pipeline has four main functions. The first function is the `normalize` function, it normalizes the text input to ASCII-only characters (say, \"cÃ³mo estÃ¡s\" becomes \"como estas\") and lowercases alphabetic symbols. The dataset contains Unicode characters (e.g., emojis and accented characters) which the function replaces to the empty string (`''`). Then, `rem_punctuation` removes the punctuation marks and special characters with the empty string. Then, `collapse_whitespace` collapses all whitespace characters to a single space. Formally, it is a transducer from $\\Box^+ \\mapsto \\Box$ where $\\Box$ is the whitespace character. Now, since the strings are cleaned at this point, the tokenization step reduces to a mere string split at word boundaries. Finally, with the tokenized string, we can do a final clean by removing the stopwords.\n",
    "\n",
    "For stop words removal, we refer to the English stopwords dataset defined in NLTK and Wolfram Mathematica [2,3].\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. George, M., & Murugesan, R., Dr. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. Procedia Computer Science, 244, 1â€“8. https://doi.org/10.1016/j.procs.2024.10.172\n",
    "2. Wolfram Language. (2015). DeleteStopwords. Wolfram Language & System Documentation Center. Retrieved from https://reference.wolfram.com/language/ref/DeleteStopwords.html\n",
    "3. Bird, S., & Loper, E. (2004, July). NLTK: The Natural Language Toolkit. Proceedings of the ACL Interactive Poster and Demonstration Sessions, 214â€“217. Retrieved from https://aclanthology.org/P04-3031/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ac69746-f471-4a66-9292-c2b363d12de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text from a pandas entry to ASCII-only lowercase characters. Hence, this removes Unicode characters with no ASCII\n",
    "    equivalent (e.g., emojis and CJKs).\n",
    "\n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "    \n",
    "    # Parameters\n",
    "    * text: String entry.\n",
    "\n",
    "    # Returns\n",
    "    ASCII-normalized text containing only lowercase letters.\n",
    "\n",
    "    # Examples\n",
    "    normalize(\"Â¿CÃ³mo estÃ¡s?\")\n",
    "    $ 'como estas?'\n",
    "\n",
    "    normalize(\" hahahaha HUY! Kamusta ðŸ˜… Mayaman $$$ ka na ba?\") \n",
    "    $ ' hahahaha huy! kamusta  mayaman $$$ ka na ba?'\n",
    "    \"\"\"\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    ascii_text = normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    return ascii_text.lower()\n",
    "\n",
    "\n",
    "def rem_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes the punctuations. This function simply replaces all punctuation marks and special characters\n",
    "    to the empty string. Hence, for symbols enclosed by whitespace, the whitespace are not collapsed to a single whitespace\n",
    "    (for more information, see the examples).\n",
    "\n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "    \n",
    "    # Parameters\n",
    "    * text: String entry.\n",
    "\n",
    "    # Returns\n",
    "    Text with the punctuation removed, or None if the result is empty or input invalid.\n",
    "\n",
    "    # Examples\n",
    "    rem_punctuation(\"this word $$ has two spaces after it!\")\n",
    "    $ 'this word  has two spaces after it'\n",
    "\n",
    "    rem_punctuation(\"these!words@have$no%space\")\n",
    "    $ 'thesewordshavenospace'\n",
    "    \"\"\"\n",
    "    return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "\n",
    "\n",
    "def collapse_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    This collapses whitespace. Here, collapsing means the transduction of all whitespace strings of any\n",
    "    length to a whitespace string of unit length (e.g., \"   \" -> \" \"; formally \" \"+ -> \" \").\n",
    "    \n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "\n",
    "    # Parameters\n",
    "    * text: String entry.\n",
    "\n",
    "    # Returns\n",
    "    Text with the whitespaces collapsed.\n",
    "\n",
    "    # Examples\n",
    "    collapse_whitespace(\"  huh,  was.  that!!! \")\n",
    "    $ 'huh, was. that!!!'\n",
    "    \"\"\"\n",
    "    return re.sub(f\" +\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def rem_stopwords(tokens: str, stopwords: set[str]) -> str:\n",
    "    \"\"\"\n",
    "    This removes all stopwords. For fast look up, `stopwords` is a set with type str.\n",
    "    This allows for constant time lookups as opposed to a linear search with a list.\n",
    "\n",
    "    Strings detected as stopwords is replaced with the empty string \"\".\n",
    "\n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "\n",
    "    # Parameters\n",
    "    * text: A tokenized string. \n",
    "    * stopwords: stopword dictionary.\n",
    "    \n",
    "    # Returns\n",
    "    Text with the stopwords removed.\n",
    "\n",
    "    # Examples\n",
    "    rem_stopwords([\"he\", \"is\", \"an\", \"amazing\", \"master\",], stopwords_lut)\n",
    "    $ ['amazing', 'master']\n",
    "\n",
    "    # Future\n",
    "    If this function is too slow, we may implement `stopwords` as an 26-ary trie to achieve log-linear time.\n",
    "    \"\"\"\n",
    "    filtered = [word for word in tokens if word not in stopwords]\n",
    "    return filtered\n",
    "    \n",
    "def clean_and_tokenize(text: str, stopwords: set[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    This is the main function for data cleaning (i.e., it calls all the cleaning functions in the prescribed order).\n",
    "\n",
    "    This function should be used as a first-class function in a map.\n",
    "\n",
    "    # Parameters\n",
    "    * text: The string entry from a DataFrame column.\n",
    "    * stopwords: stopword dictionary.\n",
    "\n",
    "    # Returns\n",
    "    Clean tokenized string. \n",
    "    \"\"\"\n",
    "    # cleaning on the base string\n",
    "    text = normalize(text)\n",
    "    text = rem_punctuation(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    \n",
    "    # tokenization is now trivial since the cleaning steps allow the tokenization to be a mere word boundary split\n",
    "    toks = text.split()\n",
    "\n",
    "    # cleaning on the tokenized string\n",
    "    toks = rem_stopwords(toks, stopwords)\n",
    "\n",
    "    return toks\n",
    "\n",
    "# perform cleaning stage\n",
    "# at this point NaN entries shouldn't exist\n",
    "df[\"clean_ours\"] = df['clean_text'].map(lambda x: clean_and_tokenize(x, stopwords_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb4d20",
   "metadata": {},
   "source": [
    "# 3 preprocessing\n",
    "\n",
    "> ðŸ—ï¸ Perhaps swap S3 and S4. Refer to literature on what comes first.\n",
    "\n",
    "This section discusses preprocessing steps for the cleaned data.\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in [1]. We preprocess the dataset entries via stemming and lemmatization. We employ NLTK for both tasks using PorterStemmer and WordNetLemmatizer for stemming and lemmatization, repectively [2]. For the lemmatization step, we use the WordNet for English lemmatization and Open Multilingual WordNet version 1.4 for translations and multilingual support which is important for our case since some tweets contain text from Indian Languages.\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. George, M., & Murugesan, R., Dr. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. Procedia Computer Science, 244, 1â€“8. https://doi.org/10.1016/j.procs.2024.10.172\n",
    "2. Bird, S., & Loper, E. (2004, July). NLTK: The Natural Language Toolkit. Proceedings of the ACL Interactive Poster and Demonstration Sessions, 214â€“217. Retrieved from https://aclanthology.org/P04-3031/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6190c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stemmed\"] = df[\"clean_ours\"].map(lambda tokens: [stemmer.stem(t) for t in tokens])\n",
    "df[\"lemmatized\"] = df[\"clean_ours\"].map(lambda tokens: [lemmatizer.lemmatize(t) for t in tokens])\n",
    "\n",
    "# do this, since vectorizer expects a string not an array of strings\n",
    "df[\"lemmatized_str\"] = df[\"lemmatized\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64d042-e337-4d55-a6e4-0d065abd2738",
   "metadata": {},
   "source": [
    "## Vector Representation\n",
    "\n",
    "After to stemming and lemmatization steps, we can now represent each each entry to its vector representation from a Bag of Words (BoW) model. We use scikit-learn's `CountVectorizer` which is an ready-to-use implementation of the BoW model.\n",
    "\n",
    "**Related Literature and Vectorization Techniques.** Traditional vectorization techniques include BoW and Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF weights each word based on its frequency in a document and how rare it is across the corpus, reducing the impact of common words. BoW, in contrast, simply counts word occurrences without considering corpus-level frequency. In this project, BoW was chosen because stopwords were already removed during preprocessing, and the dataset is domain-specific [3]. In such datasets, frequent words are often meaningful domain keywords, so scaling them down (as TF-IDF would) could reduce the importance of these key terms in the feature representation.\n",
    "\n",
    "The resulting vector has 162,969 rows which is the number of entries in the dataset (with `NaN` entries removed), and 101,284 which is the number of unique words in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**References**\n",
    "1. Rani, D., Kumar, R., & Chauhan, N. (2022, October). Study and comparision of vectorization techniques used in text classification. In 2022 13th international conference on computing communication and networking technologies (ICCCNT) (pp. 1-6). IEEE.\n",
    "2. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, Îˆ. (2011). Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res., 12(null), 2825â€“2830."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bfba459-6837-4481-929c-ef5ee023b760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business', np.int64(1)),\n",
       " ('difficult', np.int64(1)),\n",
       " ('exit', np.int64(1)),\n",
       " ('expected', np.int64(1)),\n",
       " ('governance', np.int64(1)),\n",
       " ('government', np.int64(1)),\n",
       " ('job', np.int64(1)),\n",
       " ('justice', np.int64(1)),\n",
       " ('maximum', np.int64(1)),\n",
       " ('minimum', np.int64(1)),\n",
       " ('modi', np.int64(1)),\n",
       " ('promised', np.int64(1)),\n",
       " ('psus', np.int64(1)),\n",
       " ('reforming', np.int64(1)),\n",
       " ('state', np.int64(2))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = vectorizer.fit_transform(df[\"lemmatized_str\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# testing\n",
    "sentence = \"when modi promised â€œminimum government maximum governanceâ€ expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\"\n",
    "sentence = clean_and_tokenize(sentence, stopwords_set)\n",
    "sentence = \" \".join(sentence)\n",
    "vec = vectorizer.transform([sentence])\n",
    "\n",
    "nonzero_idx = vec.nonzero()[1]\n",
    "list(zip(feature_names[nonzero_idx], vec.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb530e2",
   "metadata": {},
   "source": [
    "# 4 exploratory data analysis\n",
    "\n",
    "This section discusses the exploratory data analysis conducted on the dataset after cleaning.\n",
    "\n",
    "> Notes from Zhean: <br>\n",
    "> From manual checking via OpenRefine, there are a total of 162972. `df.info()` should have the same result post-processing.\n",
    "> Furthermore, there should be two columns, `clean_text` (which is a bit of a misnormer since it is still dirty) contains the Tweets (text data). The second column is the `category` which contains the sentiment of the Tweet and is a tribool (1 positive, 0 neutral or indeterminate, and -1 for negative).\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
