{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44906e4e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Data Description, Cleaning, Exploratory Data Analysis, and Preprocessing</b></center>\n",
    "<br>\n",
    "\n",
    "**By**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**Dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**Motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general populationâ€™s opinions and reactions.\n",
    "\n",
    "**Goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis.\n",
    "\n",
    "### **Dataset Description**\n",
    "\n",
    "The Twitter Sentiments Dataset is a dataset that contains nearly 163k tweets from Twitter. The time period of when these were collected is unknown, but it was published to Mendeley Data on May 14, 2021 by Sherif Hussein of Mansoura University.\n",
    "\n",
    "Tweets were extracted using the Twitter API, but the specifics of how the tweets were selected are unmentioned. The tweets are mostly English with a mix of some Hindi words for code-switching<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1). All of them seem to be talking about the political state of India. Most tweets mention Narendra Modi, the current Prime Minister of India.\n",
    "\n",
    "Each tweet was assigned a label using TextBlob's sentiment analysis (Elâ€‘Demerdash, Hussein, & Zaki, 2021), which assigns labels automatically.\n",
    "\n",
    "What each row and column represents: `each row represents one tweet.`\n",
    "Number of observations: `162,980`\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. Alaa A. El-Demerdash, J. F. W. Z., Sherif E. Hussein. (2022). Course Evaluation Based on Deep Learning and SSA Hyperparameters Optimization. Computers, Materials & Continua, 71(1), 941â€“959. doi:10.32604/cmc.2022.021839\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Code-switching is the practice of alternating between two languages $L_1$ (the native language) and $L_2$ (the source language) in a conversation. In this context, the code-switching is done to appear more casual since the conversation is done via Twitter (now, X). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed82ded3d24a0cb",
   "metadata": {},
   "source": [
    "Twitter_Data\n",
    "- **`clean_text`**: The tweet's text\n",
    "- **`category`**: The tweet's sentiment category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47491b2f",
   "metadata": {},
   "source": [
    "## 1 project set up\n",
    "Set up here the imports for the projects (ensure these are installed via uv and is part of the environment). Furthermore, load the dataset here. Also load the raw dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d7578d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:01:26.803271Z",
     "start_time": "2025-06-17T14:01:26.567527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162980 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162976 non-null  object \n",
      " 1   category    162973 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# no need to import `torch` here since no training occurs yet.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load raw data file\n",
    "df = pd.read_csv(\"../data/Twitter_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ec446",
   "metadata": {},
   "source": [
    "## 2 data cleaning\n",
    "This section discusses the methodology for data cleaning.\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in [1].\n",
    "\n",
    "The cleaning pipeline has four main functions. The first function is the `normalize` function, it normalizes the text input to ASCII-only characters (say, \"cÃ³mo estÃ¡s\" becomes \"como estas\") and lowercases alphabetic symbols. The dataset contains Unicode characters (e.g., emojis and accented characters) which the function replaces to the empty string (`''`). Then, `rem_punctuation` removes the punctuation marks and special characters with the empty string. Then, `collapse_whitespace` collapses all whitespace characters to a single space. Formally, it is a transducer from $\\Box^+ \\mapsto \\Box$ where $\\Box$ is the whitespace character. Now, since the strings are cleaned at this point, the tokenization step reduces to a mere string split at word boundaries. Finally, with the tokenized string, we can do a final clean by removing the stopwords.\n",
    "\n",
    "For stop words removal, we refer to the English stopwords dataset defined in NLTK and Wolfram Mathematica [2,3].\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. George, M., & Murugesan, R., Dr. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. Procedia Computer Science, 244, 1â€“8. https://doi.org/10.1016/j.procs.2024.10.172\n",
    "2. Wolfram Language. (2015). DeleteStopwords. Wolfram Language & System Documentation Center. Retrieved from https://reference.wolfram.com/language/ref/DeleteStopwords.html\n",
    "3. Bird, S., & Loper, E. (2004, July). NLTK: The Natural Language Toolkit. Proceedings of the ACL Interactive Poster and Demonstration Sessions, 214â€“217. Retrieved from https://aclanthology.org/P04-3031/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37e3cddd-9195-49b1-9ad9-a67f30e3a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = {\n",
    "\t'0o',\n",
    "\t'0s',\n",
    "\t'3a',\n",
    "\t'3b',\n",
    "\t'3d',\n",
    "\t'6b',\n",
    "\t'6o',\n",
    "\t'a',\n",
    "\t'a1',\n",
    "\t'a2',\n",
    "\t'a3',\n",
    "\t'a4',\n",
    "\t'ab',\n",
    "\t'able',\n",
    "\t'about',\n",
    "\t'above',\n",
    "\t'abst',\n",
    "\t'ac',\n",
    "\t'accordance',\n",
    "\t'according',\n",
    "\t'accordingly',\n",
    "\t'across',\n",
    "\t'act',\n",
    "\t'actually',\n",
    "\t'ad',\n",
    "\t'added',\n",
    "\t'adj',\n",
    "\t'ae',\n",
    "\t'af',\n",
    "\t'affected',\n",
    "\t'affecting',\n",
    "\t'affects',\n",
    "\t'after',\n",
    "\t'afterwards',\n",
    "\t'ag',\n",
    "\t'again',\n",
    "\t'against',\n",
    "\t'ah',\n",
    "\t'ain',\n",
    "\t'aint',\n",
    "\t'aj',\n",
    "\t'al',\n",
    "\t'all',\n",
    "\t'allow',\n",
    "\t'allows',\n",
    "\t'almost',\n",
    "\t'alone',\n",
    "\t'along',\n",
    "\t'already',\n",
    "\t'also',\n",
    "\t'although',\n",
    "\t'always',\n",
    "\t'am',\n",
    "\t'among',\n",
    "\t'amongst',\n",
    "\t'amoungst',\n",
    "\t'amount',\n",
    "\t'an',\n",
    "\t'and',\n",
    "\t'announce',\n",
    "\t'another',\n",
    "\t'any',\n",
    "\t'anybody',\n",
    "\t'anyhow',\n",
    "\t'anymore',\n",
    "\t'anyone',\n",
    "\t'anything',\n",
    "\t'anyway',\n",
    "\t'anyways',\n",
    "\t'anywhere',\n",
    "\t'ao',\n",
    "\t'ap',\n",
    "\t'apart',\n",
    "\t'apparently',\n",
    "\t'appear',\n",
    "\t'appreciate',\n",
    "\t'appropriate',\n",
    "\t'approximately',\n",
    "\t'ar',\n",
    "\t'are',\n",
    "\t'aren',\n",
    "\t'arent',\n",
    "\t'arent',\n",
    "\t'arise',\n",
    "\t'around',\n",
    "\t'as',\n",
    "\t'as',\n",
    "\t'aside',\n",
    "\t'ask',\n",
    "\t'asking',\n",
    "\t'associated',\n",
    "\t'at',\n",
    "\t'au',\n",
    "\t'auth',\n",
    "\t'av',\n",
    "\t'available',\n",
    "\t'aw',\n",
    "\t'away',\n",
    "\t'awfully',\n",
    "\t'ax',\n",
    "\t'ay',\n",
    "\t'az',\n",
    "\t'b',\n",
    "\t'b1',\n",
    "\t'b2',\n",
    "\t'b3',\n",
    "\t'ba',\n",
    "\t'back',\n",
    "\t'bc',\n",
    "\t'bd',\n",
    "\t'be',\n",
    "\t'became',\n",
    "\t'because',\n",
    "\t'become',\n",
    "\t'becomes',\n",
    "\t'becoming',\n",
    "\t'been',\n",
    "\t'before',\n",
    "\t'beforehand',\n",
    "\t'begin',\n",
    "\t'beginning',\n",
    "\t'beginnings',\n",
    "\t'begins',\n",
    "\t'behind',\n",
    "\t'being',\n",
    "\t'believe',\n",
    "\t'below',\n",
    "\t'beside',\n",
    "\t'besides',\n",
    "\t'best',\n",
    "\t'better',\n",
    "\t'between',\n",
    "\t'beyond',\n",
    "\t'bi',\n",
    "\t'bill',\n",
    "\t'biol',\n",
    "\t'bj',\n",
    "\t'bk',\n",
    "\t'bl',\n",
    "\t'bn',\n",
    "\t'both',\n",
    "\t'bottom',\n",
    "\t'bp',\n",
    "\t'br',\n",
    "\t'brief',\n",
    "\t'briefly',\n",
    "\t'bs',\n",
    "\t'bt',\n",
    "\t'bu',\n",
    "\t'but',\n",
    "\t'bx',\n",
    "\t'by',\n",
    "\t'c',\n",
    "\t'c1',\n",
    "\t'c2',\n",
    "\t'c3',\n",
    "\t'ca',\n",
    "\t'call',\n",
    "\t'came',\n",
    "\t'can',\n",
    "\t'cannot',\n",
    "\t'cant',\n",
    "\t'cant',\n",
    "\t'cause',\n",
    "\t'causes',\n",
    "\t'cc',\n",
    "\t'cd',\n",
    "\t'ce',\n",
    "\t'certain',\n",
    "\t'certainly',\n",
    "\t'cf',\n",
    "\t'cg',\n",
    "\t'ch',\n",
    "\t'changes',\n",
    "\t'ci',\n",
    "\t'cit',\n",
    "\t'cj',\n",
    "\t'cl',\n",
    "\t'clearly',\n",
    "\t'cm',\n",
    "\t'cmon',\n",
    "\t'cn',\n",
    "\t'co',\n",
    "\t'com',\n",
    "\t'come',\n",
    "\t'comes',\n",
    "\t'con',\n",
    "\t'concerning',\n",
    "\t'consequently',\n",
    "\t'consider',\n",
    "\t'considering',\n",
    "\t'contain',\n",
    "\t'containing',\n",
    "\t'contains',\n",
    "\t'corresponding',\n",
    "\t'could',\n",
    "\t'couldn',\n",
    "\t'couldnt',\n",
    "\t'couldnt',\n",
    "\t'course',\n",
    "\t'cp',\n",
    "\t'cq',\n",
    "\t'cr',\n",
    "\t'cry',\n",
    "\t'cs',\n",
    "\t'cs',\n",
    "\t'ct',\n",
    "\t'cu',\n",
    "\t'currently',\n",
    "\t'cv',\n",
    "\t'cx',\n",
    "\t'cy',\n",
    "\t'cz',\n",
    "\t'd',\n",
    "\t'd2',\n",
    "\t'da',\n",
    "\t'date',\n",
    "\t'dc',\n",
    "\t'dd',\n",
    "\t'de',\n",
    "\t'definitely',\n",
    "\t'describe',\n",
    "\t'described',\n",
    "\t'despite',\n",
    "\t'detail',\n",
    "\t'df',\n",
    "\t'di',\n",
    "\t'did',\n",
    "\t'didn',\n",
    "\t'didnt',\n",
    "\t'different',\n",
    "\t'dj',\n",
    "\t'dk',\n",
    "\t'dl',\n",
    "\t'do',\n",
    "\t'does',\n",
    "\t'doesn',\n",
    "\t'doesnt',\n",
    "\t'doing',\n",
    "\t'don',\n",
    "\t'done',\n",
    "\t'dont',\n",
    "\t'down',\n",
    "\t'downwards',\n",
    "\t'dp',\n",
    "\t'dr',\n",
    "\t'ds',\n",
    "\t'dt',\n",
    "\t'du',\n",
    "\t'due',\n",
    "\t'during',\n",
    "\t'dx',\n",
    "\t'dy',\n",
    "\t'e',\n",
    "\t'e2',\n",
    "\t'e3',\n",
    "\t'ea',\n",
    "\t'each',\n",
    "\t'ec',\n",
    "\t'ed',\n",
    "\t'edu',\n",
    "\t'ee',\n",
    "\t'ef',\n",
    "\t'effect',\n",
    "\t'eg',\n",
    "\t'ei',\n",
    "\t'eight',\n",
    "\t'eighty',\n",
    "\t'either',\n",
    "\t'ej',\n",
    "\t'el',\n",
    "\t'eleven',\n",
    "\t'else',\n",
    "\t'elsewhere',\n",
    "\t'em',\n",
    "\t'empty',\n",
    "\t'en',\n",
    "\t'end',\n",
    "\t'ending',\n",
    "\t'enough',\n",
    "\t'entirely',\n",
    "\t'eo',\n",
    "\t'ep',\n",
    "\t'eq',\n",
    "\t'er',\n",
    "\t'es',\n",
    "\t'especially',\n",
    "\t'est',\n",
    "\t'et',\n",
    "\t'et-al',\n",
    "\t'etc',\n",
    "\t'eu',\n",
    "\t'ev',\n",
    "\t'even',\n",
    "\t'ever',\n",
    "\t'every',\n",
    "\t'everybody',\n",
    "\t'everyone',\n",
    "\t'everything',\n",
    "\t'everywhere',\n",
    "\t'ex',\n",
    "\t'exactly',\n",
    "\t'example',\n",
    "\t'except',\n",
    "\t'ey',\n",
    "\t'f',\n",
    "\t'f2',\n",
    "\t'fa',\n",
    "\t'far',\n",
    "\t'fc',\n",
    "\t'few',\n",
    "\t'ff',\n",
    "\t'fi',\n",
    "\t'fifteen',\n",
    "\t'fifth',\n",
    "\t'fify',\n",
    "\t'fill',\n",
    "\t'find',\n",
    "\t'fire',\n",
    "\t'first',\n",
    "\t'five',\n",
    "\t'fix',\n",
    "\t'fj',\n",
    "\t'fl',\n",
    "\t'fn',\n",
    "\t'fo',\n",
    "\t'followed',\n",
    "\t'following',\n",
    "\t'follows',\n",
    "\t'for',\n",
    "\t'former',\n",
    "\t'formerly',\n",
    "\t'forth',\n",
    "\t'forty',\n",
    "\t'found',\n",
    "\t'four',\n",
    "\t'fr',\n",
    "\t'from',\n",
    "\t'front',\n",
    "\t'fs',\n",
    "\t'ft',\n",
    "\t'fu',\n",
    "\t'full',\n",
    "\t'further',\n",
    "\t'furthermore',\n",
    "\t'fy',\n",
    "\t'g',\n",
    "\t'ga',\n",
    "\t'gave',\n",
    "\t'ge',\n",
    "\t'get',\n",
    "\t'gets',\n",
    "\t'getting',\n",
    "\t'gi',\n",
    "\t'give',\n",
    "\t'given',\n",
    "\t'gives',\n",
    "\t'giving',\n",
    "\t'gj',\n",
    "\t'gl',\n",
    "\t'go',\n",
    "\t'goes',\n",
    "\t'going',\n",
    "\t'gone',\n",
    "\t'got',\n",
    "\t'gotten',\n",
    "\t'gr',\n",
    "\t'greetings',\n",
    "\t'gs',\n",
    "\t'gy',\n",
    "\t'h',\n",
    "\t'h2',\n",
    "\t'h3',\n",
    "\t'had',\n",
    "\t'hadn',\n",
    "\t'hadnt',\n",
    "\t'happens',\n",
    "\t'hardly',\n",
    "\t'has',\n",
    "\t'hasn',\n",
    "\t'hasnt',\n",
    "\t'hasnt',\n",
    "\t'have',\n",
    "\t'haven',\n",
    "\t'havent',\n",
    "\t'having',\n",
    "\t'he',\n",
    "\t'hed',\n",
    "\t'hed',\n",
    "\t'hell',\n",
    "\t'hello',\n",
    "\t'help',\n",
    "\t'hence',\n",
    "\t'her',\n",
    "\t'here',\n",
    "\t'hereafter',\n",
    "\t'hereby',\n",
    "\t'herein',\n",
    "\t'heres',\n",
    "\t'heres',\n",
    "\t'hereupon',\n",
    "\t'hers',\n",
    "\t'herself',\n",
    "\t'hes',\n",
    "\t'hes',\n",
    "\t'hh',\n",
    "\t'hi',\n",
    "\t'hid',\n",
    "\t'him',\n",
    "\t'himself',\n",
    "\t'his',\n",
    "\t'hither',\n",
    "\t'hj',\n",
    "\t'ho',\n",
    "\t'home',\n",
    "\t'hopefully',\n",
    "\t'how',\n",
    "\t'howbeit',\n",
    "\t'however',\n",
    "\t'hows',\n",
    "\t'hr',\n",
    "\t'hs',\n",
    "\t'http',\n",
    "\t'hu',\n",
    "\t'hundred',\n",
    "\t'hy',\n",
    "\t'i',\n",
    "\t'i2',\n",
    "\t'i3',\n",
    "\t'i4',\n",
    "\t'i6',\n",
    "\t'i7',\n",
    "\t'i8',\n",
    "\t'ia',\n",
    "\t'ib',\n",
    "\t'ibid',\n",
    "\t'ic',\n",
    "\t'id',\n",
    "\t'id',\n",
    "\t'ie',\n",
    "\t'if',\n",
    "\t'ig',\n",
    "\t'ignored',\n",
    "\t'ih',\n",
    "\t'ii',\n",
    "\t'ij',\n",
    "\t'il',\n",
    "\t'ill',\n",
    "\t'im',\n",
    "\t'im',\n",
    "\t'immediate',\n",
    "\t'immediately',\n",
    "\t'importance',\n",
    "\t'important',\n",
    "\t'in',\n",
    "\t'inasmuch',\n",
    "\t'inc',\n",
    "\t'indeed',\n",
    "\t'index',\n",
    "\t'indicate',\n",
    "\t'indicated',\n",
    "\t'indicates',\n",
    "\t'information',\n",
    "\t'inner',\n",
    "\t'insofar',\n",
    "\t'instead',\n",
    "\t'interest',\n",
    "\t'into',\n",
    "\t'invention',\n",
    "\t'inward',\n",
    "\t'io',\n",
    "\t'ip',\n",
    "\t'iq',\n",
    "\t'ir',\n",
    "\t'is',\n",
    "\t'isn',\n",
    "\t'isnt',\n",
    "\t'it',\n",
    "\t'itd',\n",
    "\t'itd',\n",
    "\t'itll',\n",
    "\t'its',\n",
    "\t'its',\n",
    "\t'itself',\n",
    "\t'iv',\n",
    "\t'ive',\n",
    "\t'ix',\n",
    "\t'iy',\n",
    "\t'iz',\n",
    "\t'j',\n",
    "\t'jj',\n",
    "\t'jr',\n",
    "\t'js',\n",
    "\t'jt',\n",
    "\t'ju',\n",
    "\t'just',\n",
    "\t'k',\n",
    "\t'ke',\n",
    "\t'keep',\n",
    "\t'keeps',\n",
    "\t'kept',\n",
    "\t'kg',\n",
    "\t'kj',\n",
    "\t'km',\n",
    "\t'know',\n",
    "\t'known',\n",
    "\t'knows',\n",
    "\t'ko',\n",
    "\t'l',\n",
    "\t'l2',\n",
    "\t'la',\n",
    "\t'largely',\n",
    "\t'last',\n",
    "\t'lately',\n",
    "\t'later',\n",
    "\t'latter',\n",
    "\t'latterly',\n",
    "\t'lb',\n",
    "\t'lc',\n",
    "\t'le',\n",
    "\t'least',\n",
    "\t'les',\n",
    "\t'less',\n",
    "\t'lest',\n",
    "\t'let',\n",
    "\t'lets',\n",
    "\t'lets',\n",
    "\t'lf',\n",
    "\t'like',\n",
    "\t'liked',\n",
    "\t'likely',\n",
    "\t'line',\n",
    "\t'little',\n",
    "\t'lj',\n",
    "\t'll',\n",
    "\t'll',\n",
    "\t'ln',\n",
    "\t'lo',\n",
    "\t'look',\n",
    "\t'looking',\n",
    "\t'looks',\n",
    "\t'los',\n",
    "\t'lr',\n",
    "\t'ls',\n",
    "\t'lt',\n",
    "\t'ltd',\n",
    "\t'm',\n",
    "\t'm2',\n",
    "\t'ma',\n",
    "\t'made',\n",
    "\t'mainly',\n",
    "\t'make',\n",
    "\t'makes',\n",
    "\t'many',\n",
    "\t'may',\n",
    "\t'maybe',\n",
    "\t'me',\n",
    "\t'mean',\n",
    "\t'means',\n",
    "\t'meantime',\n",
    "\t'meanwhile',\n",
    "\t'merely',\n",
    "\t'mg',\n",
    "\t'might',\n",
    "\t'mightn',\n",
    "\t'mightnt',\n",
    "\t'mill',\n",
    "\t'million',\n",
    "\t'mine',\n",
    "\t'miss',\n",
    "\t'ml',\n",
    "\t'mn',\n",
    "\t'mo',\n",
    "\t'more',\n",
    "\t'moreover',\n",
    "\t'most',\n",
    "\t'mostly',\n",
    "\t'move',\n",
    "\t'mr',\n",
    "\t'mrs',\n",
    "\t'ms',\n",
    "\t'mt',\n",
    "\t'mu',\n",
    "\t'much',\n",
    "\t'mug',\n",
    "\t'must',\n",
    "\t'mustn',\n",
    "\t'mustnt',\n",
    "\t'my',\n",
    "\t'myself',\n",
    "\t'n',\n",
    "\t'n2',\n",
    "\t'na',\n",
    "\t'name',\n",
    "\t'namely',\n",
    "\t'nay',\n",
    "\t'nc',\n",
    "\t'nd',\n",
    "\t'ne',\n",
    "\t'near',\n",
    "\t'nearly',\n",
    "\t'necessarily',\n",
    "\t'necessary',\n",
    "\t'need',\n",
    "\t'needn',\n",
    "\t'neednt',\n",
    "\t'needs',\n",
    "\t'neither',\n",
    "\t'never',\n",
    "\t'nevertheless',\n",
    "\t'new',\n",
    "\t'next',\n",
    "\t'ng',\n",
    "\t'ni',\n",
    "\t'nine',\n",
    "\t'ninety',\n",
    "\t'nj',\n",
    "\t'nl',\n",
    "\t'nn',\n",
    "\t'no',\n",
    "\t'nobody',\n",
    "\t'non',\n",
    "\t'none',\n",
    "\t'nonetheless',\n",
    "\t'noone',\n",
    "\t'nor',\n",
    "\t'normally',\n",
    "\t'nos',\n",
    "\t'not',\n",
    "\t'noted',\n",
    "\t'nothing',\n",
    "\t'novel',\n",
    "\t'now',\n",
    "\t'nowhere',\n",
    "\t'nr',\n",
    "\t'ns',\n",
    "\t'nt',\n",
    "\t'ny',\n",
    "\t'o',\n",
    "\t'oa',\n",
    "\t'ob',\n",
    "\t'obtain',\n",
    "\t'obtained',\n",
    "\t'obviously',\n",
    "\t'oc',\n",
    "\t'od',\n",
    "\t'of',\n",
    "\t'off',\n",
    "\t'often',\n",
    "\t'og',\n",
    "\t'oh',\n",
    "\t'oi',\n",
    "\t'oj',\n",
    "\t'ok',\n",
    "\t'okay',\n",
    "\t'ol',\n",
    "\t'old',\n",
    "\t'om',\n",
    "\t'omitted',\n",
    "\t'on',\n",
    "\t'once',\n",
    "\t'one',\n",
    "\t'ones',\n",
    "\t'only',\n",
    "\t'onto',\n",
    "\t'oo',\n",
    "\t'op',\n",
    "\t'oq',\n",
    "\t'or',\n",
    "\t'ord',\n",
    "\t'os',\n",
    "\t'ot',\n",
    "\t'other',\n",
    "\t'others',\n",
    "\t'otherwise',\n",
    "\t'ou',\n",
    "\t'ought',\n",
    "\t'our',\n",
    "\t'ours',\n",
    "\t'ourselves',\n",
    "\t'out',\n",
    "\t'outside',\n",
    "\t'over',\n",
    "\t'overall',\n",
    "\t'ow',\n",
    "\t'owing',\n",
    "\t'own',\n",
    "\t'ox',\n",
    "\t'oz',\n",
    "\t'p',\n",
    "\t'p1',\n",
    "\t'p2',\n",
    "\t'p3',\n",
    "\t'page',\n",
    "\t'pagecount',\n",
    "\t'pages',\n",
    "\t'par',\n",
    "\t'part',\n",
    "\t'particular',\n",
    "\t'particularly',\n",
    "\t'pas',\n",
    "\t'past',\n",
    "\t'pc',\n",
    "\t'pd',\n",
    "\t'pe',\n",
    "\t'per',\n",
    "\t'perhaps',\n",
    "\t'pf',\n",
    "\t'ph',\n",
    "\t'pi',\n",
    "\t'pj',\n",
    "\t'pk',\n",
    "\t'pl',\n",
    "\t'placed',\n",
    "\t'please',\n",
    "\t'plus',\n",
    "\t'pm',\n",
    "\t'pn',\n",
    "\t'po',\n",
    "\t'poorly',\n",
    "\t'possible',\n",
    "\t'possibly',\n",
    "\t'potentially',\n",
    "\t'pp',\n",
    "\t'pq',\n",
    "\t'pr',\n",
    "\t'predominantly',\n",
    "\t'present',\n",
    "\t'presumably',\n",
    "\t'previously',\n",
    "\t'primarily',\n",
    "\t'probably',\n",
    "\t'promptly',\n",
    "\t'proud',\n",
    "\t'provides',\n",
    "\t'ps',\n",
    "\t'pt',\n",
    "\t'pu',\n",
    "\t'put',\n",
    "\t'py',\n",
    "\t'q',\n",
    "\t'qj',\n",
    "\t'qu',\n",
    "\t'que',\n",
    "\t'quickly',\n",
    "\t'quite',\n",
    "\t'qv',\n",
    "\t'r',\n",
    "\t'r2',\n",
    "\t'ra',\n",
    "\t'ran',\n",
    "\t'rather',\n",
    "\t'rc',\n",
    "\t'rd',\n",
    "\t're',\n",
    "\t'readily',\n",
    "\t'really',\n",
    "\t'reasonably',\n",
    "\t'recent',\n",
    "\t'recently',\n",
    "\t'ref',\n",
    "\t'refs',\n",
    "\t'regarding',\n",
    "\t'regardless',\n",
    "\t'regards',\n",
    "\t'related',\n",
    "\t'relatively',\n",
    "\t'research',\n",
    "\t'research-articl',\n",
    "\t'respectively',\n",
    "\t'resulted',\n",
    "\t'resulting',\n",
    "\t'results',\n",
    "\t'rf',\n",
    "\t'rh',\n",
    "\t'ri',\n",
    "\t'right',\n",
    "\t'rj',\n",
    "\t'rl',\n",
    "\t'rm',\n",
    "\t'rn',\n",
    "\t'ro',\n",
    "\t'rq',\n",
    "\t'rr',\n",
    "\t'rs',\n",
    "\t'rt',\n",
    "\t'ru',\n",
    "\t'run',\n",
    "\t'rv',\n",
    "\t'ry',\n",
    "\t's',\n",
    "\t's2',\n",
    "\t'sa',\n",
    "\t'said',\n",
    "\t'same',\n",
    "\t'saw',\n",
    "\t'say',\n",
    "\t'saying',\n",
    "\t'says',\n",
    "\t'sc',\n",
    "\t'sd',\n",
    "\t'se',\n",
    "\t'sec',\n",
    "\t'second',\n",
    "\t'secondly',\n",
    "\t'section',\n",
    "\t'see',\n",
    "\t'seeing',\n",
    "\t'seem',\n",
    "\t'seemed',\n",
    "\t'seeming',\n",
    "\t'seems',\n",
    "\t'seen',\n",
    "\t'self',\n",
    "\t'selves',\n",
    "\t'sensible',\n",
    "\t'sent',\n",
    "\t'serious',\n",
    "\t'seriously',\n",
    "\t'seven',\n",
    "\t'several',\n",
    "\t'sf',\n",
    "\t'shall',\n",
    "\t'shan',\n",
    "\t'shant',\n",
    "\t'she',\n",
    "\t'shed',\n",
    "\t'shed',\n",
    "\t'shell',\n",
    "\t'shes',\n",
    "\t'shes',\n",
    "\t'should',\n",
    "\t'shouldn',\n",
    "\t'shouldnt',\n",
    "\t'shouldve',\n",
    "\t'show',\n",
    "\t'showed',\n",
    "\t'shown',\n",
    "\t'showns',\n",
    "\t'shows',\n",
    "\t'si',\n",
    "\t'side',\n",
    "\t'significant',\n",
    "\t'significantly',\n",
    "\t'similar',\n",
    "\t'similarly',\n",
    "\t'since',\n",
    "\t'sincere',\n",
    "\t'six',\n",
    "\t'sixty',\n",
    "\t'sj',\n",
    "\t'sl',\n",
    "\t'slightly',\n",
    "\t'sm',\n",
    "\t'sn',\n",
    "\t'so',\n",
    "\t'some',\n",
    "\t'somebody',\n",
    "\t'somehow',\n",
    "\t'someone',\n",
    "\t'somethan',\n",
    "\t'something',\n",
    "\t'sometime',\n",
    "\t'sometimes',\n",
    "\t'somewhat',\n",
    "\t'somewhere',\n",
    "\t'soon',\n",
    "\t'sorry',\n",
    "\t'sp',\n",
    "\t'specifically',\n",
    "\t'specified',\n",
    "\t'specify',\n",
    "\t'specifying',\n",
    "\t'sq',\n",
    "\t'sr',\n",
    "\t'ss',\n",
    "\t'st',\n",
    "\t'still',\n",
    "\t'stop',\n",
    "\t'strongly',\n",
    "\t'sub',\n",
    "\t'substantially',\n",
    "\t'successfully',\n",
    "\t'such',\n",
    "\t'sufficiently',\n",
    "\t'suggest',\n",
    "\t'sup',\n",
    "\t'sure',\n",
    "\t'sy',\n",
    "\t'system',\n",
    "\t'sz',\n",
    "\t't',\n",
    "\t't1',\n",
    "\t't2',\n",
    "\t't3',\n",
    "\t'take',\n",
    "\t'taken',\n",
    "\t'taking',\n",
    "\t'tb',\n",
    "\t'tc',\n",
    "\t'td',\n",
    "\t'te',\n",
    "\t'tell',\n",
    "\t'ten',\n",
    "\t'tends',\n",
    "\t'tf',\n",
    "\t'th',\n",
    "\t'than',\n",
    "\t'thank',\n",
    "\t'thanks',\n",
    "\t'thanx',\n",
    "\t'that',\n",
    "\t'thatll',\n",
    "\t'thats',\n",
    "\t'thats',\n",
    "\t'thatve',\n",
    "\t'the',\n",
    "\t'their',\n",
    "\t'theirs',\n",
    "\t'them',\n",
    "\t'themselves',\n",
    "\t'then',\n",
    "\t'thence',\n",
    "\t'there',\n",
    "\t'thereafter',\n",
    "\t'thereby',\n",
    "\t'thered',\n",
    "\t'therefore',\n",
    "\t'therein',\n",
    "\t'therell',\n",
    "\t'thereof',\n",
    "\t'therere',\n",
    "\t'theres',\n",
    "\t'theres',\n",
    "\t'thereto',\n",
    "\t'thereupon',\n",
    "\t'thereve',\n",
    "\t'these',\n",
    "\t'they',\n",
    "\t'theyd',\n",
    "\t'theyd',\n",
    "\t'theyll',\n",
    "\t'theyre',\n",
    "\t'theyre',\n",
    "\t'theyve',\n",
    "\t'thickv',\n",
    "\t'thin',\n",
    "\t'think',\n",
    "\t'third',\n",
    "\t'this',\n",
    "\t'thorough',\n",
    "\t'thoroughly',\n",
    "\t'those',\n",
    "\t'thou',\n",
    "\t'though',\n",
    "\t'thoughh',\n",
    "\t'thousand',\n",
    "\t'three',\n",
    "\t'throug',\n",
    "\t'through',\n",
    "\t'throughout',\n",
    "\t'thru',\n",
    "\t'thus',\n",
    "\t'ti',\n",
    "\t'til',\n",
    "\t'tip',\n",
    "\t'tj',\n",
    "\t'tl',\n",
    "\t'tm',\n",
    "\t'tn',\n",
    "\t'to',\n",
    "\t'together',\n",
    "\t'too',\n",
    "\t'took',\n",
    "\t'top',\n",
    "\t'toward',\n",
    "\t'towards',\n",
    "\t'tp',\n",
    "\t'tq',\n",
    "\t'tr',\n",
    "\t'tried',\n",
    "\t'tries',\n",
    "\t'truly',\n",
    "\t'try',\n",
    "\t'trying',\n",
    "\t'ts',\n",
    "\t'ts',\n",
    "\t'tt',\n",
    "\t'tv',\n",
    "\t'twelve',\n",
    "\t'twenty',\n",
    "\t'twice',\n",
    "\t'two',\n",
    "\t'tx',\n",
    "\t'u',\n",
    "\t'u201d',\n",
    "\t'ue',\n",
    "\t'ui',\n",
    "\t'uj',\n",
    "\t'uk',\n",
    "\t'um',\n",
    "\t'un',\n",
    "\t'under',\n",
    "\t'unfortunately',\n",
    "\t'unless',\n",
    "\t'unlike',\n",
    "\t'unlikely',\n",
    "\t'until',\n",
    "\t'unto',\n",
    "\t'uo',\n",
    "\t'up',\n",
    "\t'upon',\n",
    "\t'ups',\n",
    "\t'ur',\n",
    "\t'us',\n",
    "\t'use',\n",
    "\t'used',\n",
    "\t'useful',\n",
    "\t'usefully',\n",
    "\t'usefulness',\n",
    "\t'uses',\n",
    "\t'using',\n",
    "\t'usually',\n",
    "\t'ut',\n",
    "\t'v',\n",
    "\t'va',\n",
    "\t'value',\n",
    "\t'various',\n",
    "\t'vd',\n",
    "\t've',\n",
    "\t've',\n",
    "\t'very',\n",
    "\t'via',\n",
    "\t'viz',\n",
    "\t'vj',\n",
    "\t'vo',\n",
    "\t'vol',\n",
    "\t'vols',\n",
    "\t'volumtype',\n",
    "\t'vq',\n",
    "\t'vs',\n",
    "\t'vt',\n",
    "\t'vu',\n",
    "\t'w',\n",
    "\t'wa',\n",
    "\t'want',\n",
    "\t'wants',\n",
    "\t'was',\n",
    "\t'wasn',\n",
    "\t'wasnt',\n",
    "\t'wasnt',\n",
    "\t'way',\n",
    "\t'we',\n",
    "\t'wed',\n",
    "\t'wed',\n",
    "\t'welcome',\n",
    "\t'well',\n",
    "\t'well',\n",
    "\t'well-b',\n",
    "\t'went',\n",
    "\t'were',\n",
    "\t'were',\n",
    "\t'weren',\n",
    "\t'werent',\n",
    "\t'werent',\n",
    "\t'weve',\n",
    "\t'what',\n",
    "\t'whatever',\n",
    "\t'whatll',\n",
    "\t'whats',\n",
    "\t'whats',\n",
    "\t'when',\n",
    "\t'whence',\n",
    "\t'whenever',\n",
    "\t'whens',\n",
    "\t'where',\n",
    "\t'whereafter',\n",
    "\t'whereas',\n",
    "\t'whereby',\n",
    "\t'wherein',\n",
    "\t'wheres',\n",
    "\t'wheres',\n",
    "\t'whereupon',\n",
    "\t'wherever',\n",
    "\t'whether',\n",
    "\t'which',\n",
    "\t'while',\n",
    "\t'whim',\n",
    "\t'whither',\n",
    "\t'who',\n",
    "\t'whod',\n",
    "\t'whoever',\n",
    "\t'whole',\n",
    "\t'wholl',\n",
    "\t'whom',\n",
    "\t'whomever',\n",
    "\t'whos',\n",
    "\t'whos',\n",
    "\t'whose',\n",
    "\t'why',\n",
    "\t'whys',\n",
    "\t'wi',\n",
    "\t'widely',\n",
    "\t'will',\n",
    "\t'willing',\n",
    "\t'wish',\n",
    "\t'with',\n",
    "\t'within',\n",
    "\t'without',\n",
    "\t'wo',\n",
    "\t'won',\n",
    "\t'wonder',\n",
    "\t'wont',\n",
    "\t'wont',\n",
    "\t'words',\n",
    "\t'world',\n",
    "\t'would',\n",
    "\t'wouldn',\n",
    "\t'wouldnt',\n",
    "\t'wouldnt',\n",
    "\t'www',\n",
    "\t'x',\n",
    "\t'x1',\n",
    "\t'x2',\n",
    "\t'x3',\n",
    "\t'xf',\n",
    "\t'xi',\n",
    "\t'xj',\n",
    "\t'xk',\n",
    "\t'xl',\n",
    "\t'xn',\n",
    "\t'xo',\n",
    "\t'xs',\n",
    "\t'xt',\n",
    "\t'xv',\n",
    "\t'xx',\n",
    "\t'y',\n",
    "\t'y2',\n",
    "\t'yes',\n",
    "\t'yet',\n",
    "\t'yj',\n",
    "\t'yl',\n",
    "\t'you',\n",
    "\t'youd',\n",
    "\t'youd',\n",
    "\t'youll',\n",
    "\t'your',\n",
    "\t'youre',\n",
    "\t'youre',\n",
    "\t'yours',\n",
    "\t'yourself',\n",
    "\t'yourselves',\n",
    "\t'youve',\n",
    "\t'yr',\n",
    "\t'ys',\n",
    "\t'yt',\n",
    "\t'z',\n",
    "\t'zero',\n",
    "\t'zi',\n",
    "\t'zz',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ac69746-f471-4a66-9292-c2b363d12de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text from a pandas entry to ASCII-only lowercase characters. Hence, this removes Unicode characters with no ASCII\n",
    "    equivalent (e.g., emojis and CJKs).\n",
    "\n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "    \n",
    "    # Parameters\n",
    "    * text: String entry.\n",
    "\n",
    "    # Returns\n",
    "    ASCII-normalized text containing only lowercase letters.\n",
    "\n",
    "    # Examples\n",
    "    normalize(\"Â¿CÃ³mo estÃ¡s?\")\n",
    "    $ 'como estas?'\n",
    "\n",
    "    normalize(\" hahahaha HUY! Kamusta ðŸ˜… Mayaman $$$ ka na ba?\") \n",
    "    $ ' hahahaha huy! kamusta  mayaman $$$ ka na ba?'\n",
    "    \"\"\"\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    ascii_text = normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    return ascii_text.lower()\n",
    "\n",
    "\n",
    "def rem_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes the punctuations. This function simply replaces all punctuation marks and special characters\n",
    "    to the empty string. Hence, for symbols enclosed by whitespace, the whitespace are not collapsed to a single whitespace\n",
    "    (for more information, see the examples).\n",
    "\n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "    \n",
    "    # Parameters\n",
    "    * text: String entry.\n",
    "\n",
    "    # Returns\n",
    "    Text with the punctuation removed, or None if the result is empty or input invalid.\n",
    "\n",
    "    # Examples\n",
    "    rem_punctuation(\"this word $$ has two spaces after it!\")\n",
    "    $ 'this word  has two spaces after it'\n",
    "\n",
    "    rem_punctuation(\"these!words@have$no%space\")\n",
    "    $ 'thesewordshavenospace'\n",
    "    \"\"\"\n",
    "    return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "\n",
    "\n",
    "def collapse_whitespace(text: str) -> str:\n",
    "    \"\"\"\n",
    "    This collapses whitespace. Here, collapsing means the transduction of all whitespace strings of any\n",
    "    length to a whitespace string of unit length (e.g., \"   \" -> \" \"; formally \" \"+ -> \" \").\n",
    "    \n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "\n",
    "    # Parameters\n",
    "    * text: String entry.\n",
    "\n",
    "    # Returns\n",
    "    Text with the whitespaces collapsed.\n",
    "\n",
    "    # Examples\n",
    "    collapse_whitespace(\"  huh,  was.  that!!! \")\n",
    "    $ 'huh, was. that!!!'\n",
    "    \"\"\"\n",
    "    return re.sub(f\" +\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def rem_stopwords(tokens: str, stopwords: set[str]) -> str:\n",
    "    \"\"\"\n",
    "    This removes all stopwords. For fast look up, `stopwords` is a set with type str.\n",
    "    This allows for constant time lookups as opposed to a linear search with a list.\n",
    "\n",
    "    Strings detected as stopwords is replaced with the empty string \"\".\n",
    "\n",
    "    Do not use this function alone, use `clean_and_tokenize()`.\n",
    "\n",
    "    # Parameters\n",
    "    * text: A tokenized string. \n",
    "    * stopwords: stopword dictionary.\n",
    "    \n",
    "    # Returns\n",
    "    Text with the stopwords removed.\n",
    "\n",
    "    # Examples\n",
    "    rem_stopwords([\"he\", \"is\", \"an\", \"amazing\", \"master\",], stopwords_lut)\n",
    "    $ ['amazing', 'master']\n",
    "\n",
    "    # Future\n",
    "    If this function is too slow, we may implement `stopwords` as an 26-ary trie to achieve log-linear time.\n",
    "    \"\"\"\n",
    "    filtered = [word for word in tokens if word not in stopwords]\n",
    "    return filtered\n",
    "    \n",
    "def clean_and_tokenize(text: str, stopwords: set[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    This is the main function for data cleaning (i.e., it calls all the cleaning functions in the prescribed order).\n",
    "\n",
    "    This function should be used as a first-class function in a map.\n",
    "\n",
    "    # Parameters\n",
    "    * text: The string entry from a DataFrame column.\n",
    "    * stopwords: stopword dictionary.\n",
    "\n",
    "    # Returns\n",
    "    Clean tokenized string. \n",
    "    \"\"\"\n",
    "    # cleaning on the base string\n",
    "    text = normalize(text)\n",
    "    text = rem_punctuation(text)\n",
    "    text = collapse_whitespace(text)\n",
    "    \n",
    "    # tokenization is now trivial since the cleaning steps allow the tokenization to be a mere word boundary split\n",
    "    toks = text.split()\n",
    "\n",
    "    # cleaning on the tokenized string\n",
    "    toks = rem_stopwords(toks, stopwords)\n",
    "\n",
    "    return toks\n",
    "\n",
    "# perform cleaning stage\n",
    "df[\"clean_text\"] = df['clean_text'].fillna(\"\")\n",
    "df[\"clean_ours\"] = df['clean_text'].map(lambda x: clean_and_tokenize(x, stopwords_set))\n",
    "\n",
    "# sanity check\n",
    "df.to_csv(\"Cleaned_Data.csv\", index=False)\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb4d20",
   "metadata": {},
   "source": [
    "# 3 preprocessing\n",
    "\n",
    "> ðŸ—ï¸ Perhaps swap S3 and S4. Refer to literature on what comes first.\n",
    "\n",
    "This section discusses preprocessing steps for the cleaned data.\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6190c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb530e2",
   "metadata": {},
   "source": [
    "# 4 exploratory data analysis\n",
    "\n",
    "This section discusses the exploratory data analysis conducted on the dataset after cleaning.\n",
    "\n",
    "> Notes from Zhean: <br>\n",
    "> From manual checking via OpenRefine, there are a total of 162972. `df.info()` should have the same result post-processing.\n",
    "> Furthermore, there should be two columns, `clean_text` (which is a bit of a misnormer since it is still dirty) contains the Tweets (text data). The second column is the `category` which contains the sentiment of the Tweet and is a tribool (1 positive, 0 neutral or indeterminate, and -1 for negative).\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64d042-e337-4d55-a6e4-0d065abd2738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
