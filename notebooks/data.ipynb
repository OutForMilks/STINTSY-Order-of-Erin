{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44906e4e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Data Description, Cleaning, Exploratory Data Analysis, and Preprocessing</b></center>\n",
    "<br>\n",
    "\n",
    "**by**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general population‚Äôs opinions and reactions.\n",
    "\n",
    "**goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis.\n",
    "\n",
    "### **dataset description**\n",
    "\n",
    "The Twitter Sentiments Dataset is a dataset that contains nearly 163k tweets from Twitter. The time period of when these were collected is unknown, but it was published to Mendeley Data on May 14, 2021 by Sherif Hussein of Mansoura University.\n",
    "\n",
    "Tweets were extracted using the Twitter API, but the specifics of how the tweets were selected are unmentioned. The tweets are mostly English with a mix of some Hindi words for code-switching <u>(El-Demerdash., 2021)</u>. All of them seem to be talking about the political state of India. Most tweets mention Narendra Modi, the current Prime Minister of India.\n",
    "\n",
    "Each tweet was assigned a label using TextBlob's sentiment analysis <u>(El‚ÄëDemerdash, Hussein, & Zaki, 2021)</u>, which assigns labels automatically.\n",
    "\n",
    "Twitter_Data\n",
    "- **`clean_text`**: The tweet's text\n",
    "- **`category`**: The tweet's sentiment category\n",
    "\n",
    "What each row and column represents: `each row represents one tweet.` <br>\n",
    "Number of observations: `162,980`\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Code-switching is the practice of alternating between two languages $L_1$ (the native language) and $L_2$ (the source language) in a conversation. In this context, the code-switching is done to appear more casual since the conversation is done via Twitter (now, X). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47491b2f",
   "metadata": {},
   "source": [
    "## **1 project set up**\n",
    "We set the global imports for the projects (ensure these are installed via uv and is part of the environment). Furthermore, load the dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7578d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:01:26.803271Z",
     "start_time": "2025-06-17T14:01:26.567527Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Use lib directory\n",
    "sys.path.append(os.path.abspath(\"../lib\"))\n",
    "\n",
    "# Imports from lib files\n",
    "from janitor import *\n",
    "from lemmatize import lemmatizer\n",
    "from boilerplate import stopwords_set\n",
    "from bag_of_words import BagOfWordsModel\n",
    "\n",
    "# Load raw data file\n",
    "df = pd.read_csv(\"../data/Twitter_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ec446",
   "metadata": {},
   "source": [
    "## **2 data cleaning**\n",
    "This section discusses the methodology for data cleaning.\n",
    "\n",
    "As to not waste computational time, a preliminary step is to ensure that no `NaN` and duplicates entries exist before the cleaning steps. Everytime we call a `.drop()` function, we will show the result of `info()` to see how many entries are filtered out.\n",
    "\n",
    "Let's first drop the `NaN` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28609dbc-1d51-4bba-b64c-a570c9ac4729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162969 non-null  object \n",
      " 1   category    162969 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8865f85-09ec-4528-8b69-4f39ec72ae26",
   "metadata": {},
   "source": [
    "Now, remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452f1167-41cd-49a9-a328-8311f1567071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162969 non-null  object \n",
      " 1   category    162969 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec33f2-9967-4458-878b-79b697ec9e4e",
   "metadata": {},
   "source": [
    "## **main cleaning pipeline**\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in (George & Murugesan, 2024). \n",
    "\n",
    "### **normalization**\n",
    "The first function is the `normalize` function, it normalizes the text input to ASCII-only characters (say, \"c√≥mo est√°s\" becomes \"como estas\") and lowercases alphabetic symbols. The dataset contains Unicode characters (e.g., emojis and accented characters) which the function replaces to the empty string (`''`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0425f96-ed91-41ca-9033-5bb53f14c9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m normalize(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m normalize(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Normalize text from a pandas entry to ASCII-only lowercase characters. Hence, this removes Unicode characters with no ASCII\u001b[39m\n",
       "\u001b[33m    equivalent (e.g., emojis and CJKs).\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    ASCII-normalized text containing only lowercase letters.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    normalize(\"¬øC√≥mo est√°s?\")\u001b[39m\n",
       "\u001b[33m    $ 'como estas?'\u001b[39m\n",
       "\n",
       "\u001b[33m    normalize(\" hahahaha HUY! Kamusta üòÖ Mayaman $$$ ka na ba?\")\u001b[39m\n",
       "\u001b[33m    $ ' hahahaha huy! kamusta  mayaman $$$ ka na ba?'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    normalized = unicodedata.normalize(\u001b[33m\"NFKD\"\u001b[39m, text)\n",
       "    ascii_text = normalized.encode(\u001b[33m\"ascii\"\u001b[39m, \u001b[33m\"ignore\"\u001b[39m).decode(\u001b[33m\"ascii\"\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m ascii_text.lower()\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalize??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc84505-f974-442b-8763-62bd20d729ea",
   "metadata": {},
   "source": [
    "### **punctuations**\n",
    "\n",
    "Punctuations do not add much information to the sentiment of a message. The sentiment of `i hate you!` and `i hate you` are going to be the same (of course, the exclamation point accentuates the emotion invoked in the message, but that is irrelevant in a classification study). Hence we defined `rem_punctuation` as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62aa658b-8dbd-4533-af84-cbb221571835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m rem_punctuation(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m rem_punctuation(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Removes the punctuations. This function simply replaces all punctuation marks and special characters\u001b[39m\n",
       "\u001b[33m    to the empty string. Hence, for symbols enclosed by whitespace, the whitespace are not collapsed to a single whitespace\u001b[39m\n",
       "\u001b[33m    (for more information, see the examples).\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    Text with the punctuation removed.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    rem_punctuation(\"this word $$ has two spaces after it!\")\u001b[39m\n",
       "\u001b[33m    $ 'this word  has two spaces after it'\u001b[39m\n",
       "\n",
       "\u001b[33m    rem_punctuation(\"these!words@have$no%space\")\u001b[39m\n",
       "\u001b[33m    $ 'thesewordshavenospace'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mf\"[{re.escape(string.punctuation)}]\"\u001b[39m, \u001b[33m\"\"\u001b[39m, text)\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rem_punctuation??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47e93e-aa0e-473c-89d5-f30df86815da",
   "metadata": {},
   "source": [
    "### **numbers**\n",
    "Similar to punctuations, numbers do not add any information to the sentiment of a message. Hence we defined the `rem_numbers` as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a58576-8dd7-4043-a903-41b7cfab88a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m rem_numbers(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m rem_numbers(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Removes numbers. This function simply replaces all numerical symbols to the empty string. Hence, for symbols enclosed by\u001b[39m\n",
       "\u001b[33m    whitespace, the whitespace are not collapsed to a single whitespace (for more information, see the examples).\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    Text with the numerical symbol removed\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    rem_numbers(\" h3llo, k4must4 k4  n4?\")\u001b[39m\n",
       "\u001b[33m    ' hllo, kmust k  n?'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\"\\d+\"\u001b[39m, \u001b[33m\"\"\u001b[39m, text)\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rem_numbers??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b1491-1821-42b1-a63f-dab941dc003f",
   "metadata": {},
   "source": [
    "### **whitespace**\n",
    "Finally, `collapse_whitespace` collapses all whitespace characters to a single space. Formally, it is a transducer \n",
    "\n",
    "$$\n",
    "\\Box^+ \\mapsto \\Box \\qquad \\text{where the space character is } \\Box\n",
    "$$\n",
    "\n",
    "Informally, it replaces all strings of whitespaces to a single whitespace character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3823f3bc-0a42-473a-8888-a4c06f0659ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m collapse_whitespace(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m collapse_whitespace(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    This collapses whitespace. Here, collapsing means the transduction of all whitespace strings of any\u001b[39m\n",
       "\u001b[33m    length to a whitespace string of unit length (e.g., \"   \" -> \" \"; formally \" \"+ -> \" \").\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    Text with the whitespaces collapsed.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    collapse_whitespace(\"  huh,  was.  that!!! \")\u001b[39m\n",
       "\u001b[33m    $ 'huh, was. that!!!'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33m\" +\"\u001b[39m, \u001b[33m\" \"\u001b[39m, text).strip()\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collapse_whitespace??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd388a-e397-4100-a6df-a971db6f85df",
   "metadata": {},
   "source": [
    "To seamlessly call all these cleaning functions, we have the `clean` function that acts as a container that calls these separate components.\n",
    "\n",
    "We can now clean the dataset and store it in a new column names `clean_ours` (to differentiate it will the, still dirty, column `clean_text` from the dataset author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4ea4d2-e8fe-437f-a46f-a18b1b34f2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162969 non-null  object \n",
      " 1   category    162969 non-null  float64\n",
      " 2   clean_ours  162969 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_ours\"] = df[\"clean_text\"].map(clean)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6110683a-3879-452e-9d2f-50c4af4e0ad6",
   "metadata": {},
   "source": [
    "### **spam, expressions, onomatopoeia, etc**\n",
    "\n",
    "Since the domain of the corpus is Twitter, spam (e.g., `bbbb`), expressions (e.g., `bruhhhh`), and onomatopoeia (e.g., `hahahaha`) may become an issue by the vector representation step. Hence we employed a simple rule-based spam removal algorithm.\n",
    "\n",
    "We remove words in the string that contains the same letter or substring thrice and consecutively. These were done using regular expressions:\n",
    "\n",
    "$$\n",
    "\\text{same\\_char\\_thrice} := (.)\\textbackslash1^{\\{2,\\}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text{same\\_substring\\_twice} := (.^+)\\textbackslash1^+\n",
    "$$\n",
    "\n",
    "Furthermore, we also remove any string that has a length less than three, since these are either stopwords (that weren't detected in the stopword removal stage) or more spam. \n",
    "\n",
    "Finally, we employ adaptive character diversity threshold for the string $s$. \n",
    "\n",
    "$$\n",
    "\\frac{\\texttt{\\#\\_unique\\_chars}(s)}{|s|} < 0.3 + \\left(\\frac{0.1 \\cdot \\text{min}(|s|, 10)}{10}\\right)\n",
    "$$\n",
    "\n",
    "It calculates the diversity of characters in a string; if the string repeats the same character alot, we expect it to be unintelligible or useless, hence we remove it.\n",
    "\n",
    "Let's now call this function on the `clean_ours` column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac69746-f471-4a66-9292-c2b363d12de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162969 non-null  object \n",
      " 1   category    162969 non-null  float64\n",
      " 2   clean_ours  162942 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_ours\"] = df[\"clean_ours\"].map(find_spam_and_empty)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49108da7-5528-4a3d-9b37-9c3d1b76adab",
   "metadata": {},
   "source": [
    "Looking at the size of the dataset post-cleaning stage, we now have:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d3dbd4-474c-4d76-9afe-198c4c53c063",
   "metadata": {},
   "source": [
    "## **post-cleaning steps**\n",
    "\n",
    "At some point during the cleaning stage, some entries of the dataset could have been reduced to `NaN` or the empty string `\"\"`, or we could have introduced duplicates again. So, let's call `dropna` and `drop_duplicates` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f365bcfa-e44b-46a1-9702-ad36cfcf7896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162942 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162942 non-null  object \n",
      " 1   category    162942 non-null  float64\n",
      " 2   clean_ours  162942 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd5c44f-32ab-4b34-b7e4-121c2a898c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162942 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162942 non-null  object \n",
      " 1   category    162942 non-null  float64\n",
      " 2   clean_ours  162942 non-null  object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb4d20",
   "metadata": {},
   "source": [
    "# **3 preprocessing**\n",
    "\n",
    "> üèóÔ∏è Perhaps swap S3 and S4. Refer to literature on what comes first.\n",
    "\n",
    "This section discusses preprocessing steps for the cleaned data.\n",
    "\n",
    "## **lemmatization**\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in <u>(George & Murugesan, 2024)</u>. We preprocess the dataset entries via lemmatization. We use NLTK for this task using WordNetLemmatizer lemmatization, repectively <u>(Bird & Loper, 2004)</u>. For the lemmatization step, we use the WordNet for English lemmatization and Open Multilingual WordNet version 1.4 for translations and multilingual support which is important for our case since some tweets contain text from Indian Languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c3a9b5-b35a-47ca-9ecf-7f950db07395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized\"] = df[\"clean_ours\"].map(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42589219-d005-4ab3-b825-cccb7fa6d663",
   "metadata": {},
   "source": [
    "## **stop word removal**\n",
    "\n",
    "After lemmatization, we may now remove the stop words present in the dataset. The stopword removal _needs_ to be after lemmatization since this step requires all words to be reduces to their base dictionary form, and the `stopword_set` only considers base dictionary forms of the stopwords.\n",
    "\n",
    "**stopwords.** For stop words removal, we refer to the English stopwords dataset defined in NLTK and Wolfram Mathematica <u>(Bird & Loper, 2004; Wolfram Research, 2015)</u>. However, since the task is sentiment analysis, words that invoke polarity, intensification, and negation are important. Words like \"not\" and \"okay\" are commonly included as stopwords. Therefore, the stopwords from [nltk,mathematica] are manually adjusted to only include stopwords that invoke neutrality, examples are \"after\", \"when\", and \"you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a91231f-c21d-41da-9296-7b8607f9cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized\"] = df[\"lemmatized\"].map(lambda t: rem_stopwords(t, stopwords_set))\n",
    "df = df.dropna(subset=[\"lemmatized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd19e4-020a-4e3d-9d0f-6187ce4102d0",
   "metadata": {},
   "source": [
    "## **looking at the DataFrame**\n",
    "\n",
    "After preprocessing, the dataset now contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "883b7f78-ba3d-4bad-9969-0a09b067e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162942 entries, 0 to 162979\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162942 non-null  object \n",
      " 1   category    162942 non-null  float64\n",
      " 2   clean_ours  162942 non-null  object \n",
      " 3   lemmatized  162942 non-null  object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54e5b1-3b81-477e-96ee-602c33533a41",
   "metadata": {},
   "source": [
    "Here are 10 randomly picked entries in the dataframe with all columns shown for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e069ce-83c3-4769-8914-b442625a6032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_ours</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18509</th>\n",
       "      <td>bjp modi again for vote</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bjp modi again for vote</td>\n",
       "      <td>bjp modi vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83612</th>\n",
       "      <td>coins sarab acronym slam opposition alliance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>coins sarab acronym slam opposition alliance</td>\n",
       "      <td>coin sarab acronym slam opposition alliance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94107</th>\n",
       "      <td>bjp will stop little short finish line probablygadkari will become pmshah and modi will get side wing jobswami will have major roleoverall probability 072 odds ratio 1832</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>bjp will stop little short finish line probablygadkari will become pmshah and modi will get side wing jobswami will have major roleoverall probability odds ratio</td>\n",
       "      <td>bjp stop little short finish probablygadkari pmshah modi wing jobswami major roleoverall probability odds ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128720</th>\n",
       "      <td>our india have jobs our india very very very poor indiaour modi worst prime minister history</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>our india have jobs our india very very very poor indiaour modi worst prime minister history</td>\n",
       "      <td>india job india very very very poor indiaour modi worst prime minister history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106218</th>\n",
       "      <td>and who would have blamed for balakote failure some mishap occured modi army say loudly biased</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>and who would have blamed for balakote failure some mishap occured modi army say loudly biased</td>\n",
       "      <td>blamed balakote failure mishap occured modi army loudly biased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                        clean_text  \\\n",
       "18509                                                                                                                                                     bjp modi again for vote    \n",
       "83612                                                                                                                                coins sarab acronym slam opposition alliance    \n",
       "94107   bjp will stop little short finish line probablygadkari will become pmshah and modi will get side wing jobswami will have major roleoverall probability 072 odds ratio 1832   \n",
       "128720                                                                                our india have jobs our india very very very poor indiaour modi worst prime minister history   \n",
       "106218                                                                              and who would have blamed for balakote failure some mishap occured modi army say loudly biased   \n",
       "\n",
       "        category  \\\n",
       "18509        0.0   \n",
       "83612        0.0   \n",
       "94107       -1.0   \n",
       "128720      -1.0   \n",
       "106218      -1.0   \n",
       "\n",
       "                                                                                                                                                               clean_ours  \\\n",
       "18509                                                                                                                                             bjp modi again for vote   \n",
       "83612                                                                                                                        coins sarab acronym slam opposition alliance   \n",
       "94107   bjp will stop little short finish line probablygadkari will become pmshah and modi will get side wing jobswami will have major roleoverall probability odds ratio   \n",
       "128720                                                                       our india have jobs our india very very very poor indiaour modi worst prime minister history   \n",
       "106218                                                                     and who would have blamed for balakote failure some mishap occured modi army say loudly biased   \n",
       "\n",
       "                                                                                                             lemmatized  \n",
       "18509                                                                                                     bjp modi vote  \n",
       "83612                                                                       coin sarab acronym slam opposition alliance  \n",
       "94107   bjp stop little short finish probablygadkari pmshah modi wing jobswami major roleoverall probability odds ratio  \n",
       "128720                                   india job india very very very poor indiaour modi worst prime minister history  \n",
       "106218                                                   blamed balakote failure mishap occured modi army loudly biased  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64d042-e337-4d55-a6e4-0d065abd2738",
   "metadata": {},
   "source": [
    "## **tokenization** \n",
    "\n",
    "Since the data cleaning and preprocessing stage is comprehensive, the tokenization step in the BoW model reduces to a simple word-boundary split operation. Each preprocessed entry in the DataFrame is split by spaces. For example, the entry `\"shri narendra modis\"` (entry: 42052) becomes `[\"shri\", \"narendra\", \"modis\"]`. By the end of tokenization, all entries are transformed into arrays of strings.\n",
    "\n",
    "## **word bigrams** \n",
    "\n",
    "As noted earlier, modifiers and polarity words are not included in the stopword set. The BoW model constructs a vocabulary containing both unigrams and bigrams. Including bigrams allows the model to capture common word patterns, such as  \n",
    "\n",
    "$$\n",
    "\\left\\langle \\texttt{Adj}\\right\\rangle \\left\\langle \\texttt{M} \\mid \\texttt{Pron} \\right\\rangle \n",
    "$$  \n",
    "\n",
    "or  \n",
    "\n",
    "$$\n",
    "\\left\\langle \\texttt{Adv}\\right\\rangle \\left\\langle \\texttt{V} \\mid \\texttt{Adj} \\mid \\texttt{Adv} \\right\\rangle \n",
    "$$  \n",
    "\n",
    "## **vector representation**\n",
    "\n",
    "After the stemming and lemmatization steps, each entry can now be represented as a vector using a Bag of Words (BoW) model. We employ scikit-learn's `CountVectorizer`, which provides a ready-to-use implementation of BoW <u>(Pedregosa et al., 2011)</u>.\n",
    "\n",
    "A comparison of other traditional vector representations are discussed in [this appendix](#appendix:-comparison-of-traditional-vectorization-techniques).\n",
    "Words with modifiers have the modifiers directly attached, enabling subsequent models to capture the concept of modification fully. Consequently, after tokenization and bigram construction, the vocabulary size can grow up to $O(n^2)$, where $n$ is the number of unique tokens.\n",
    "\n",
    "**minimum document frequency constraint:** Despite cleaning and spam removal, some tokens remain irrelevant or too rare. To address this, a minimum document frequency constraint is applied: $\\texttt{min\\_df} = 10$, meaning a token must appear in at least 10 documents to be included in the BoW vocabulary. This reduces noise and ensures the model focuses on meaningful terms.\n",
    "\n",
    "---\n",
    "\n",
    "These parameters of the BoW model are encapsulated in the `BagOfWordsModel` class. The class definition is available in [this appendix](#appendix:-BagOfWordsModel-class-definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bfba459-6837-4481-929c-ef5ee023b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = BagOfWordsModel(df[\"lemmatized\"], 10)\n",
    "\n",
    "# some sanity checks\n",
    "assert bow.matrix.shape[0] == df.shape[0], \"number of rows in the matrix DOES NOT matches the number of documents\"\n",
    "assert bow.sparsity,                       \"the sparsity is TOO HIGH, something went wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75919779-1b56-4bec-9662-90afc11e1356",
   "metadata": {},
   "source": [
    "The error above is normal, recall that our tokenization step essentially reduced into an array split step. With this, we need to set the `tokenizer` function attribute of the `BagOfWordsModel` to not use its default tokenization pattern. That causes this warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf5220-712f-4af3-9e2e-0fc3753f5cb1",
   "metadata": {},
   "source": [
    "### **model metrics**\n",
    "\n",
    "To get an idea of the model, we will now look at its shape and sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0246d-899e-4bb6-957a-75419316197a",
   "metadata": {},
   "source": [
    "The resulting vector has a shape of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6a5b688-b636-4320-bf22-e508a97aa862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162942, 30386)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc081f2-0630-4a0c-a40a-aeddcdfc8fb8",
   "metadata": {},
   "source": [
    "The first entry of the pair is the number of documents (the ones that remain after all the data cleaning and preprocessing steps) and the second entry is the number of tokens (or unique words in the vocabulary).\n",
    "\n",
    "The resulting model has a sparsity of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fd01aa7-4842-4473-b410-591fd47983f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004960460127828437"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3277d-2106-490d-a304-8ff6e0780fd5",
   "metadata": {},
   "source": [
    "> üèóÔ∏è perhaps discuss sparsity's relevance\n",
    "\n",
    "Now, looking at the most frequent and least frequent terms in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10dd4551-90d0-4795-9303-1118fcc058c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['modi', 'india', 'ha', 'all', 'people', 'bjp', 'like', 'congress',\n",
       "       'narendra', 'only', 'election', 'narendra modi', 'vote', 'govt',\n",
       "       'about', 'indian', 'year', 'time', 'country', 'just', 'modis',\n",
       "       'more', 'nation', 'rahul', 'even', 'government', 'party', 'power',\n",
       "       'gandhi', 'minister', 'leader', 'good', 'modi govt', 'need',\n",
       "       'modi ha', 'space', 'work', 'prime', 'money', 'credit', 'sir',\n",
       "       'pakistan', 'back', 'day', 'today', 'prime minister', 'scientist',\n",
       "       'never', 'support', 'win'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_frequencies = np.asarray((bow.matrix > 0).sum(axis=0)).flatten()\n",
    "freq_order = np.argsort(doc_frequencies)[::-1]\n",
    "bow.feature_names[freq_order[:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c2f23-6494-4d41-937f-19664010b138",
   "metadata": {},
   "source": [
    "We see that the main talking point of the Tweets, which hovers around Indian politics with keywords like \"modi\", \"india\", and \"bjp\". For additional context, \"bjp\" referes to the _Bharatiya Janata Party_ which is a conservative political party in India, and one of the two major Indian political parties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b775494-3472-4089-8d86-e24346616155",
   "metadata": {},
   "source": [
    "Now, looking at the least popular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f91a0d-dae1-41af-9f92-c06a973ebbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['healthy democracy', 'ha mass', 'ha separate', 'ha shifted',\n",
       "       'hat drdo', 'about defeat', 'yet ha', 'yes more', 'yes narendra',\n",
       "       'hatred people', 'ha requested', 'hate more', 'hate much',\n",
       "       'hatemonger', 'hater gonna', 'heal', 'hazaribagh', 'head drdo',\n",
       "       'sleep night', 'abinandan', 'able provide', 'able speak',\n",
       "       'able vote', 'youth need', 'youth power', 'hai isliye', 'hai chor',\n",
       "       'handy', 'hand narendra', 'hand people', 'hae', 'ha withdrawn',\n",
       "       'happens credit', 'happier', 'bhaiyo', 'socha', 'social political',\n",
       "       'social security', 'biased journalist', 'big congratulation',\n",
       "       'sirmodi', 'bhutan', 'bhi berozgar', 'bhi mumkin', 'skta',\n",
       "       'bhatt aditi', 'bhi aur', 'slamming', 'smart modi', 'slogan blame'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.feature_names[freq_order[-50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5478a6-4ff0-4d64-a329-11278fe4e60a",
   "metadata": {},
   "source": [
    "We still see that the themes mentioned in the most frequent terms are still present in this subset. Although, more filler or non-distinct words do appear more often, like \"photos\", \"soft\" and \"types\".\n",
    "\n",
    "But the present of words like \"reelection\" and \"wars\" still point to this subset still being relevant to the main theme of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb530e2",
   "metadata": {},
   "source": [
    "# **4 exploratory data analysis**\n",
    "\n",
    "This section discusses the exploratory data analysis conducted on the dataset after cleaning.\n",
    "\n",
    "> Notes from Zhean: <br>\n",
    "> From manual checking via OpenRefine, there are a total of 162972. `df.info()` should have the same result post-processing.\n",
    "> Furthermore, there should be two columns, `clean_text` (which is a bit of a misnormer since it is still dirty) contains the Tweets (text data). The second column is the `category` which contains the sentiment of the Tweet and is a tribool (1 positive, 0 neutral or indeterminate, and -1 for negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ad21e-cb7b-4d91-87f9-a14c15ec8365",
   "metadata": {},
   "source": [
    "# **references**\n",
    "Bird, S., & Loper, E. (2004, July). NLTK: The natural language toolkit. *Proceedings of the ACL Interactive Poster and Demonstration Sessions*, 214‚Äì217. https://aclanthology.org/P04-3031/\n",
    "\n",
    "El-Demerdash, A. A., Hussein, S. E., & Zaki, J. F. W. (2021). Course evaluation based on deep learning and SSA hyperparameters optimization. *Computers, Materials & Continua, 71*(1), 941‚Äì959. https://doi.org/10.32604/cmc.2022.021839\n",
    "\n",
    "George, M., & Murugesan, R. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. *Procedia Computer Science, 244*, 1‚Äì8.\n",
    "\n",
    "Hussein, S. (2021). *Twitter sentiments dataset*. Mendeley.\n",
    "\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. *Journal of Machine Learning Research, 12*, 2825‚Äì2830.\n",
    "\n",
    "Rani, D., Kumar, R., & Chauhan, N. (2022, October). Study and comparison of vectorization techniques used in text classification. In *2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)* (pp. 1‚Äì6). IEEE.\n",
    "\n",
    "Wolfram Research. (2015). *DeleteStopwords*. https://reference.wolfram.com/language/ref/DeleteStopwords.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f63c6-670f-42b1-8195-3c8156b2f4be",
   "metadata": {},
   "source": [
    "# **appendix: comparison of traditional vectorization techniques**\n",
    "\n",
    "Traditional vectorization techniques include BoW and Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF weights each word based on its frequency in a document and its rarity across the corpus, reducing the impact of common words. BoW, in contrast, simply counts word occurrences without considering corpus-level frequency. In this project, BoW was chosen because stopwords were already removed during preprocessing, and the dataset is domain-specific <u>(Rani et al., 2022)</u>. In such datasets, frequent words are often meaningful domain keywords, so scaling them down (as TF-IDF would) could reduce the importance of these key terms in the feature representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e651fa3-b1f3-4e71-9092-018bbabc07dc",
   "metadata": {},
   "source": [
    "# **appendix: `BagOfWordsModel` class definition**\n",
    "Below is the definition of the `BagOfWordsModel` class that encapsulates the desired parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a9c57a0-98b3-40cb-8cd4-a1d9b30eb6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m BagOfWordsModel(texts: Iterable[str], min_freq: int | float | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m)\n",
       "\u001b[31mSource:\u001b[39m        \n",
       "\u001b[38;5;28;01mclass\u001b[39;00m BagOfWordsModel:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    A Bag-of-Words representation for a text corpus.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Attributes\u001b[39m\n",
       "\u001b[33m    * matrix (scipy.sparse.csr_matrix): The document-term matrix of word counts.\u001b[39m\n",
       "\u001b[33m    * feature_names (list[str]): List of feature names corresponding to the matrix columns.\u001b[39m\n",
       "\u001b[33m    *\u001b[39m\n",
       "\u001b[33m    # Usage\u001b[39m\n",
       "\u001b[33m    ```\u001b[39m\n",
       "\u001b[33m    bow = BagOfWordsModel(df[\"lemmatized_str\"])\u001b[39m\n",
       "\u001b[33m    ```\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m __init__(self, texts: Iterable[str], min_freq: int | float | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m):\n",
       "        \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m        Initialize the BagOfWordsModel by fitting the vectorizer to the text corpus. This also filters out tokens\u001b[39m\n",
       "\u001b[33m        that do not appear more than five times in the dataset.\u001b[39m\n",
       "\n",
       "\u001b[33m        This sets its tokenizer to the word boundary tokenizer since the input, at this point, **should** be\u001b[39m\n",
       "\u001b[33m        cleaned and processed text.\u001b[39m\n",
       "\n",
       "\u001b[33m        This also uses both unigrams and bigrams, hence, at the worst case its space complexity is O(n^2).\u001b[39m\n",
       "\n",
       "\u001b[33m        # Parameters\u001b[39m\n",
       "\u001b[33m        * texts: An iterable of cleaned text documents.\u001b[39m\n",
       "\u001b[33m        * min_freq: Determines the document frequency of a token for it to appear in the model.\u001b[39m\n",
       "\u001b[33m        Can be a type of int (i.e., the token must appear min_freq number of times in the document)\u001b[39m\n",
       "\u001b[33m        or a float (i.e, token must be in min_freq% of the documents)\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        vectorizer = CountVectorizer(\n",
       "            min_df=min_freq \u001b[38;5;28;01mif\u001b[39;00m min_freq \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m,\n",
       "            tokenizer=str.split,    \u001b[38;5;66;03m# Use str.split instead of lambda\u001b[39;00m\n",
       "            lowercase=\u001b[38;5;28;01mFalse\u001b[39;00m,        \u001b[38;5;66;03m# Don't lowercase\u001b[39;00m\n",
       "            ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m),      \u001b[38;5;66;03m# Unigrams and bigrams\u001b[39;00m\n",
       "        )\n",
       "        self.matrix = vectorizer.fit_transform(texts)\n",
       "        self.feature_names = vectorizer.get_feature_names_out()\n",
       "        self.vectorizer = vectorizer\n",
       "        self.sparsity = self.matrix.nnz / (self.matrix.shape[\u001b[32m0\u001b[39m] * self.matrix.shape[\u001b[32m1\u001b[39m])\n",
       "\n",
       "    \u001b[38;5;28;01mdef\u001b[39;00m transform_sentence(self, sentence: str):\n",
       "        \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m        Returns the embedding of the sentence using the BoW matrix.\u001b[39m\n",
       "\n",
       "\u001b[33m        # Parameters:\u001b[39m\n",
       "\u001b[33m        * sentence: Cleaned sentence to vectorize.\u001b[39m\n",
       "\n",
       "\u001b[33m        # Returns\u001b[39m\n",
       "\u001b[33m        Sentence embedding.\u001b[39m\n",
       "\u001b[33m        \"\"\"\u001b[39m\n",
       "        \u001b[38;5;28;01mreturn\u001b[39;00m self.vectorizer.transform([sentence])\n",
       "\u001b[31mFile:\u001b[39m           ~/STINTSY-Order-of-Erin/lib/bag_of_words.py\n",
       "\u001b[31mType:\u001b[39m           type\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BagOfWordsModel??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
