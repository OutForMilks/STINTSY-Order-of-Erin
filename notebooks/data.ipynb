{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44906e4e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Data Description, Cleaning, Exploratory Data Analysis, and Preprocessing</b></center>\n",
    "<br>\n",
    "\n",
    "**By**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**Dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**Motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general population‚Äôs opinions and reactions.\n",
    "\n",
    "**Goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis.\n",
    "\n",
    "### **Dataset Description**\n",
    "\n",
    "The Twitter Sentiments Dataset is a dataset that contains nearly 163k tweets from Twitter. The time period of when these were collected is unknown, but it was published to Mendeley Data on May 14, 2021 by Sherif Hussein of Mansoura University.\n",
    "\n",
    "Tweets were extracted using the Twitter API, but the specifics of how the tweets were selected are unmentioned. The tweets are mostly English with a mix of some Hindi words for code-switching<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1). All of them seem to be talking about the political state of India. Most tweets mention Narendra Modi, the current Prime Minister of India.\n",
    "\n",
    "Each tweet was assigned a label using TextBlob's sentiment analysis (El‚ÄëDemerdash, Hussein, & Zaki, 2021), which assigns labels automatically.\n",
    "\n",
    "Twitter_Data\n",
    "- **`clean_text`**: The tweet's text\n",
    "- **`category`**: The tweet's sentiment category\n",
    "\n",
    "What each row and column represents: `each row represents one tweet.` <br>\n",
    "Number of observations: `162,980`\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. Alaa A. El-Demerdash, J. F. W. Z., Sherif E. Hussein. (2022). Course Evaluation Based on Deep Learning and SSA Hyperparameters Optimization. Computers, Materials & Continua, 71(1), 941‚Äì959. doi:10.32604/cmc.2022.021839\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Code-switching is the practice of alternating between two languages $L_1$ (the native language) and $L_2$ (the source language) in a conversation. In this context, the code-switching is done to appear more casual since the conversation is done via Twitter (now, X). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47491b2f",
   "metadata": {},
   "source": [
    "## 1 project set up\n",
    "Set up here the imports for the projects (ensure these are installed via uv and is part of the environment). Furthermore, load the dataset here. Also load the raw dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7578d03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:01:26.803271Z",
     "start_time": "2025-06-17T14:01:26.567527Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/zrgnt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/zrgnt/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import boilerplate file generated by `codegen.py`\n",
    "from boilerplate import stopwords_set\n",
    "\n",
    "# Import lib file for function definitions\n",
    "sys.path.append(os.path.abspath(\"../lib\"))\n",
    "from janitor import normalize, rem_punctuation, rem_numbers, collapse_whitespace, rem_stopwords, clean_and_tokenize, find_spam_and_empty\n",
    "from bag_of_words import BagOfWordsModel\n",
    "\n",
    "# Imports for Stemming and Lematization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "# Imports for Text Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Set up NLTK objects for stemming and lemmatization\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load raw data file\n",
    "df = pd.read_csv(\"../data/Twitter_Data.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66ec446",
   "metadata": {},
   "source": [
    "## 2 data cleaning\n",
    "This section discusses the methodology for data cleaning.\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in [1].\n",
    "\n",
    "The cleaning pipeline has four main functions. The first function is the `normalize` function, it normalizes the text input to ASCII-only characters (say, \"c√≥mo est√°s\" becomes \"como estas\") and lowercases alphabetic symbols. The dataset contains Unicode characters (e.g., emojis and accented characters) which the function replaces to the empty string (`''`). Then, `rem_punctuation` removes the punctuation marks and special characters with the empty string. Then, `collapse_whitespace` collapses all whitespace characters to a single space. Formally, it is a transducer from $\\Box^+ \\mapsto \\Box$ where $\\Box$ is the whitespace character. Now, since the strings are cleaned at this point, the tokenization step reduces to a mere string split at word boundaries. Finally, with the tokenized string, we can do a final clean by removing the stopwords.\n",
    "\n",
    "**Remark on Stopwords.** For stop words removal, we refer to the English stopwords dataset defined in NLTK and Wolfram Mathematica [2,3]. However, since the task is sentiment analysis, words that invoke polarity, intensification, and negation are important. Words like \"not\" and \"okay\" are commonly included as stopwords. Therefore, the stopwords from [2,3] are manually adjusted to only include stopwords that invoke neutrality, examples are \"after\", \"when\", and \"you.\"\n",
    "\n",
    "**Remark on Potential Spam.** Since the domain of the corpus is Twitter, spam may become an issue by the vector representation step. Hence we employed some simple rule-based spam detection systems.\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. George, M., & Murugesan, R., Dr. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. Procedia Computer Science, 244, 1‚Äì8. https://doi.org/10.1016/j.procs.2024.10.172\n",
    "2. Wolfram Language. (2015). DeleteStopwords. Wolfram Language & System Documentation Center. Retrieved from https://reference.wolfram.com/language/ref/DeleteStopwords.html\n",
    "3. Bird, S., & Loper, E. (2004, July). NLTK: The Natural Language Toolkit. Proceedings of the ACL Interactive Poster and Demonstration Sessions, 214‚Äì217. Retrieved from https://aclanthology.org/P04-3031/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac69746-f471-4a66-9292-c2b363d12de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform cleaning stage\n",
    "# at this point NaN entries shouldn't exist\n",
    "df[\"clean_ours\"] = df[\"clean_text\"].map(lambda x: clean_and_tokenize(x, stopwords_set))\n",
    "df[\"clean_ours\"] = df[\"clean_ours\"].map(lambda toks: find_spam_and_empty(toks))\n",
    "\n",
    "df = df.dropna(subset=[\"clean_ours\"]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb4d20",
   "metadata": {},
   "source": [
    "# 3 preprocessing\n",
    "\n",
    "> üèóÔ∏è Perhaps swap S3 and S4. Refer to literature on what comes first.\n",
    "\n",
    "This section discusses preprocessing steps for the cleaned data.\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in [1]. We preprocess the dataset entries via stemming and lemmatization. We employ NLTK for both tasks using PorterStemmer and WordNetLemmatizer for stemming and lemmatization, repectively [2]. For the lemmatization step, we use the WordNet for English lemmatization and Open Multilingual WordNet version 1.4 for translations and multilingual support which is important for our case since some tweets contain text from Indian Languages.\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. George, M., & Murugesan, R., Dr. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. Procedia Computer Science, 244, 1‚Äì8. https://doi.org/10.1016/j.procs.2024.10.172\n",
    "2. Bird, S., & Loper, E. (2004, July). NLTK: The Natural Language Toolkit. Proceedings of the ACL Interactive Poster and Demonstration Sessions, 214‚Äì217. Retrieved from https://aclanthology.org/P04-3031/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6190c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stemmed\"] = df[\"clean_ours\"].map(lambda tokens: [stemmer.stem(t) for t in tokens])\n",
    "df[\"lemmatized\"] = df[\"clean_ours\"].map(lambda tokens: [lemmatizer.lemmatize(t) for t in tokens])\n",
    "\n",
    "# do this, since vectorizer expects a string not an array of strings\n",
    "df[\"lemmatized_str\"] = df[\"lemmatized\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a64d042-e337-4d55-a6e4-0d065abd2738",
   "metadata": {},
   "source": [
    "## Vector Representation\n",
    "\n",
    "After to stemming and lemmatization steps, we can now represent each each entry to its vector representation from a Bag of Words (BoW) model. We use scikit-learn's `CountVectorizer` which is an ready-to-use implementation of the BoW model.\n",
    "\n",
    "**Related Literature and Vectorization Techniques.** Traditional vectorization techniques include BoW and Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF weights each word based on its frequency in a document and how rare it is across the corpus, reducing the impact of common words. BoW, in contrast, simply counts word occurrences without considering corpus-level frequency. In this project, BoW was chosen because stopwords were already removed during preprocessing, and the dataset is domain-specific [3]. In such datasets, frequent words are often meaningful domain keywords, so scaling them down (as TF-IDF would) could reduce the importance of these key terms in the feature representation.\n",
    "\n",
    "The resulting vector has 162,969 rows which is the number of entries in the dataset (with `NaN` entries removed), and 101,284 which is the number of unique words in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "**References**\n",
    "1. Rani, D., Kumar, R., & Chauhan, N. (2022, October). Study and comparision of vectorization techniques used in text classification. In 2022 13th international conference on computing communication and networking technologies (ICCCNT) (pp. 1-6). IEEE.\n",
    "2. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, Œà. (2011). Scikit-learn: Machine Learning in Python. J. Mach. Learn. Res., 12(null), 2825‚Äì2830."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfba459-6837-4481-929c-ef5ee023b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aadhaar' 'aadhar' 'aaj' 'aam' 'aap' 'aayega' 'aayog' 'abdullah' 'abe'\n",
      " 'abhi' 'abhinandan' 'abhisar' 'ability' 'abki' 'able' 'about' 'above'\n",
      " 'abroad' 'absolute' 'absolutely' 'absurd' 'abt' 'abuse' 'abused'\n",
      " 'abusing' 'abusive' 'abv' 'accept' 'acceptable' 'accepted' 'accepting'\n",
      " 'access' 'acche' 'accomplished' 'accomplishment' 'account'\n",
      " 'accountability' 'accountable' 'accuse' 'accused' 'accusing' 'ache'\n",
      " 'achhe' 'achieve' 'achieved' 'achievement' 'achieving' 'acknowledge'\n",
      " 'acronym' 'act' 'acting' 'action' 'active' 'activist' 'activity' 'actor'\n",
      " 'actual' 'actually' 'ad' 'adani' 'adanis' 'add' 'added' 'address'\n",
      " 'addressed' 'addressing' 'adityanath' 'administration' 'admire' 'admit'\n",
      " 'advance' 'advani' 'advantage' 'advertisement' 'advertising' 'advice'\n",
      " 'advise' 'advisor' 'affair' 'affect' 'affected' 'afford' 'affordable'\n",
      " 'afraid' 'agar' 'age' 'agency' 'agenda' 'agent' 'aggressive' 'ago'\n",
      " 'agree' 'agreed' 'agriculture' 'ahead' 'aid' 'aiims' 'aim' 'air'\n",
      " 'aircraft']\n"
     ]
    }
   ],
   "source": [
    "bow = BagOfWordsModel(df[\"lemmatized_str\"], 0.0004)\n",
    "\n",
    "# some sanity checks\n",
    "assert bow.matrix.shape[0] == df.shape[0],                               \"number of rows in the matrix DOES NOT matches the number of documents\"\n",
    "assert bow.matrix.nnz / (bow.matrix.shape[0] * bow.matrix.shape[1]) < 1, \"the sparsity is TOO HIGH, something went wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb530e2",
   "metadata": {},
   "source": [
    "# 4 exploratory data analysis\n",
    "\n",
    "This section discusses the exploratory data analysis conducted on the dataset after cleaning.\n",
    "\n",
    "> Notes from Zhean: <br>\n",
    "> From manual checking via OpenRefine, there are a total of 162972. `df.info()` should have the same result post-processing.\n",
    "> Furthermore, there should be two columns, `clean_text` (which is a bit of a misnormer since it is still dirty) contains the Tweets (text data). The second column is the `category` which contains the sentiment of the Tweet and is a tribool (1 positive, 0 neutral or indeterminate, and -1 for negative).\n",
    "\n",
    "---\n",
    "**References**\n",
    "1. ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
