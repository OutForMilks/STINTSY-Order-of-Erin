{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53472c7e090c8f69",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Posts\n",
    "\n",
    "<!-- Notebook name goes here -->\n",
    "<center><b>Notebook: Data Description, Cleaning, Exploratory Data Analysis, and Preprocessing</b></center>\n",
    "<br>\n",
    "\n",
    "**by**: Stephen Borja, Justin Ching, Erin Chua, and Zhean Ganituen.\n",
    "\n",
    "**dataset**: Hussein, S. (2021). Twitter Sentiments Dataset [Dataset]. Mendeley. https://doi.org/10.17632/Z9ZW7NT5H2.1\n",
    "\n",
    "**motivation**: Every minute, social media users generate a large influx of textual data on live events. Performing sentiment analysis on this data provides a real-time view of public perception, enabling quick insights into the general population‚Äôs opinions and reactions.\n",
    "\n",
    "**goal**: By the end of the project, our goal is to create and compare supervised learning algorithms for sentiment analysis.\n",
    "\n",
    "### **dataset description**\n",
    "\n",
    "The Twitter Sentiments Dataset is a dataset that contains nearly 163k tweets from Twitter. The time period of when these were collected is unknown, but it was published to Mendeley Data on May 14, 2021 by Sherif Hussein of Mansoura University.\n",
    "\n",
    "Tweets were extracted using the Twitter API, but the specifics of how the tweets were selected are unmentioned. The tweets are mostly English with a mix of some Hindi words for code-switching <u>(El-Demerdash., 2021)</u>. All of them seem to be talking about the political state of India. Most tweets mention Narendra Modi, the current Prime Minister of India.\n",
    "\n",
    "Each tweet was assigned a label using TextBlob's sentiment analysis <u>(El‚ÄëDemerdash, Hussein, & Zaki, 2021)</u>, which assigns labels automatically.\n",
    "\n",
    "Twitter_Data\n",
    "\n",
    "- **`clean_text`**: The tweet's text\n",
    "- **`category`**: The tweet's sentiment category\n",
    "\n",
    "What each row and column represents: `each row represents one tweet.` <br>\n",
    "Number of observations: `162,980`\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Code-switching is the practice of alternating between two languages $L_1$ (the native language) and $L_2$ (the source language) in a conversation. In this context, the code-switching is done to appear more casual since the conversation is done via Twitter (now, X).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7e21044b35b0",
   "metadata": {},
   "source": [
    "## **1. Project Set-up**\n",
    "\n",
    "We set the global imports for the projects (ensure these are installed via uv and is part of the environment). Furthermore, load the dataset here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bcb4f466343588b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:04.326Z",
     "start_time": "2026-01-18T06:20:58.034693Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set tqdm to pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Use lib directory\n",
    "sys.path.append(os.path.abspath(\"../lib\"))\n",
    "\n",
    "# Imports from lib files\n",
    "from janitor import *\n",
    "from lemmatize import lemmatizer\n",
    "from boilerplate import stopwords_set\n",
    "from bag_of_words import BagOfWordsModel\n",
    "\n",
    "# Pandas congiruation\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Load raw data file\n",
    "df = pd.read_csv(\"../data/Twitter_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596dbd95e7acff0",
   "metadata": {},
   "source": [
    "## **2. Data Cleaning**\n",
    "\n",
    "This section discusses the methodology for data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfff9b821cc9c4",
   "metadata": {},
   "source": [
    "As to not waste computational time, a preliminary step is to ensure that no **`NaN`** or duplicate entries exist before the cleaning steps. We can call on `info()` after each step to see the rows changed in our DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566ce4ae11c933c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:04.371067Z",
     "start_time": "2026-01-18T06:21:04.335720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 162980 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162976 non-null  object \n",
      " 1   category    162973 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f32c981e8471f07",
   "metadata": {},
   "source": [
    "There are clear inconsistencies with the amount of non-null values between column **`clean_text`** and **`category`** versus the total entries, so our first step would be to drop the `NaN` entries. We can first check which rows have **`category`** as **`NaN`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27dae3f6dace99eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:04.956966Z",
     "start_time": "2026-01-18T06:21:04.919090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130448</th>\n",
       "      <td>the foundation stone northeast gas grid inaugurated modi came major</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155642</th>\n",
       "      <td>dear terrorists you can run but you cant hide are giving more years modi which you won‚Äô see you</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155698</th>\n",
       "      <td>offense the best defence with mission shakti modi has again proved why the real chowkidar our</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155770</th>\n",
       "      <td>have always heard politicians backing out their promises but modi has been fulfilling his each every</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158693</th>\n",
       "      <td>modi government plans felicitate the faceless nameless warriors india totally deserved</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158694</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159442</th>\n",
       "      <td>chidambaram gives praises modinomics</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159443</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160559</th>\n",
       "      <td>the reason why modi contested from seats 2014 and the real reason why rahul doing the same now</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160560</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                  clean_text  \\\n",
       "148                                                                                                      NaN   \n",
       "130448                                   the foundation stone northeast gas grid inaugurated modi came major   \n",
       "155642       dear terrorists you can run but you cant hide are giving more years modi which you won‚Äô see you   \n",
       "155698         offense the best defence with mission shakti modi has again proved why the real chowkidar our   \n",
       "155770  have always heard politicians backing out their promises but modi has been fulfilling his each every   \n",
       "158693                modi government plans felicitate the faceless nameless warriors india totally deserved   \n",
       "158694                                                                                                   NaN   \n",
       "159442                                                                  chidambaram gives praises modinomics   \n",
       "159443                                                                                                   NaN   \n",
       "160559        the reason why modi contested from seats 2014 and the real reason why rahul doing the same now   \n",
       "160560                                                                                                   NaN   \n",
       "\n",
       "        category  \n",
       "148          0.0  \n",
       "130448       NaN  \n",
       "155642       NaN  \n",
       "155698       NaN  \n",
       "155770       NaN  \n",
       "158693       NaN  \n",
       "158694      -1.0  \n",
       "159442       NaN  \n",
       "159443       0.0  \n",
       "160559       NaN  \n",
       "160560       1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN_rows = df[df.isna().any(axis=1)]\n",
    "NaN_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48097ffba7ef3c93",
   "metadata": {},
   "source": [
    "We found that there were a total of 11 rows that have **`NaN`** values, thus we drop them to ensure the integrity and accuracy of our data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf50ad27d980938c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.037243Z",
     "start_time": "2026-01-18T06:21:05.005568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [clean_text, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "NaN_rows = df[df.isna().any(axis=1)]\n",
    "NaN_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea031b736939b87",
   "metadata": {},
   "source": [
    "Another issue found commonly in real-world datasets would be duplicate rows, often from manual data entry errors, system glitches, or when merging data from multiple, overlapping sources. We can first check for duplicates in our `DataFrame` then remove them.\n",
    "\n",
    "> üç† do i need to cite this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb668f33467552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.153818Z",
     "start_time": "2026-01-18T06:21:05.070449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [clean_text, category]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df691ae200170b1",
   "metadata": {},
   "source": [
    "There exist no duplicate rows within our `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b689d",
   "metadata": {},
   "source": [
    "By converting a CSV file into a DataFrame, pandas automatically defaults numeric values to `float64` when it encounters decimals or **`NaN`** types. Text of `str` type get inferred and loaded into a `object` as the generic type for strings. We can check the dtype of our `DataFrame` columns through [`info()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f81e0da29e5aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.287356Z",
     "start_time": "2026-01-18T06:21:05.207429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   clean_text  162969 non-null  object \n",
      " 1   category    162969 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd0450b29b6ba9",
   "metadata": {},
   "source": [
    "We can see that **`clean_text`** column dtype is of `object` and category is of dytpe `float64`, to determine if the columns are assigned the right data type we check the unqiue values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bf936e0ad2379a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.364903Z",
     "start_time": "2026-01-18T06:21:05.346554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for item in df[\"category\"].unique():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb98597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when modi promised ‚Äúminimum government maximum governance‚Äù expected him begin the difficult job reforming the state why does take years get justice state should and not business and should exit psus and temples\n",
      "talk all the nonsense and continue all the drama will vote for modi \n",
      "what did just say vote for modi  welcome bjp told you rahul the main campaigner for modi think modi should just relax\n"
     ]
    }
   ],
   "source": [
    "for item in df[\"clean_text\"].unique()[:3]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd190f",
   "metadata": {},
   "source": [
    "Now that we have seen the unique values of each column, we can safely say that the data types assigned to both columns were not the right ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e1102c8494356",
   "metadata": {},
   "source": [
    "We first will convert column **`category`** from `float64` to `int64` considering that the range of values (**`-1`**, **`0`**, **`1`**) for a tweet's sentiment category will only ever be whole numbers. This step is done after dropping **`NaN`** value rows because **`NaN`** is fundamentally a float type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1ce2797338f58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.421456Z",
     "start_time": "2026-01-18T06:21:05.401566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162969 non-null  object\n",
      " 1   category    162969 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df[\"category\"] = df[\"category\"].astype(int)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e52f8fbb2545e",
   "metadata": {},
   "source": [
    "After successfully converting the **`category`** column into `int64`, next we convert column `clean_string` from `object` type into the pandas defined `string` type for consistency and better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c66a2c528a0aa906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.461778Z",
     "start_time": "2026-01-18T06:21:05.446433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162969 non-null  string\n",
      " 1   category    162969 non-null  int64 \n",
      "dtypes: int64(1), string(1)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"clean_text\"].astype(\"string\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354bfa7833f86544",
   "metadata": {},
   "source": [
    "We are now finished with the _initial_ data cleaning steps, this level is more focused on the standard or common issues present in public datasets and the cleaning of it before we move onto our main cleaning pipeline, which would be more focused on cleaning the tweets themselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b4229459afd05",
   "metadata": {},
   "source": [
    "## **Main Cleaning Pipeline**\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in (George & Murugesan, 2024).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c128c25acb4962",
   "metadata": {},
   "source": [
    "### **Normalization**\n",
    "\n",
    "Due to the nature of the text being tweets, the presence of emojis and accented characters are to be expected. To see if our data has these special characters, we selected a sample set of them to be displayed if they were in **`clean_text`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57bb23e03c788b58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.774698Z",
     "start_time": "2026-01-18T06:21:05.682456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65299                                                                                                                                                                        credit goes modi‚úå‚úå\n",
       "46551                                                                                                                                                                       right said madam ‚ò∫Ô∏è\n",
       "133143    criminals love their number\\nÔ∏èstolen modireg tse 1563 section‚öñÔ∏èfir\\ncheckoutÔ∏èplaces youll find daily harassmentcriminal instigatingtaunting\\nyeah right its all coincidence\\ntheresÔ∏è \n",
       "17203                                                                                                                        nirav modis paintings may fetch crore auction ndtv news ‚ö°buttler‚ö° \n",
       "44710                                                                                                                                                            space war message from modi‚úåÔ∏è \n",
       "Name: clean_text, dtype: string"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a sample of rows with emojis\n",
    "rows_with_emojis = df[df[\"clean_text\"].str.contains(r\"[\\u263a-\\U0001f645]\", regex=True)]\n",
    "rows_with_emojis[\"clean_text\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5273a3e06026932",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.677375Z",
     "start_time": "2026-01-18T06:21:05.575436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156327    advani must ruing the day nourished his prot√©g√©s bjp such modi arun jaitley venkaiah naidu sushma swaraj who serially betrayed him the longheld view that the kind politics you practise eventually catches with you \n",
       "23047                                                                                                                     unlikely titfortat istan darpok nikamm√© babus chorriforri crook donnie bullyfears strength look jago \n",
       "23608                                                                                          dinesh rodi ardent fan modi has opened rodi resto cafe themed modi tamil nadus thoothukudi take peep inside the modithemed caf√© \n",
       "24641     sagara sangamam moment for komali haasan  just how many blows can the ulaga nalayagan take first his prot√©g√© madhavan backs modi now this komali haasan can always drown away his sorrows the teynampet tasmac store \n",
       "161501                                                                                                                calm but confident and assertive thats like great statesman modi the peoples choice for decad√©s come jay \n",
       "Name: clean_text, dtype: string"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a sample of rows with accented characters\n",
    "accented_char_rows = df[df[\"clean_text\"].str.contains(r\"√â|√©|√Å|√°|√≥|√ì|√∫|√ö|√≠|√ç\")]\n",
    "accented_char_rows[\"clean_text\"].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67c48c",
   "metadata": {},
   "source": [
    "Although in a real-world context these do serve as a form of emotional expression, they provide no relevance towards _textual_ sentiment analysis, thus we normalize the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f71be",
   "metadata": {},
   "source": [
    "To normalize the text, the `normalize` function was created. It normalizes the text input to ASCII-only characters (say, \"c√≥mo est√°s\" becomes \"como estas\") and lowercased alphabetic symbols. The dataset contains Unicode characters (e.g., emojis and accented characters) which the function replaces to the empty string (`''`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c51b18406b48165a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:05.871805Z",
     "start_time": "2026-01-18T06:21:05.834143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m normalize(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m normalize(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Normalize text from a pandas entry to ASCII-only lowercase characters. Hence, this removes Unicode characters with no ASCII\u001b[39m\n",
       "\u001b[33m    equivalent (e.g., emojis and CJKs).\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    ASCII-normalized text containing only lowercase letters.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    normalize(\"¬øC√≥mo est√°s?\")\u001b[39m\n",
       "\u001b[33m    $ 'como estas?'\u001b[39m\n",
       "\n",
       "\u001b[33m    normalize(\" hahahaha HUY! Kamusta üòÖ Mayaman $$$ ka na ba?\")\u001b[39m\n",
       "\u001b[33m    $ ' hahahaha huy! kamusta  mayaman $$$ ka na ba?'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    normalized = unicodedata.normalize(\u001b[33m\"NFKD\"\u001b[39m, text)\n",
       "    ascii_text = normalized.encode(\u001b[33m\"ascii\"\u001b[39m, \u001b[33m\"ignore\"\u001b[39m).decode(\u001b[33m\"ascii\"\u001b[39m)\n",
       "\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m ascii_text.lower()\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalize??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec84455a626f9f",
   "metadata": {},
   "source": [
    "### **Punctuations**\n",
    "\n",
    "Punctuations are part of natural speech and reading to provide a sense of structure, clarity, and tone to sentences, but in the context of a classification study, punctuations do not add much information to the sentiment of a message. The sentiment of `i hate you!` and `i hate you` are going to be the same despite the punctuation mark `!` being used to accentuate the sentiment. We can see a sample of rows with punctations below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9886571d774545",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:06.314882Z",
     "start_time": "2026-01-18T06:21:06.106106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2404                                                                                                                 nonexhaustive list important data that the modi govt has not released doesn‚Äô have via \n",
       "118497                                                                                                                congress equated modi stands for masood azhar osama bin laden dawood ibrahim and isi‚Äô\n",
       "4698                                            yes the time has come\\nbut 1st have receive ‚Çπ lakh from otherwise will miss ‚Çπ lakh get ‚Çπ72000 thank you sir will not leave modi till receive amount ‚Çπ lakh \n",
       "111183    let india crore out 130 crore people are belonging below proverty then said that get 72000 now solution 20of 10cr 2cr √ó72000 xxxxxx then what about people lets modi may take care remain people \n",
       "146677                                                                                                                                 ‚Äòoppn scared chowkidar people trust‚Äô assam modi targets rivals\\n‚Å¶ ‚Å¶ \n",
       "Name: clean_text, dtype: string"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a sample of rows with punctuation\n",
    "rows_with_punc = df[df[\"clean_text\"].str.contains(r\"[^\\w\\s]\")]\n",
    "rows_with_punc[\"clean_text\"].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4886fa6c4576e",
   "metadata": {},
   "source": [
    "To address this, the function `rem_punctuation` was made, which replaces all punctuations and special characters with an empty string (`''`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ff9598587e9ad4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:06.350266Z",
     "start_time": "2026-01-18T06:21:06.345561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m rem_punctuation(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m rem_punctuation(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Removes the punctuations. This function simply replaces all punctuation marks and special characters\u001b[39m\n",
       "\u001b[33m    to the empty string. Hence, for symbols enclosed by whitespace, the whitespace are not collapsed to a single whitespace\u001b[39m\n",
       "\u001b[33m    (for more information, see the examples).\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    Text with the punctuation removed.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    rem_punctuation(\"this word $$ has two spaces after it!\")\u001b[39m\n",
       "\u001b[33m    $ 'this word  has two spaces after it'\u001b[39m\n",
       "\n",
       "\u001b[33m    rem_punctuation(\"these!words@have$no%space\")\u001b[39m\n",
       "\u001b[33m    $ 'thesewordshavenospace'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mf\"[{re.escape(string.punctuation)}]\"\u001b[39m, \u001b[33m\"\"\u001b[39m, text)\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rem_punctuation??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a8d22b332d2c8",
   "metadata": {},
   "source": [
    "### **Numbers**\n",
    "\n",
    "Similar to punctuations, numbers do not add any information to the sentiment of a message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeff5ecbc15fd114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:06.564822Z",
     "start_time": "2026-01-18T06:21:06.427412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122093         actually 1984 was bloody massacre and living never forgot they have been anticongress since then might have changed congress perception with his push for small but important step towards peace delaying til elections\n",
       "102669                                                                                                                                                      off course stealing 30000 crores better than stealing 250 crores chor modi\n",
       "141013    after losing ge2019 messers should follow footsteps gods rama laxmana taking jalsamaadhi committing suicide drowning oneself this will set scintillating example for generations aryanbrahminist politicians their followers\n",
       "152527                                 lolak begging with party seat delhilol all these fellow were collie before 2014 and yes aap irrelevant even delhi nowif modi has done nothing than why the hell begging front rahul for lagbagh\n",
       "100530                  gradually moving from modi baiting agendapolicy issues election speeches first 72000 scheme now new enterprise sops his positive response balakote msat also commendable now voting will much more interesting\n",
       "Name: clean_text, dtype: string"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a sample of rows that contain numbers\n",
    "rows_with_numbers = df[df[\"clean_text\"].str.contains(r\"\\d\")]\n",
    "rows_with_numbers[\"clean_text\"].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de020442d3e40f",
   "metadata": {},
   "source": [
    "Hence, we defined the `rem_numbers` as a function that replaces all numerical values as an empty string (`''`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a884584dae378a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:06.615464Z",
     "start_time": "2026-01-18T06:21:06.610827Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m rem_numbers(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m rem_numbers(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    Removes numbers. This function simply replaces all numerical symbols to the empty string. Hence, for symbols enclosed by\u001b[39m\n",
       "\u001b[33m    whitespace, the whitespace are not collapsed to a single whitespace (for more information, see the examples).\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    Text with the numerical symbol removed\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    rem_numbers(\" h3llo, k4must4 k4  n4?\")\u001b[39m\n",
       "\u001b[33m    ' hllo, kmust k  n?'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33mr\"\\d+\"\u001b[39m, \u001b[33m\"\"\u001b[39m, text)\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rem_numbers??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ef7dcd56e59d3",
   "metadata": {},
   "source": [
    "### **Whitespace**\n",
    "\n",
    "Similar to punctations, whitespaces do not add any information to the text and are from user errors. We check if our data has whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc5ddba1dcb1bd69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:07.034791Z",
     "start_time": "2026-01-18T06:21:06.705142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106781                                                             those who criticise today arnab live interview with modi  challenge those people have guts tell face live interview with arnab goswami waiting hashtag \n",
       "45637            indias chowkidar narendra modi makes india capable hunting down its enemies land air sea and now space another addition india arsenal  india has entered its name elite space power under leadership modi\n",
       "88894                                  this proves you congress are corrupt and congress with corruption scams policy paralysis indecisiveness shame this ideology which wants take india back into stone age namo again  \n",
       "37519     dear failif insist him pay lakh promise which modi never gave might try meet this certain extentif comes back power remember also said that‚Äô the kind money kept family swiss mauritius  will take and give poor\n",
       "59339                                                                                    \\ndidnt believe modi will space personally and collect proof and see successful not\\nbhai vapas mat ana please\\nsome one shared  \n",
       "Name: clean_text, dtype: string"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a sample of rows that contain 2 or more whitespaces in a row\n",
    "rows_with_whitespaces = df[df[\"clean_text\"].str.contains(r\"\\s{2,}\")]\n",
    "rows_with_whitespaces[\"clean_text\"].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153cf39c16bb5bc6",
   "metadata": {},
   "source": [
    "Thus to address the problem, the function `collapse_whitespace` was made, which collapses all whitespace characters to a single space. Formally, it is a transducer\n",
    "\n",
    "$$\n",
    "\\Box^+ \\mapsto \\Box \\qquad \\text{where the space character is } \\Box\n",
    "$$\n",
    "\n",
    "Informally, it replaces all strings of whitespaces to a single whitespace character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8202ff80fa87b6b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:07.080263Z",
     "start_time": "2026-01-18T06:21:07.075640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m collapse_whitespace(text: str) -> str\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m collapse_whitespace(text: str) -> str:\n",
       "    \u001b[33m\"\"\"\u001b[39m\n",
       "\u001b[33m    This collapses whitespace. Here, collapsing means the transduction of all whitespace strings of any\u001b[39m\n",
       "\u001b[33m    length to a whitespace string of unit length (e.g., \"   \" -> \" \"; formally \" \"+ -> \" \").\u001b[39m\n",
       "\n",
       "\u001b[33m    Do not use this function alone, use `clean_and_tokenize()`.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Parameters\u001b[39m\n",
       "\u001b[33m    * text: String entry.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Returns\u001b[39m\n",
       "\u001b[33m    Text with the whitespaces collapsed.\u001b[39m\n",
       "\n",
       "\u001b[33m    # Examples\u001b[39m\n",
       "\u001b[33m    collapse_whitespace(\"  huh,  was.  that!!! \")\u001b[39m\n",
       "\u001b[33m    $ 'huh, was. that!!!'\u001b[39m\n",
       "\u001b[33m    \"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m re.sub(\u001b[33m\" +\"\u001b[39m, \u001b[33m\" \"\u001b[39m, text).strip()\n",
       "\u001b[31mFile:\u001b[39m      ~/STINTSY-Order-of-Erin/lib/janitor.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collapse_whitespace??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bba168b5d34a2",
   "metadata": {},
   "source": [
    "To seamlessly call all these cleaning functions, we have the `clean` function that acts as a container that calls these separate components. The definition of this wrapper function is quite long, see [this appendix](#appendix:-clean-wrapper-function-definition) for its definition.\n",
    "\n",
    "We can now clean the dataset and store it in a new column named `clean_ours` (to differentiate it with the, still dirty, column `clean_text` from the dataset author)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98d9e4b732739713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:08.436169Z",
     "start_time": "2026-01-18T06:21:07.125391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162969 non-null  string\n",
      " 1   category    162969 non-null  int64 \n",
      " 2   clean_ours  162969 non-null  string\n",
      "dtypes: int64(1), string(2)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_ours\"] = df[\"clean_text\"].map(clean).astype(\"string\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fcaa27b0aa148",
   "metadata": {},
   "source": [
    "To confirm if the character cleaning worked, we can check for the differences between `clean_text` and `clean_ours` from the filtered rows below and compare the differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef150976aecf6a01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:09.075969Z",
     "start_time": "2026-01-18T06:21:08.453548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>clean_ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94411</th>\n",
       "      <td>sees the biggest danger his design propaganda and deceit however must remember that don‚Äô protect our freedoms they may lost forever must fight back this election save the soul india</td>\n",
       "      <td>-1</td>\n",
       "      <td>sees the biggest danger his design propaganda and deceit however must remember that don protect our freedoms they may lost forever must fight back this election save the soul india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29593</th>\n",
       "      <td>politics was never about courtesy but what‚Äô happening since 2014 new heres what changed</td>\n",
       "      <td>1</td>\n",
       "      <td>politics was never about courtesy but what happening since new heres what changed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152631</th>\n",
       "      <td>you have any proof that modi had committed 1500000 what had actually talked why you are bluffing jumla rahul the grandson feroze</td>\n",
       "      <td>0</td>\n",
       "      <td>you have any proof that modi had committed what had actually talked why you are bluffing jumla rahul the grandson feroze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106890</th>\n",
       "      <td>this viscerally antimodi joker spewing out his bile will eat crow 23rd may</td>\n",
       "      <td>0</td>\n",
       "      <td>this viscerally antimodi joker spewing out his bile will eat crow rd may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133462</th>\n",
       "      <td>legendary singer lata mangeshkar releases song which recital the ‚Äòsaugandh mujhe mitti ‚Äô poem which prime minister modi has often recited</td>\n",
       "      <td>1</td>\n",
       "      <td>legendary singer lata mangeshkar releases song which recital the saugandh mujhe mitti poem which prime minister modi has often recited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5678</th>\n",
       "      <td>the last four years not even single legislative step has been taken the central government protect the environment‚Äù said lawyer ritwick dutta ‚Äùevery single law related environment being diluted which will make urban areas unliveable\\n</td>\n",
       "      <td>-1</td>\n",
       "      <td>the last four years not even single legislative step has been taken the central government protect the environment said lawyer ritwick dutta every single law related environment being diluted which will make urban areas unliveable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95979</th>\n",
       "      <td>thats what modi did 2014\\nrararara achhe din aayenge</td>\n",
       "      <td>0</td>\n",
       "      <td>thats what modi did \\nrararara achhe din aayenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17308</th>\n",
       "      <td>same when promise give 72000 but you are little think people and you could not think about modis thinking\\nwhere your brains nerves stop workingfrom there starts think about powerful india</td>\n",
       "      <td>1</td>\n",
       "      <td>same when promise give but you are little think people and you could not think about modis thinking\\nwhere your brains nerves stop workingfrom there starts think about powerful india</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21683</th>\n",
       "      <td>useful chart modi still popular mprajjharkhand but less gujkarnataka maharashtra etc punjab just tweeted ‚Äô orissa 625 but 649 236cm 413 wbengal 432cm 456 439cm 222 game</td>\n",
       "      <td>1</td>\n",
       "      <td>useful chart modi still popular mprajjharkhand but less gujkarnataka maharashtra etc punjab just tweeted orissa but cm wbengal cm cm game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53684</th>\n",
       "      <td>wait and see its not over yet more come \\nmodi here for least more years and manmohan innocent trees bring real smile face media yet why</td>\n",
       "      <td>1</td>\n",
       "      <td>wait and see its not over yet more come \\nmodi here for least more years and manmohan innocent trees bring real smile face media yet why</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                        clean_text  \\\n",
       "94411                                                       sees the biggest danger his design propaganda and deceit however must remember that don‚Äô protect our freedoms they may lost forever must fight back this election save the soul india    \n",
       "29593                                                                                                                                                     politics was never about courtesy but what‚Äô happening since 2014 new heres what changed    \n",
       "152631                                                                                                           you have any proof that modi had committed 1500000 what had actually talked why you are bluffing jumla rahul the grandson feroze    \n",
       "106890                                                                                                                                                                  this viscerally antimodi joker spewing out his bile will eat crow 23rd may   \n",
       "133462                                                                                                  legendary singer lata mangeshkar releases song which recital the ‚Äòsaugandh mujhe mitti ‚Äô poem which prime minister modi has often recited    \n",
       "5678    the last four years not even single legislative step has been taken the central government protect the environment‚Äù said lawyer ritwick dutta ‚Äùevery single law related environment being diluted which will make urban areas unliveable\\n   \n",
       "95979                                                                                                                                                                                        thats what modi did 2014\\nrararara achhe din aayenge    \n",
       "17308                                                 same when promise give 72000 but you are little think people and you could not think about modis thinking\\nwhere your brains nerves stop workingfrom there starts think about powerful india   \n",
       "21683                                                                    useful chart modi still popular mprajjharkhand but less gujkarnataka maharashtra etc punjab just tweeted ‚Äô orissa 625 but 649 236cm 413 wbengal 432cm 456 439cm 222 game    \n",
       "53684                                                                                                     wait and see its not over yet more come \\nmodi here for least more years and manmohan innocent trees bring real smile face media yet why   \n",
       "\n",
       "        category  \\\n",
       "94411         -1   \n",
       "29593          1   \n",
       "152631         0   \n",
       "106890         0   \n",
       "133462         1   \n",
       "5678          -1   \n",
       "95979          0   \n",
       "17308          1   \n",
       "21683          1   \n",
       "53684          1   \n",
       "\n",
       "                                                                                                                                                                                                                                    clean_ours  \n",
       "94411                                                     sees the biggest danger his design propaganda and deceit however must remember that don protect our freedoms they may lost forever must fight back this election save the soul india  \n",
       "29593                                                                                                                                                        politics was never about courtesy but what happening since new heres what changed  \n",
       "152631                                                                                                                you have any proof that modi had committed what had actually talked why you are bluffing jumla rahul the grandson feroze  \n",
       "106890                                                                                                                                                                this viscerally antimodi joker spewing out his bile will eat crow rd may  \n",
       "133462                                                                                                  legendary singer lata mangeshkar releases song which recital the saugandh mujhe mitti poem which prime minister modi has often recited  \n",
       "5678    the last four years not even single legislative step has been taken the central government protect the environment said lawyer ritwick dutta every single law related environment being diluted which will make urban areas unliveable  \n",
       "95979                                                                                                                                                                                         thats what modi did \\nrararara achhe din aayenge  \n",
       "17308                                                   same when promise give but you are little think people and you could not think about modis thinking\\nwhere your brains nerves stop workingfrom there starts think about powerful india  \n",
       "21683                                                                                                useful chart modi still popular mprajjharkhand but less gujkarnataka maharashtra etc punjab just tweeted orissa but cm wbengal cm cm game  \n",
       "53684                                                                                                 wait and see its not over yet more come \\nmodi here for least more years and manmohan innocent trees bring real smile face media yet why  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_rows = df[\n",
    "    df[\"clean_text\"].str.contains(r\"\\s{2,}|\\d|[^\\w\\s]|[\\u263a-\\U0001f645]|[√â√©√Å√°√≥√ì√∫√ö√≠√ç]\")\n",
    "]\n",
    "example_rows.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbd10a15775c3e4",
   "metadata": {},
   "source": [
    "We are now finished with basic text cleaning, but the data cleaning does not end here. Given that the text is sourced from Twitter, it includes characteristics, such as spam and informal expressions, which are not addressed by basic cleaning methods. As a result, we move on to further cleaning tailored to the nature of Twitter data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b6ce7a527697a",
   "metadata": {},
   "source": [
    "### **Spam, Expressions, Onomatopoeia, etc.**\n",
    "\n",
    "Since the domain of the corpus is Twitter, spam (e.g., `bbbb`), expressions (e.g., `bruhhhh`), and onomatopoeia (e.g., `hahahaha`) may become an issue by the vector representation step. Hence we employed a simple rule-based spam removal algorithm.\n",
    "\n",
    "We remove words in the string that contains the same letter or substring thrice and consecutively. These were done using regular expressions:\n",
    "\n",
    "$$\n",
    "\\text{same\\_char\\_thrice} := (.)\\textbackslash1^{\\{2,\\}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\text{same\\_substring\\_twice} := (.^+)\\textbackslash1^+\n",
    "$$\n",
    "\n",
    "Furthermore, we also remove any string that has a length less than three, since these are either stopwords (that weren't detected in the stopword removal stage) or more spam.\n",
    "\n",
    "Finally, we employ adaptive character diversity threshold for the string $s$.\n",
    "\n",
    "$$\n",
    "\\frac{\\texttt{\\#\\_unique\\_chars}(s)}{|s|} < 0.3 + \\left(\\frac{0.1 \\cdot \\text{min}(|s|, 10)}{10}\\right)\n",
    "$$\n",
    "\n",
    "It calculates the diversity of characters in a string; if the string repeats the same character alot, we expect it to be unintelligible or useless, hence we remove the string.\n",
    "\n",
    "The definition of this wrapper function is quite long, see its definition in [this appendix](#appendix:-find_spam_and_empty-wrapper-function-definition).\n",
    "\n",
    "Let's first look at a random sample of 10 entries from the dataset that will be modified by the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7daa866e78a3186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:12.849605Z",
     "start_time": "2026-01-18T06:21:09.111698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4463                                                                                                                                                                                      can identify slave did ever modi said l that rafool and his pidis said\n",
       "116058                                                                                                                                                                                    aap files complaint with against narendra modi for violating poll code\n",
       "65237                                                                                                                                          year old tejasvi surya candidate for bangalore south place late anantkumar inspiration for youth jai bjp jai modi\n",
       "18013                     strange logic since death rajiv gandhi theirs gandhi family minister they have given the best till date pvn then mms both were intellectuals non politician both steered india new economic aspersions which destroyed past years modi\n",
       "76428                                                                                                  yes before modi had army and drdo cheap politics ever history india cashing everything for politics sensible ppl except bhakt should think and reward him\n",
       "117605      another indian govt lie how can sophisticated air defense system hit own aircraft pakistani fighters shot down this chopper too russian air defense system awesome maybe modi sarkar looking for discounted price for their next deal with russian s\n",
       "45730                                                                                                                                                                                                                             amazing hats off you modi kaka\n",
       "81216                                                                                                                                                                                   modi doesnt put ppl who abuse him jail unlike happy abuse away its karma\n",
       "103716    actually anandabazar become mouth pice pakistani pakistani supporter desh gaddar news paper mein anandabazar will write golden word continue propaganda against india modi birodh aur desh birodh mein difference pata hai fir desh birodhi propaganda\n",
       "125544                                                                                                                                  you said that you told that for defeat modi you ready any job\\npls resigh gaunfor farming\\nv hindu hates you\\ntum gaddar\n",
       "Name: clean_ours, dtype: string"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affected = df[df[\"clean_ours\"].apply(spam_affected)]\n",
    "affected_sample = affected[\"clean_ours\"].sample(10)\n",
    "affected_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149953bd76116eb7",
   "metadata": {},
   "source": [
    "Let's now call this function on the `clean_ours` column of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c3dd0bd6dcc1559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:16.927333Z",
     "start_time": "2026-01-18T06:21:12.904253Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"clean_ours\"] = df[\"clean_ours\"].map(find_spam_and_empty).astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f32c3775a4fcb",
   "metadata": {},
   "source": [
    "To confirm if the function was able to do remove all the spammy substrings, we can check `before` and `after` to compare their differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc0b5e0ac9920197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:17.048206Z",
     "start_time": "2026-01-18T06:21:17.005954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116058</th>\n",
       "      <td>aap files complaint with against narendra modi for violating poll code</td>\n",
       "      <td>files complaint with against narendra modi for violating poll code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4463</th>\n",
       "      <td>can identify slave did ever modi said l that rafool and his pidis said</td>\n",
       "      <td>can identify slave did ever modi said that rafool and his pidis said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45730</th>\n",
       "      <td>amazing hats off you modi kaka</td>\n",
       "      <td>amazing hats off you modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76428</th>\n",
       "      <td>yes before modi had army and drdo cheap politics ever history india cashing everything for politics sensible ppl except bhakt should think and reward him</td>\n",
       "      <td>yes before modi had army and drdo cheap politics ever history india cashing everything for politics sensible except bhakt should think and reward him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18013</th>\n",
       "      <td>strange logic since death rajiv gandhi theirs gandhi family minister they have given the best till date pvn then mms both were intellectuals non politician both steered india new economic aspersions which destroyed past years modi</td>\n",
       "      <td>strange logic since death rajiv gandhi theirs gandhi family minister they have given the best till date pvn then both were intellectuals non politician both steered india new economic aspersions which destroyed past years modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125544</th>\n",
       "      <td>you said that you told that for defeat modi you ready any job\\npls resigh gaunfor farming\\nv hindu hates you\\ntum gaddar</td>\n",
       "      <td>you said that you told that for defeat modi you ready any job pls resigh gaunfor farming hindu hates you tum gaddar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81216</th>\n",
       "      <td>modi doesnt put ppl who abuse him jail unlike happy abuse away its karma</td>\n",
       "      <td>modi doesnt put who abuse him jail unlike happy abuse away its karma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117605</th>\n",
       "      <td>another indian govt lie how can sophisticated air defense system hit own aircraft pakistani fighters shot down this chopper too russian air defense system awesome maybe modi sarkar looking for discounted price for their next deal with russian s</td>\n",
       "      <td>another indian govt lie how can sophisticated air defense system hit own aircraft pakistani fighters shot down this chopper too russian air defense system awesome maybe modi sarkar looking for discounted price for their next deal with russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65237</th>\n",
       "      <td>year old tejasvi surya candidate for bangalore south place late anantkumar inspiration for youth jai bjp jai modi</td>\n",
       "      <td>year old tejasvi surya candidate for bangalore south place late inspiration for youth jai bjp jai modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103716</th>\n",
       "      <td>actually anandabazar become mouth pice pakistani pakistani supporter desh gaddar news paper mein anandabazar will write golden word continue propaganda against india modi birodh aur desh birodh mein difference pata hai fir desh birodhi propaganda</td>\n",
       "      <td>actually become mouth pice pakistani pakistani supporter desh gaddar news paper mein will write golden word continue propaganda against india modi birodh aur desh birodh mein difference pata hai fir desh birodhi propaganda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        before  \\\n",
       "116058                                                                                                                                                                                  aap files complaint with against narendra modi for violating poll code   \n",
       "4463                                                                                                                                                                                    can identify slave did ever modi said l that rafool and his pidis said   \n",
       "45730                                                                                                                                                                                                                           amazing hats off you modi kaka   \n",
       "76428                                                                                                yes before modi had army and drdo cheap politics ever history india cashing everything for politics sensible ppl except bhakt should think and reward him   \n",
       "18013                   strange logic since death rajiv gandhi theirs gandhi family minister they have given the best till date pvn then mms both were intellectuals non politician both steered india new economic aspersions which destroyed past years modi   \n",
       "125544                                                                                                                                you said that you told that for defeat modi you ready any job\\npls resigh gaunfor farming\\nv hindu hates you\\ntum gaddar   \n",
       "81216                                                                                                                                                                                 modi doesnt put ppl who abuse him jail unlike happy abuse away its karma   \n",
       "117605    another indian govt lie how can sophisticated air defense system hit own aircraft pakistani fighters shot down this chopper too russian air defense system awesome maybe modi sarkar looking for discounted price for their next deal with russian s   \n",
       "65237                                                                                                                                        year old tejasvi surya candidate for bangalore south place late anantkumar inspiration for youth jai bjp jai modi   \n",
       "103716  actually anandabazar become mouth pice pakistani pakistani supporter desh gaddar news paper mein anandabazar will write golden word continue propaganda against india modi birodh aur desh birodh mein difference pata hai fir desh birodhi propaganda   \n",
       "\n",
       "                                                                                                                                                                                                                                                     after  \n",
       "116058                                                                                                                                                                                  files complaint with against narendra modi for violating poll code  \n",
       "4463                                                                                                                                                                                  can identify slave did ever modi said that rafool and his pidis said  \n",
       "45730                                                                                                                                                                                                                            amazing hats off you modi  \n",
       "76428                                                                                                yes before modi had army and drdo cheap politics ever history india cashing everything for politics sensible except bhakt should think and reward him  \n",
       "18013                   strange logic since death rajiv gandhi theirs gandhi family minister they have given the best till date pvn then both were intellectuals non politician both steered india new economic aspersions which destroyed past years modi  \n",
       "125544                                                                                                                                 you said that you told that for defeat modi you ready any job pls resigh gaunfor farming hindu hates you tum gaddar  \n",
       "81216                                                                                                                                                                                 modi doesnt put who abuse him jail unlike happy abuse away its karma  \n",
       "117605  another indian govt lie how can sophisticated air defense system hit own aircraft pakistani fighters shot down this chopper too russian air defense system awesome maybe modi sarkar looking for discounted price for their next deal with russian  \n",
       "65237                                                                                                                                               year old tejasvi surya candidate for bangalore south place late inspiration for youth jai bjp jai modi  \n",
       "103716                      actually become mouth pice pakistani pakistani supporter desh gaddar news paper mein will write golden word continue propaganda against india modi birodh aur desh birodh mein difference pata hai fir desh birodhi propaganda  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\"before\": affected_sample, \"after\": df[\"clean_ours\"]})\n",
    "\n",
    "changed = comparison[comparison[\"before\"] != comparison[\"after\"]]\n",
    "changed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574d372de7c6187",
   "metadata": {},
   "source": [
    "Let‚Äôs examine whether applying this function has caused any significant changes to the DataFrame structure, given that it can convert entire cells to `NaN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fcae05be6790b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:17.140680Z",
     "start_time": "2026-01-18T06:21:17.114551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162969 non-null  string\n",
      " 1   category    162969 non-null  int64 \n",
      " 2   clean_ours  162942 non-null  string\n",
      "dtypes: int64(1), string(2)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad287419963b0b",
   "metadata": {},
   "source": [
    "The DataFrame structure is intact, but **`clean_ours`** now has 27 fewer non-null values, reflecting cells that were entirely filtered out as spam as seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c623977ed939eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:17.356637Z",
     "start_time": "2026-01-18T06:21:17.337251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_ours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21806</th>\n",
       "      <td>bjpmpsubramanianswamyiamchowkidarcampaignpmmodi</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21855</th>\n",
       "      <td>terrorfundinghurriyatleaderspropertyseizedhafizsaeedmodigovt</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24148</th>\n",
       "      <td>pmnarendramodirequestsofexservicemanindianarmyhavildarombirsinghsharma9258</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35636</th>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35866</th>\n",
       "      <td>‚Äç</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35968</th>\n",
       "      <td>whattttttt</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37837</th>\n",
       "      <td>allllll</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40587</th>\n",
       "      <td>1145am</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40977</th>\n",
       "      <td>‚åö1145 ‚ù§</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48127</th>\n",
       "      <td>birthdaaaaaay</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51360</th>\n",
       "      <td>panchattt</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53552</th>\n",
       "      <td>2findia2fwhatpmmodididnottellyouinaddresstonationindiahadachievedasatcapabil</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73958</th>\n",
       "      <td>oops</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77224</th>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83278</th>\n",
       "      <td>jawanshaaaa</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92073</th>\n",
       "      <td>‚Å¶ ‚Å¶ ‚Å¶ ‚Å¶ ‚Å¶ ‚Å¶</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102992</th>\n",
       "      <td>importanthearingsinuktodayoncasesoffugitivevijaymallyaandniravmodi</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103389</th>\n",
       "      <td>ismoditakesganjasaidaamadamiministernotgoodpltoinewsseenpl</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103563</th>\n",
       "      <td>missionshaktipmmodiviolatemodelcodeofconductectodecideontoday</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107944</th>\n",
       "      <td>mmurderer\\noof\\nddemocratic\\niindia</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114693</th>\n",
       "      <td>iindia\\nddwalpment oonly mmodi</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122246</th>\n",
       "      <td>‚Äô  ‚Äô</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136055</th>\n",
       "      <td>sixerrrmodi sixxerrrr</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141136</th>\n",
       "      <td>tooo</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141812</th>\n",
       "      <td>checkoutcccchhhnnnddrrraa\\n</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152943</th>\n",
       "      <td>congressizcommingtakebeefmodikehtahaicowizmotherandbjptakingthemeatofcow</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161347</th>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          clean_text  \\\n",
       "21806                                bjpmpsubramanianswamyiamchowkidarcampaignpmmodi   \n",
       "21855                   terrorfundinghurriyatleaderspropertyseizedhafizsaeedmodigovt   \n",
       "24148     pmnarendramodirequestsofexservicemanindianarmyhavildarombirsinghsharma9258   \n",
       "35636                                                                          2019    \n",
       "35866                                                                             ‚Äç    \n",
       "35968                                                                    whattttttt    \n",
       "37837                                                                        allllll   \n",
       "40587                                                                         1145am   \n",
       "40977                                                                        ‚åö1145 ‚ù§   \n",
       "48127                                                                birthdaaaaaay     \n",
       "51360                                                                     panchattt    \n",
       "53552   2findia2fwhatpmmodididnottellyouinaddresstonationindiahadachievedasatcapabil   \n",
       "73958                                                                           oops   \n",
       "77224                                                                                  \n",
       "83278                                                                   jawanshaaaa    \n",
       "92073                                                                   ‚Å¶ ‚Å¶ ‚Å¶ ‚Å¶ ‚Å¶ ‚Å¶    \n",
       "102992            importanthearingsinuktodayoncasesoffugitivevijaymallyaandniravmodi   \n",
       "103389                    ismoditakesganjasaidaamadamiministernotgoodpltoinewsseenpl   \n",
       "103563                 missionshaktipmmodiviolatemodelcodeofconductectodecideontoday   \n",
       "107944                                          mmurderer\\noof\\nddemocratic\\niindia    \n",
       "114693                                                iindia\\nddwalpment oonly mmodi   \n",
       "122246                                                                        ‚Äô  ‚Äô     \n",
       "136055                                                         sixerrrmodi sixxerrrr   \n",
       "141136                                                                         tooo    \n",
       "141812                                                   checkoutcccchhhnnnddrrraa\\n   \n",
       "152943     congressizcommingtakebeefmodikehtahaicowizmotherandbjptakingthemeatofcow    \n",
       "161347                                                                          2019   \n",
       "\n",
       "       clean_ours  \n",
       "21806        <NA>  \n",
       "21855        <NA>  \n",
       "24148        <NA>  \n",
       "35636        <NA>  \n",
       "35866        <NA>  \n",
       "35968        <NA>  \n",
       "37837        <NA>  \n",
       "40587        <NA>  \n",
       "40977        <NA>  \n",
       "48127        <NA>  \n",
       "51360        <NA>  \n",
       "53552        <NA>  \n",
       "73958        <NA>  \n",
       "77224        <NA>  \n",
       "83278        <NA>  \n",
       "92073        <NA>  \n",
       "102992       <NA>  \n",
       "103389       <NA>  \n",
       "103563       <NA>  \n",
       "107944       <NA>  \n",
       "114693       <NA>  \n",
       "122246       <NA>  \n",
       "136055       <NA>  \n",
       "141136       <NA>  \n",
       "141812       <NA>  \n",
       "152943       <NA>  \n",
       "161347       <NA>  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_rows = df[df[\"clean_ours\"].isna()]\n",
    "spam_rows[[\"clean_text\", \"clean_ours\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a6ebf34e4c5ac",
   "metadata": {},
   "source": [
    "## **Post-Cleaning Steps**\n",
    "\n",
    "At some point during the cleaning stage, some entries of the dataset could have been reduced to `NaN` or the empty string `\"\"`, or we could have introduced duplicates again. So, let's call `dropna` and `drop_duplicates` again to finalize the cleaning stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b77394f1ac3f1e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:17.500470Z",
     "start_time": "2026-01-18T06:21:17.435678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162942 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162942 non-null  string\n",
      " 1   category    162942 non-null  int64 \n",
      " 2   clean_ours  162942 non-null  string\n",
      "dtypes: int64(1), string(2)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a76bc53fd892f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:17.708244Z",
     "start_time": "2026-01-18T06:21:17.558266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 162942 entries, 0 to 162979\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162942 non-null  string\n",
      " 1   category    162942 non-null  int64 \n",
      " 2   clean_ours  162942 non-null  string\n",
      "dtypes: int64(1), string(2)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509bf0e0b5d42386",
   "metadata": {},
   "source": [
    "# **3. Preprocessing**\n",
    "\n",
    "> WIP Narrative and Sequence\n",
    "> üèóÔ∏è Perhaps swap S3 and S4. Refer to literature on what comes first.\n",
    "\n",
    "This section discusses preprocessing steps for the cleaned data. Because the goal is to analyze the textual sentiments of tweets the following preprocessing steps are needed to provide the Bag of Words model with the relevant information required to get the semantic embeddings of each tweet.\n",
    "\n",
    "Before and after each preprocessing step, we will show 5 random entries in the dataset to show the effects of each preprocessing task.\n",
    "\n",
    "## **Lemmatization**\n",
    "\n",
    "We follow a similar methodology for data cleaning presented in <u>(George & Murugesan, 2024)</u>. We preprocess the dataset entries via lemmatization. For the lemmatization step, we use the SpaCy's `en_core_web_sm` version 3.8.0, which is a pretrained language model for English <u>(Honnibal et al., 2020)</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c888e9c01204e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:24.926107Z",
     "start_time": "2026-01-18T06:21:17.724566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0f867b94f2494a9895bed929b98f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/162942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"lemmatized\"] = df[\"clean_ours\"].progress_apply(lemmatizer)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55df5f4383e01e",
   "metadata": {},
   "source": [
    "## **Stop Word Removal**\n",
    "\n",
    "After lemmatization, we may now remove the stop words present in the dataset. The stopword removal _needs_ to be after lemmatization since this step requires all words to be reduces to their base dictionary form, and the `stopword_set` only considers base dictionary forms of the stopwords.\n",
    "\n",
    "**stopwords.** For stop words removal, we refer to the English stopwords dataset defined in NLTK and Wolfram Mathematica <u>(Bird & Loper, 2004; Wolfram Research, 2015)</u>. However, since the task is sentiment analysis, words that invoke polarity, intensification, and negation are important. Words like \"not\" and \"okay\" are commonly included as stopwords. Therefore, the stopwords from <u>(Bird & Loper, 2004; Wolfram Research, 2015)</u> are manually adjusted to only include stopwords that invoke neutrality, examples are \"after\", \"when\", and \"you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9844bc3f9d46f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:25.352712Z",
     "start_time": "2026-01-18T06:21:24.964381Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"lemmatized\"] = df[\"lemmatized\"].map(lambda t: rem_stopwords(t, stopwords_set))\n",
    "df = df.dropna(subset=[\"lemmatized\"])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f50b437ea40de",
   "metadata": {},
   "source": [
    "After preprocessing, the dataset now contains:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9d95ffe762e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:25.459441Z",
     "start_time": "2026-01-18T06:21:25.419112Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfccf121834e3b",
   "metadata": {},
   "source": [
    "Here are 5 randomly picked entries in the dataframe with all columns shown for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcbd4ea7782282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:25.515365Z",
     "start_time": "2026-01-18T06:21:25.506719Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c603cf685e9431",
   "metadata": {},
   "source": [
    "## **Tokenization**\n",
    "\n",
    "Since the data cleaning and preprocessing stage is comprehensive, the tokenization step in the BoW model reduces to a simple word-boundary split operation. Each preprocessed entry in the DataFrame is split by spaces. For example, the entry `\"shri narendra modis\"` (entry: 42052) becomes `[\"shri\", \"narendra\", \"modis\"]`. By the end of tokenization, all entries are transformed into arrays of strings.\n",
    "\n",
    "## **Word Bigrams**\n",
    "\n",
    "As noted earlier, modifiers and polarity words are not included in the stopword set. The BoW model constructs a vocabulary containing both unigrams and bigrams. Including bigrams allows the model to capture common word patterns, such as\n",
    "\n",
    "$$\n",
    "\\left\\langle \\texttt{Adj}\\right\\rangle \\left\\langle \\texttt{M} \\mid \\texttt{Pron} \\right\\rangle\n",
    "$$\n",
    "\n",
    "<center>or</center>\n",
    "\n",
    "$$\n",
    "\\left\\langle \\texttt{Adv}\\right\\rangle \\left\\langle \\texttt{V} \\mid \\texttt{Adj} \\mid \\texttt{Adv} \\right\\rangle\n",
    "$$\n",
    "\n",
    "## **Vector Representation**\n",
    "\n",
    "After the stemming and lemmatization steps, each entry can now be represented as a vector using a Bag of Words (BoW) model. We employ scikit-learn's `CountVectorizer`, which provides a ready-to-use implementation of BoW <u>(Pedregosa et al., 2011)</u>.\n",
    "\n",
    "A comparison of other traditional vector representations are discussed in [this appendix](#appendix:-comparison-of-traditional-vectorization-techniques).\n",
    "Words with modifiers have the modifiers directly attached, enabling subsequent models to capture the concept of modification fully. Consequently, after tokenization and bigram construction, the vocabulary size can grow up to $O(n^2)$, where $n$ is the number of unique tokens.\n",
    "\n",
    "**minimum document frequency constraint:** Despite cleaning and spam removal, some tokens remain irrelevant or too rare. To address this, a minimum document frequency constraint is applied: $\\texttt{min\\_df} = 10$, meaning a token must appear in at least 10 documents to be included in the BoW vocabulary. This reduces noise and ensures the model focuses on meaningful terms.\n",
    "\n",
    "---\n",
    "\n",
    "These parameters of the BoW model are encapsulated in the `BagOfWordsModel` class. The class definition is available in [this appendix](#appendix:-BagOfWordsModel-class-definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003bbf8d9918445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.766272Z",
     "start_time": "2026-01-18T06:21:25.596754Z"
    }
   },
   "outputs": [],
   "source": [
    "bow = BagOfWordsModel(\n",
    "    texts=df[\"lemmatized\"],   # list of words to include in the model\n",
    "    min_freq=10,              # words must appear in at least 10 different documents to be included\n",
    ")\n",
    "\n",
    "# some sanity checks\n",
    "assert (\n",
    "    bow.matrix.shape[0] == df.shape[0]\n",
    "), \"number of rows in the matrix DOES NOT matches the number of documents\"\n",
    "assert bow.sparsity, \"the sparsity is TOO HIGH, something went wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee9ff7b30988ace",
   "metadata": {},
   "source": [
    "The error above is normal, recall that our tokenization step essentially reduced into an array split step. With this, we need to set the `tokenizer` function attribute of the `BagOfWordsModel` to not use its default tokenization pattern. That causes this warning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710423a158dd13b",
   "metadata": {},
   "source": [
    "### **Model Metrics**\n",
    "\n",
    "To get an idea of the model, we will now look at its shape and sparsity, with shape being the number of documents and tokens present in the model. While sparsity refers to the number of elements in a matrix that are zero, calculating how sparse or varied the words are in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62098b7e6071263",
   "metadata": {},
   "source": [
    "The resulting vector has a shape of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0ee30845f6fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.786731Z",
     "start_time": "2026-01-18T06:21:27.783589Z"
    }
   },
   "outputs": [],
   "source": [
    "bow.matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7dae29215d3b1",
   "metadata": {},
   "source": [
    "The first entry of the pair is the number of documents (the ones that remain after all the data cleaning and preprocessing steps) and the second entry is the number of tokens (or unique words in the vocabulary).\n",
    "\n",
    "The resulting model has a sparsity of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e218a41e44b93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.820837Z",
     "start_time": "2026-01-18T06:21:27.817268Z"
    }
   },
   "outputs": [],
   "source": [
    "1 - bow.sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a3a77dd8a23a9",
   "metadata": {},
   "source": [
    "The model is 99.95% sparse, meaning the tweets often do not share the same words leading to a large vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d976024abbcd70",
   "metadata": {},
   "source": [
    "Now, looking at the most frequent and least frequent terms in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f373a6cf90549c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.908510Z",
     "start_time": "2026-01-18T06:21:27.855472Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_frequencies = np.asarray((bow.matrix > 0).sum(axis=0)).flatten()\n",
    "freq_order = np.argsort(doc_frequencies)[::-1]\n",
    "bow.feature_names[freq_order[:50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c89906e1ae7af0",
   "metadata": {},
   "source": [
    "We see that the main talking point of the Tweets, which hovers around Indian politics with keywords like \"modi\", \"india\", and \"bjp\". For additional context, \"bjp\" referes to the _Bharatiya Janata Party_ which is a conservative political party in India, and one of the two major Indian political parties.\n",
    "\n",
    "To better understand these, we can check the wordcloud generated from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb5eaa-7bdf-4a95-a3a1-807c8d2978f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=400, background_color=\"white\", min_font_size=10).generate(\" \".join(bow.feature_names))\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c88a87388f60c8",
   "metadata": {},
   "source": [
    "Now, looking at the least popular words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37452260c594dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.929028Z",
     "start_time": "2026-01-18T06:21:27.925370Z"
    }
   },
   "outputs": [],
   "source": [
    "bow.feature_names[freq_order[-50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd28368c97c6c8",
   "metadata": {},
   "source": [
    "We still see that the themes mentioned in the most frequent terms are still present in this subset. Although, more filler or non-distinct words do appear more often, like \"photos\", \"soft\" and \"types\".\n",
    "\n",
    "But the present of words like \"reelection\" and \"wars\" still point to this subset still being relevant to the main theme of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad3d1624ce68f6",
   "metadata": {},
   "source": [
    "# **4. Exploratory Data Analysis**\n",
    "\n",
    "This section discusses the exploratory data analysis conducted on the dataset after cleaning.\n",
    "\n",
    "> Notes from Zhean: <br>\n",
    "> From manual checking via OpenRefine, there are a total of 162972. `df.info()` should have the same result post-processing.\n",
    "> Furthermore, there should be two columns, `clean_text` (which is a bit of a misnormer since it is still dirty) contains the Tweets (text data). The second column is the `category` which contains the sentiment of the Tweet and is a tribool (1 positive, 0 neutral or indeterminate, and -1 for negative).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9278af5079cb5",
   "metadata": {},
   "source": [
    "Now that we have our clean, lemmatized tweets, we can now work with a new DataFrame containing only **`lemmatized`** and the **`category`** columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f89b5b4b91609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T07:05:46.701864Z",
     "start_time": "2026-01-18T07:05:46.672234Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "df_cleaned = df_cleaned.drop([\"clean_text\", \"clean_ours\"], axis=1)\n",
    "\n",
    "df_cleaned = df_cleaned[[\"lemmatized\", \"category\"]]  # for column reordering\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee16a3fb030f7e",
   "metadata": {},
   "source": [
    "Because we will be splitting this dataset later, we need to know if the distribution of the categories is balanced. An imbalanced distribution may cause a bias to the majority class. Understanding the distribution will inform us whether stratified splitting is necessary so that we do not have an under or overrepresented class.\n",
    "\n",
    "We'll be using a bar graph as that is the simplest way for us to see the differences between the categorical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67944d4d3e6c7cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T08:14:04.450375Z",
     "start_time": "2026-01-18T08:14:04.397918Z"
    }
   },
   "outputs": [],
   "source": [
    "count = df_cleaned[\"category\"].value_counts()\n",
    "\n",
    "plt.title(\"Sentiment Labels Distribution\")\n",
    "\n",
    "count.plot(kind=\"bar\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.8)  # horizontal lines\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc83a7d68cac422",
   "metadata": {},
   "source": [
    "We can see that there is a noticeable difference between the three classes. The positive class (1) has a count of over 70,000, the neutral class (0) has around 55,000, and the negative class (-1) has around 30,000.\n",
    "\n",
    "This imbalance indicates that we must use stratified splitting in the later section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64dc010b96a7640",
   "metadata": {},
   "source": [
    "# **5 Dataset Splitting**\n",
    "\n",
    "Before being able to use the dataset, we need to partition it into three sets:\n",
    "\n",
    "1. **Training** - used to train the model to learn and change its parameters\n",
    "2. **Validation** - used to evaluate the model, comparing its predictions to correct answers for hyperparameter tuning\n",
    "3. **Test** - used to test the model with new, unseen data\n",
    "\n",
    "The following section will be dedicated solely to splitting the dataset. We will split the dataset with 70% for training, 15% for validation, and 15% for testing as this is a standard partitioning.\n",
    "\n",
    "## **Splitting the dataset into Training, Validation, and Testing sets**\n",
    "\n",
    "We'll first split the dataset into 70% and 30% parts by using Scikit-learn's `train_test_split` function. As mentioned earlier, the distribution of categories is imbalanced, so we have to use the function's `stratify` parameter to maintain an even proportion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf28d8ea9cff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(\n",
    "    df_cleaned, test_size=0.3, stratify=df_cleaned[\"category\"], random_state=5\n",
    ")  # 70/30 split\n",
    "\n",
    "print(train.shape, temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3714bf88796e0b",
   "metadata": {},
   "source": [
    "We now have our two sets for training and testing, but we're still missing one more for validation. We can split the 30% part into two halves of 15% so that we have a part for validation and the other part for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2387efb82788bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation, test = train_test_split(\n",
    "    temp, test_size=0.5, stratify=temp[\"category\"], random_state=5\n",
    ")  # 15/15 split\n",
    "\n",
    "print(train.shape, validation.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939a8c2b341ba65",
   "metadata": {},
   "source": [
    "Now that we have our training, validation, and testing sets, we can use these on the models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b7792877acf7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **References**\n",
    "\n",
    "Bird, S., & Loper, E. (2004, July). NLTK: The natural language toolkit. _Proceedings of the ACL Interactive Poster and Demonstration Sessions_, 214‚Äì217. https://aclanthology.org/P04-3031/\n",
    "\n",
    "El-Demerdash, A. A., Hussein, S. E., & Zaki, J. F. W. (2021). Course evaluation based on deep learning and SSA hyperparameters optimization. _Computers, Materials & Continua, 71_(1), 941‚Äì959. https://doi.org/10.32604/cmc.2022.021839\n",
    "\n",
    "George, M., & Murugesan, R. (2024). Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique. _Procedia Computer Science, 244_, 1‚Äì8.\n",
    "\n",
    "Honnibal, M., Montani, I., Van Landeghem, S., & Boyd, A. (2020). spaCy: Industrial-strength Natural Language Processing in Python. https://doi.org/10.5281/zenodo.1212303\n",
    "\n",
    "Hussein, S. (2021). _Twitter sentiments dataset_. Mendeley.\n",
    "\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research, 12_, 2825‚Äì2830.\n",
    "\n",
    "Rani, D., Kumar, R., & Chauhan, N. (2022, October). Study and comparison of vectorization techniques used in text classification. In _2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT)_ (pp. 1‚Äì6). IEEE.\n",
    "\n",
    "Wolfram Research. (2015). _DeleteStopwords_. https://reference.wolfram.com/language/ref/DeleteStopwords.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815ac23f698e335",
   "metadata": {},
   "source": [
    "# **Appendix: `clean` wrapper function definition**\n",
    "\n",
    "Below is the definition of the `clean` wrapper function that encapsulates all internal functions used in the cleaning pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358fa3203a7db464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.964730Z",
     "start_time": "2026-01-18T06:21:27.960135Z"
    }
   },
   "outputs": [],
   "source": [
    "clean??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab34dc65c70050",
   "metadata": {},
   "source": [
    "# **Appendix: `find_spam_and_empty` wrapper function definition**\n",
    "\n",
    "Below is the definition of the `find_spam_and_empty` wrapper function that encapsulates all internal functions for the spam detection algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6288565add6781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:27.987375Z",
     "start_time": "2026-01-18T06:21:27.981713Z"
    }
   },
   "outputs": [],
   "source": [
    "find_spam_and_empty??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d213afb0cdbe7",
   "metadata": {},
   "source": [
    "# **Appendix: comparison of traditional vectorization techniques**\n",
    "\n",
    "Traditional vectorization techniques include BoW and Term Frequency-Inverse Document Frequency (TF-IDF). TF-IDF weights each word based on its frequency in a document and its rarity across the corpus, reducing the impact of common words. BoW, in contrast, simply counts word occurrences without considering corpus-level frequency. In this project, BoW was chosen because stopwords were already removed during preprocessing, and the dataset is domain-specific <u>(Rani et al., 2022)</u>. In such datasets, frequent words are often meaningful domain keywords, so scaling them down (as TF-IDF would) could reduce the importance of these key terms in the feature representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e36fb9e0a3cbe4",
   "metadata": {},
   "source": [
    "# **Appendix: `BagOfWordsModel` class definition**\n",
    "\n",
    "Below is the definition of the `BagOfWordsModel` class that encapsulates the desired parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53618a4be23028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T06:21:28.044372Z",
     "start_time": "2026-01-18T06:21:28.024473Z"
    }
   },
   "outputs": [],
   "source": [
    "BagOfWordsModel??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
